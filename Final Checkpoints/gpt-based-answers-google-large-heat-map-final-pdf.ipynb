{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ce272bd753214c4581a663d1352de470":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fec4f3a6eec24ee28c5f14b4b1a4af51","IPY_MODEL_db18797e4b6648c888328a7a090d5ead","IPY_MODEL_ffb5dc2afc5d4ea797503b55c1f60d72"],"layout":"IPY_MODEL_ae9f0e3238f4441dac8fb7409fe7e9ca"}},"fec4f3a6eec24ee28c5f14b4b1a4af51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_098716c4cb9c417fbb4300f953b0611d","placeholder":"​","style":"IPY_MODEL_ee4e7b3f53a947c8a83e0c1f5120b88a","value":"config.json: 100%"}},"db18797e4b6648c888328a7a090d5ead":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c72237289e749eba4a7efe8094ee87e","max":662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3bdbf78c05b429baa9865049d3fa24d","value":662}},"ffb5dc2afc5d4ea797503b55c1f60d72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82789f95bfe3423cbb3a25a1ef205669","placeholder":"​","style":"IPY_MODEL_a822574493f84e0aabc213ec46afd117","value":" 662/662 [00:00&lt;00:00, 80.7kB/s]"}},"ae9f0e3238f4441dac8fb7409fe7e9ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"098716c4cb9c417fbb4300f953b0611d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee4e7b3f53a947c8a83e0c1f5120b88a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c72237289e749eba4a7efe8094ee87e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3bdbf78c05b429baa9865049d3fa24d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82789f95bfe3423cbb3a25a1ef205669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a822574493f84e0aabc213ec46afd117":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86dd7f0cba3b48699084fdf2a96247c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e05cc23f121641fdad20db5e6de206b2","IPY_MODEL_41714b086cdb4f8bb952d88cb7e2dd40","IPY_MODEL_b1d524c540874729af68aa497e7357d4"],"layout":"IPY_MODEL_cf2c8651dd694a59a1807eac671c3206"}},"e05cc23f121641fdad20db5e6de206b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c63101885884b3791c41bb36e7802fe","placeholder":"​","style":"IPY_MODEL_996bda6fdef94f57b80913e213f1b73a","value":"model.safetensors: 100%"}},"41714b086cdb4f8bb952d88cb7e2dd40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ccdfe10c1ca40cc91227821740aee15","max":3132668804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50d3bf0bf12c4237bf55e327c330dadd","value":3132668804}},"b1d524c540874729af68aa497e7357d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd796a699654bdfb1de9a3370410f2f","placeholder":"​","style":"IPY_MODEL_b1d6d2b3220b48d1ae08f1de92cb4694","value":" 3.13G/3.13G [00:25&lt;00:00, 127MB/s]"}},"cf2c8651dd694a59a1807eac671c3206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c63101885884b3791c41bb36e7802fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"996bda6fdef94f57b80913e213f1b73a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ccdfe10c1ca40cc91227821740aee15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50d3bf0bf12c4237bf55e327c330dadd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecd796a699654bdfb1de9a3370410f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d6d2b3220b48d1ae08f1de92cb4694":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21499c6654e84747836989f8a5cb3341":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75c39fe1fc95439db2c05780d17a670d","IPY_MODEL_b8303f3f282346c59ac54fedc1f0a869","IPY_MODEL_cf0af215c75042ffac849cffc787f94f"],"layout":"IPY_MODEL_e6f057248c714a9b8b3f666bc86d38ec"}},"75c39fe1fc95439db2c05780d17a670d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c57704e3fe64a339407c15eddc5d679","placeholder":"​","style":"IPY_MODEL_0457d42601f1487db124a6749420afdc","value":"generation_config.json: 100%"}},"b8303f3f282346c59ac54fedc1f0a869":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_834bf2ced6ab42e39b2d6a762d60da14","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd1f0d79716e4838920062323f13a55a","value":147}},"cf0af215c75042ffac849cffc787f94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_699878b8d2df4b03b7b2395c61b58027","placeholder":"​","style":"IPY_MODEL_b732d26a7e554eb9b18c319b34b8607b","value":" 147/147 [00:00&lt;00:00, 22.1kB/s]"}},"e6f057248c714a9b8b3f666bc86d38ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c57704e3fe64a339407c15eddc5d679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0457d42601f1487db124a6749420afdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"834bf2ced6ab42e39b2d6a762d60da14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd1f0d79716e4838920062323f13a55a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"699878b8d2df4b03b7b2395c61b58027":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b732d26a7e554eb9b18c319b34b8607b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e94162eac57544aa9f8c10c79771b05f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a9d9eb6e754379b87811834e760d5d","IPY_MODEL_2384453afe254b518e30ec3c2fc94952","IPY_MODEL_569ab38150214f4aad9ef019d4f98b52"],"layout":"IPY_MODEL_1ea083a00bb34126848df779768c8972"}},"96a9d9eb6e754379b87811834e760d5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8de0d17a9e2e416f86b2552860fadf81","placeholder":"​","style":"IPY_MODEL_20230f87c44f43dea784a6a51d3ed7c4","value":"tokenizer_config.json: 100%"}},"2384453afe254b518e30ec3c2fc94952":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_655d1fcfdda1461a80252468a57bfc50","max":2539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e274f5418954492a98154057e059592","value":2539}},"569ab38150214f4aad9ef019d4f98b52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2039bc802db749a49e50d3d6b084a2bf","placeholder":"​","style":"IPY_MODEL_f65d6fb15bc34937b64ca3ef20388571","value":" 2.54k/2.54k [00:00&lt;00:00, 399kB/s]"}},"1ea083a00bb34126848df779768c8972":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8de0d17a9e2e416f86b2552860fadf81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20230f87c44f43dea784a6a51d3ed7c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"655d1fcfdda1461a80252468a57bfc50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e274f5418954492a98154057e059592":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2039bc802db749a49e50d3d6b084a2bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f65d6fb15bc34937b64ca3ef20388571":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8286ff3f92a942068ffc6be599ae5064":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23e71c6333624c44b31a9141585ef837","IPY_MODEL_6ef7e66074ff4b959d8168c4595d36ca","IPY_MODEL_ee7ad369e2a549b8aceb2520a33222b6"],"layout":"IPY_MODEL_82cff09161984fb8b30189706a1f2b98"}},"23e71c6333624c44b31a9141585ef837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a00e4c2a89aa4419a11e1178c19eb45b","placeholder":"​","style":"IPY_MODEL_8b38eed874dc497f8c337d1d2cb7b799","value":"spiece.model: 100%"}},"6ef7e66074ff4b959d8168c4595d36ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f64c23c26a8f4e48b07c79b79667b7fc","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6107a2ee9b84ed78ba733f506d6f39b","value":791656}},"ee7ad369e2a549b8aceb2520a33222b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74b6aedf17fd4759a6b4487a5bf0b40f","placeholder":"​","style":"IPY_MODEL_4fa6657de7894c898eb322f0f19b950b","value":" 792k/792k [00:00&lt;00:00, 49.0MB/s]"}},"82cff09161984fb8b30189706a1f2b98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a00e4c2a89aa4419a11e1178c19eb45b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b38eed874dc497f8c337d1d2cb7b799":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f64c23c26a8f4e48b07c79b79667b7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6107a2ee9b84ed78ba733f506d6f39b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74b6aedf17fd4759a6b4487a5bf0b40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa6657de7894c898eb322f0f19b950b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b315f15b7a894d83bb8e31f52931276b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_395239a18aad4603baf4faac7b521161","IPY_MODEL_3b27c3ed61a44a31982dcd720ff42453","IPY_MODEL_367649468c3c40bcb88eb821a229ee93"],"layout":"IPY_MODEL_80ae2fe68489447bbb365affc976a15e"}},"395239a18aad4603baf4faac7b521161":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc96ba90e41d4dd18863eeefd840502b","placeholder":"​","style":"IPY_MODEL_66ab0df0c6db4ea18eeaaefbf6229987","value":"tokenizer.json: 100%"}},"3b27c3ed61a44a31982dcd720ff42453":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_544c49ae0d8c4835a8cae7a75e8ed6e3","max":2424064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bcc2edcbd0944e1a0b0853bad38524d","value":2424064}},"367649468c3c40bcb88eb821a229ee93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ce865046724eb286594c902afe5861","placeholder":"​","style":"IPY_MODEL_9cc9846554d046a78b80dfd8edffa91d","value":" 2.42M/2.42M [00:00&lt;00:00, 31.2MB/s]"}},"80ae2fe68489447bbb365affc976a15e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc96ba90e41d4dd18863eeefd840502b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ab0df0c6db4ea18eeaaefbf6229987":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"544c49ae0d8c4835a8cae7a75e8ed6e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcc2edcbd0944e1a0b0853bad38524d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41ce865046724eb286594c902afe5861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cc9846554d046a78b80dfd8edffa91d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07ed84259ec64b2bb95b76a8010bc6bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2099dae21b44c09a78b947f618666ef","IPY_MODEL_5789d402bea1468f94fbd1069fe6d4cc","IPY_MODEL_e3a41a8786e24aad8890e96e1a8bfe6f"],"layout":"IPY_MODEL_07c81bd2c5534e4a9b8487170993c95d"}},"e2099dae21b44c09a78b947f618666ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4734d02ab98f4e33a86d4962e3bf167a","placeholder":"​","style":"IPY_MODEL_f211367c35ad4a998bbc2199a9bc7c11","value":"special_tokens_map.json: 100%"}},"5789d402bea1468f94fbd1069fe6d4cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bddde1a956f7446692d70dae4344d79e","max":2201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb41bc8dc71d4a4890249d7933d34098","value":2201}},"e3a41a8786e24aad8890e96e1a8bfe6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcd8d3a102f844d49ccbe4a8071adf51","placeholder":"​","style":"IPY_MODEL_11ddbc6a2fe945718a84ef589eff3a40","value":" 2.20k/2.20k [00:00&lt;00:00, 334kB/s]"}},"07c81bd2c5534e4a9b8487170993c95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4734d02ab98f4e33a86d4962e3bf167a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f211367c35ad4a998bbc2199a9bc7c11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bddde1a956f7446692d70dae4344d79e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb41bc8dc71d4a4890249d7933d34098":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcd8d3a102f844d49ccbe4a8071adf51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11ddbc6a2fe945718a84ef589eff3a40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11481618,"sourceType":"datasetVersion","datasetId":7196190}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opencv-python numpy ultralytics transformers pillow matplotlib seaborn pandas torch reportlab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDQ9BsGKsEH7","outputId":"bfb4a623-d090-4eef-9386-dd1c38729b5b","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:29:27.822572Z","iopub.execute_input":"2025-04-20T07:29:27.823009Z","iopub.status.idle":"2025-04-20T07:30:43.698557Z","shell.execute_reply.started":"2025-04-20T07:29:27.822986Z","shell.execute_reply":"2025-04-20T07:30:43.697804Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting ultralytics\n  Downloading ultralytics-8.3.111-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting reportlab\n  Downloading reportlab-4.4.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading ultralytics-8.3.111-py3-none-any.whl (978 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.8/978.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading reportlab-4.4.0-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: reportlab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 reportlab-4.4.0 ultralytics-8.3.111 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T06:50:17.470437Z","iopub.execute_input":"2025-04-20T06:50:17.471246Z","iopub.status.idle":"2025-04-20T06:50:38.563833Z","shell.execute_reply.started":"2025-04-20T06:50:17.471206Z","shell.execute_reply":"2025-04-20T06:50:38.562914Z"}},"outputs":[{"name":"stderr","text":"2025-04-20 06:50:20.867993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745131821.315320      74 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745131821.448410      74 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom transformers import pipeline\nfrom PIL import Image\nimport re\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport torch\nimport gc\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib import colors\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image as ReportLabImage\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nimport uuid\n\n# Clear any lingering GPU memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()\n\n# Verify input files\nmodel_path = r\"/kaggle/input/test-file/best.pt\"\nvideo_path = r\"/kaggle/input/test-file/test_10s.mp4\"\nif not os.path.exists(model_path) or not os.path.exists(video_path):\n    print(\"Error: Model or video file not found.\")\n    exit()\n\n# Load YOLO model\ntry:\n    model = YOLO(model_path)\n    print(\"YOLO model loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading YOLO model: {e}\")\n    exit()\n\n# Video setup\ncap = cv2.VideoCapture(video_path)\nif not cap.isOpened():\n    print(f\"Error: Could not open video at {video_path}\")\n    exit()\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nif total_frames == 0:\n    print(\"Error: Video has no frames.\")\n    exit()\nmiddle_index = total_frames // 2\nfps = cap.get(cv2.CAP_PROP_FPS)\nselected_frame = None\npeak_frame = None\nvehicle_counts = {}\ntrack_id_to_class = {}\nframe_vehicle_counts = []\nframe_index = 0\nmax_vehicles = 0\nmax_frame_index = 0\nemergency_alerts = []\ntrack_positions = defaultdict(list)\naverage_speeds = {}\ncongestion_indices = []\n\n# Process video\nprint(\"Processing video...\")\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if not success:\n        break\n    frame = cv2.resize(frame, (700, 500))\n    results = model.track(frame, persist=True)\n    boxes = results[0].boxes.xyxy.cpu().numpy()\n    confidences = results[0].boxes.conf.cpu().numpy()\n    classes = results[0].boxes.cls.cpu().numpy().astype(int)\n    track_ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else []\n\n    # Unique vehicle counts and speed estimation\n    current_frame_counts = defaultdict(int)\n    for conf, cls, track_id, box in zip(confidences, classes, track_ids, boxes):\n        if conf < 0.5:\n            continue\n        label = results[0].names[cls]\n        if track_id not in track_id_to_class:\n            track_id_to_class[track_id] = label\n            vehicle_counts[label] = vehicle_counts.get(label, 0) + 1\n        current_frame_counts[label] += 1\n        # Track position for speed\n        center_x = (box[0] + box[2]) / 2\n        center_y = (box[1] + box[3]) / 2\n        track_positions[track_id].append((frame_index, center_x, center_y))\n\n    frame_vehicle_counts.append(dict(current_frame_counts))\n\n    # Emergency vehicle alerts\n    if current_frame_counts.get(\"Ambulance\", 0) > 1:\n        alert = f\"High ambulance activity at {frame_index/fps:.2f}s: {current_frame_counts['Ambulance']} ambulances\"\n        emergency_alerts.append(alert)\n        print(f\"Emergency alert: {alert}\")\n\n    # Congestion index\n    total_in_frame = sum(current_frame_counts.values())\n    congestion_index = total_in_frame / 5.0\n    congestion_indices.append(congestion_index)\n    if total_in_frame > max_vehicles:\n        max_vehicles = total_in_frame\n        max_frame_index = frame_index\n        peak_frame = results[0].plot()\n\n    # Save middle frame\n    if frame_index == middle_index:\n        selected_frame = results[0].plot()\n\n    frame_index += 1\n\ncap.release()\ncv2.destroyAllWindows()\nprint(f\"Video processing complete: {total_frames} frames processed.\")\n\n# Calculate average speeds\nprint(\"Calculating average speeds...\")\nfor track_id, positions in track_positions.items():\n    label = track_id_to_class[track_id]\n    if len(positions) < 2:\n        continue\n    total_speed = 0\n    count = 0\n    for i in range(1, len(positions)):\n        frame_diff = positions[i][0] - positions[i-1][0]\n        if frame_diff == 0:\n            continue\n        dx = positions[i][1] - positions[i-1][1]\n        dy = positions[i][2] - positions[i-1][2]\n        distance = np.sqrt(dx**2 + dy**2)\n        time = frame_diff / fps\n        speed = distance / time\n        total_speed += speed\n        count += 1\n    if count > 0:\n        avg_speed = total_speed / count\n        average_speeds[label] = average_speeds.get(label, 0) + avg_speed\n        average_speeds[f\"{label}_count\"] = average_speeds.get(f\"{label}_count\", 0) + 1\n\nfor label in vehicle_counts.keys():\n    count_key = f\"{label}_count\"\n    if count_key in average_speeds:\n        average_speeds[label] = average_speeds[label] / average_speeds[count_key]\n        del average_speeds[count_key]\n\n# Save annotated frames\ntry:\n    if selected_frame is not None:\n        cv2.imwrite(\"middle_frame.jpg\", selected_frame)\n        print(\"Middle frame saved as 'middle_frame.jpg'.\")\n    else:\n        print(\"Warning: Middle frame not saved.\")\n    if peak_frame is not None:\n        cv2.imwrite(\"peak_frame.jpg\", peak_frame)\n        print(\"Peak frame saved as 'peak_frame.jpg'.\")\n    else:\n        print(\"Warning: Peak frame not saved.\")\nexcept Exception as e:\n    print(f\"Error saving frames: {e}\")\n\n# Convert middle frame to PIL\nif selected_frame is not None:\n    selected_frame_rgb = cv2.cvtColor(selected_frame, cv2.COLOR_BGR2RGB)\n    selected_frame_pil = Image.fromarray(selected_frame_rgb)\nelse:\n    print(\"Warning: No middle frame selected for PIL conversion.\")\n    selected_frame_pil = None\n\n# Load models for ensemble pipeline\nprint(\"Loading language models...\")\ntry:\n    flan_t5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n    print(\"Flan-T5-Large loaded.\")\nexcept Exception as e:\n    print(f\"Error loading Flan-T5-Large: {e}\")\n    flan_t5 = None\n\ntry:\n    bart = pipeline(\"summarization\", model=\"facebook/bart-large\")\n    print(\"BART-Large loaded for creative summaries.\")\nexcept Exception as e:\n    print(f\"Error loading BART-Large: {e}\")\n    bart = None\n\n# Calculate averages and congestion\naverage_counts = {}\nfor vtype in vehicle_counts.keys():\n    total = sum(frame_counts.get(vtype, 0) for frame_counts in frame_vehicle_counts)\n    average_counts[vtype] = total / len(frame_vehicle_counts) if frame_vehicle_counts else 0\nmax_time_sec = max_frame_index / fps if fps > 0 else 0\naverage_congestion = np.mean(congestion_indices) if congestion_indices else 0\n\n# Generate traffic density heatmap\nprint(\"Generating traffic density heatmap...\")\ntimes = [i / fps for i in range(len(frame_vehicle_counts))]\ntotal_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in frame_vehicle_counts]\nplt.figure(figsize=(10, 4))\nsns.heatmap([total_vehicles_per_frame], cmap=\"YlOrRd\", xticklabels=50, cbar_kws={'label': 'Vehicle Count'})\nplt.xlabel(\"Time (seconds)\")\nplt.ylabel(\"Density\")\nplt.title(\"Traffic Density Heatmap\")\nplt.xticks(ticks=np.linspace(0, len(times)-1, 5), labels=[f\"{t:.1f}\" for t in np.linspace(0, max(times), 5)])\ntry:\n    plt.savefig(\"heatmap.png\")\n    plt.close()\n    print(\"Heatmap saved as 'heatmap.png'.\")\nexcept Exception as e:\n    print(f\"Error saving heatmap: {e}\")\n\n# Export to CSV\nprint(\"Exporting data to CSV...\")\ncsv_data = {\n    \"Frame\": list(range(len(frame_vehicle_counts))),\n    \"Time (s)\": times,\n    \"Total Vehicles\": total_vehicles_per_frame,\n    \"Congestion Index\": congestion_indices\n}\nfor vtype in vehicle_counts.keys():\n    csv_data[vtype] = [frame_counts.get(vtype, 0) for frame_counts in frame_vehicle_counts]\ndf = pd.DataFrame(csv_data)\ntry:\n    df.to_csv(\"traffic_data.csv\", index=False)\n    print(\"Traffic data exported to 'traffic_data.csv'.\")\nexcept Exception as e:\n    print(f\"Error exporting CSV: {e}\")\n\n# Enhanced context with temporal trends\ncontext = (\n    f\"- Video duration: {(total_frames / fps):.2f} seconds\\n\"\n    f\"- Frames analyzed: {total_frames}\\n\"\n    f\"- FPS: {fps:.2f}\\n\"\n    f\"- Unique vehicles detected: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])} (total: {sum(vehicle_counts.values())})\\n\"\n    f\"- Average vehicles per frame: {', '.join([f'{v}: {c:.2f}' for v, c in average_counts.items()])}\\n\"\n    f\"- Average speeds (pixels/s): {', '.join([f'{v}: {s:.2f}' for v, s in average_speeds.items()])}\\n\"\n    f\"- Peak traffic: {max_vehicles} vehicles at {max_time_sec:.2f} seconds\\n\"\n    f\"- Middle frame (at {(middle_index / fps):.2f} seconds): {', '.join([f'{v}: {c}' for v, c in frame_vehicle_counts[middle_index].items()]) if middle_index < len(frame_vehicle_counts) else 'no vehicles'}\\n\"\n    f\"- Traffic condition: {'Heavy' if sum(vehicle_counts.values()) > 30 else 'Moderate' if sum(vehicle_counts.values()) > 15 else 'Light'}\\n\"\n    f\"- Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high)\\n\"\n    f\"- Temporal trends:\\n\"\n    f\"  - First 25% of video: {sum(sum(fc.values()) for fc in frame_vehicle_counts[:len(frame_vehicle_counts)//4])} vehicles\\n\"\n    f\"  - Middle 50% of video: {sum(sum(fc.values()) for fc in frame_vehicle_counts[len(frame_vehicle_counts)//4:3*len(frame_vehicle_counts)//4])} vehicles\\n\"\n    f\"  - Last 25% of video: {sum(sum(fc.values()) for fc in frame_vehicle_counts[3*len(frame_vehicle_counts)//4:])} vehicles\\n\"\n    f\"{'- Emergency alerts: ' + '; '.join(emergency_alerts) if emergency_alerts else ''}\"\n)\n\n# Generate text report\nprint(\"Generating text report...\")\nreport_prompt = (\n    f\"Vehicle Detection Report:\\n\"\n    f\"Unique vehicle counts across all frames:\\n\"\n    + \"\\n\".join([f\"{vtype}: {count}\" for vtype, count in vehicle_counts.items()]) + \"\\n\"\n    f\"Additional Insights:\\n\"\n    f\"Average vehicles per frame:\\n\"\n    + \"\\n\".join([f\"{vtype}: {avg:.2f}\" for vtype, avg in average_counts.items()]) + \"\\n\"\n    f\"Average speeds (pixels/s):\\n\"\n    + \"\\n\".join([f\"{vtype}: {speed:.2f}\" for vtype, speed in average_speeds.items()]) + \"\\n\"\n    f\"Peak traffic at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\\n\"\n    f\"Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high).\\n\"\n    f\"Temporal trends:\\n\"\n    f\"  - First 25% of video: {sum(sum(fc.values()) for fc in frame_vehicle_counts[:len(frame_vehicle_counts)//4])} vehicles\\n\"\n    f\"  - Middle 50% of video: {sum(sum(fc.values()) for fc in frame_vehicle_counts[len(frame_vehicle_counts)//4:3*len(frame_vehicle_counts)//4])} vehicles\\n\"\n    f\"  - Last 25% of video: {sum(sum(fc.values()) for fc in frame_vehicle_counts[3*len(frame_vehicle_counts)//4:])} vehicles\\n\"\n)\nif emergency_alerts:\n    report_prompt += \"Emergency Alerts:\\n\" + \"\\n\".join(emergency_alerts) + \"\\n\"\nif frame_vehicle_counts and middle_index < len(frame_vehicle_counts):\n    selected_counts = frame_vehicle_counts[middle_index]\n    report_prompt += \"Selected middle frame shows:\\n\"\n    for vtype, count in selected_counts.items():\n        report_prompt += f\"{vtype}: {count}\\n\"\nreport_prompt += (\n    \"Generate a detailed summary based on this data. \"\n    \"Start with 'The scene appears to be...' and describe the traffic conditions, vehicle types, trends, and possible implications. \"\n    \"Conclude with a paragraph on the report's insights and their potential use for traffic planning or urban design.\"\n)\n\ntry:\n    if flan_t5:\n        result = flan_t5(report_prompt, max_new_tokens=300)\n        report_text = result[0]['generated_text']\n        print(\"Text report generated with Flan-T5.\")\n        if bart:\n            summary_prompt = f\"Summarize the following report creatively, focusing on the scene and implications:\\n{report_text}\"\n            bart_result = bart(summary_prompt, max_length=200)\n            report_text = bart_result[0]['summary_text']\n            print(\"Report enhanced with BART summarization.\")\n    else:\n        report_text = (\n            f\"Vehicle Detection Report:\\n\"\n            f\"Unique vehicle counts across all frames:\\n\"\n            + \"\\n\".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()]) + \"\\n\"\n            f\"Additional Insights:\\n\"\n            f\"Average vehicles per frame:\\n\"\n            + \"\\n\".join([f\"{v}: {c:.2f}\" for v, c in average_counts.items()]) + \"\\n\"\n            f\"Peak traffic at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\\n\"\n            f\"Selected middle frame shows:\\n\"\n            + \"\\n\".join([f\"{v}: {c}\" for v, c in frame_vehicle_counts[middle_index].items()]) + \"\\n\"\n            f\"The scene appears to be a typical urban intersection with {sum(frame_vehicle_counts[middle_index].values())} vehicles. \"\n            f\"Traffic is {'heavy' if sum(vehicle_counts.values()) > 30 else 'moderate' if sum(vehicle_counts.values()) > 15 else 'light'}. \"\n            f\"This report provides basic counts for analysis.\"\n        )\n        print(\"Warning: Text report generation failed, using fallback.\")\nexcept Exception as e:\n    print(f\"Report error: {e}\")\n    report_text = (\n        f\"Vehicle Detection Report:\\n\"\n        f\"Unique vehicle counts across all frames:\\n\"\n        + \"\\n\".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()]) + \"\\n\"\n        f\"Additional Insights:\\n\"\n        f\"Average vehicles per frame:\\n\"\n        + \"\\n\".join([f\"{v}: {c:.2f}\" for v, c in average_counts.items()]) + \"\\n\"\n        f\"Peak traffic at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\\n\"\n        f\"Selected middle frame shows:\\n\"\n        + \"\\n\".join([f\"{v}: {c}\" for v, c in frame_vehicle_counts[middle_index].items()]) + \"\\n\"\n        f\"The scene appears to be a typical urban intersection with {sum(frame_vehicle_counts[middle_index].values())} vehicles. \"\n        f\"Traffic is {'heavy' if sum(vehicle_counts.values()) > 30 else 'moderate' if sum(vehicle_counts.values()) > 15 else 'light'}. \"\n        f\"This report provides basic counts for analysis.\"\n    )\n    print(\"Warning: Text report generation failed, using fallback.\")\n\nprint(\"\\n--- Generated Report ---\\n\")\nprint(report_text)\n\n# Generate PDF report\nprint(\"Generating PDF report...\")\ntry:\n    pdf = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\n    styles = getSampleStyleSheet()\n    normal_style = ParagraphStyle(name='NormalWrap', parent=styles['Normal'], wordWrap='CJK')\n    heading_style = styles['Heading1']\n    subheading_style = styles['Heading2']\n    elements = []\n\n    # Title\n    elements.append(Paragraph(\"Vehicle Detection Report\", heading_style))\n    elements.append(Spacer(1, 0.2 * inch))\n\n    # Summary Report Section\n    elements.append(Paragraph(\"Summary Report\", subheading_style))\n    elements.append(Spacer(1, 0.1 * inch))\n    summary_lines = report_text.split('\\n')\n    for line in summary_lines:\n        elements.append(Paragraph(line, normal_style))\n    elements.append(Spacer(1, 0.2 * inch))\n\n    # Vehicle Statistics Table\n    elements.append(Paragraph(\"Vehicle Statistics\", subheading_style))\n    elements.append(Spacer(1, 0.1 * inch))\n    table_data = [['Vehicle Type', 'Unique Count', 'Avg/Frame', 'Middle Frame', 'Avg Speed (px/s)']]\n    for vtype in vehicle_counts.keys():\n        unique_count = vehicle_counts.get(vtype, 0)\n        avg_count = average_counts.get(vtype, 0)\n        middle_count = frame_vehicle_counts[middle_index].get(vtype, 0) if middle_index < len(frame_vehicle_counts) else 0\n        speed = average_speeds.get(vtype, 0)\n        table_data.append([vtype, str(unique_count), f\"{avg_count:.2f}\", str(middle_count), f\"{speed:.2f}\"])\n    table = Table(table_data)\n    table.setStyle(TableStyle([\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, 0), 10),\n        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n        ('GRID', (0, 0), (-1, -1), 1, colors.black)\n    ]))\n    elements.append(table)\n    elements.append(Spacer(1, 0.2 * inch))\n\n    # Key Insights\n    elements.append(Paragraph(\"Key Insights\", subheading_style))\n    elements.append(Spacer(1, 0.1 * inch))\n    elements.append(Paragraph(f\"Peak traffic occurred at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\", normal_style))\n    elements.append(Paragraph(f\"Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high).\", normal_style))\n    if emergency_alerts:\n        elements.append(Paragraph(\"Emergency Alerts:\", normal_style))\n        for alert in emergency_alerts:\n            elements.append(Paragraph(f\"- {alert}\", normal_style))\n    elements.append(Spacer(1, 0.2 * inch))\n\n    # Visualizations\n    elements.append(Paragraph(\"Visualizations\", subheading_style))\n    elements.append(Spacer(1, 0.1 * inch))\n    image_paths = [\n        (\"heatmap.png\", \"Traffic Density Heatmap\"),\n        (\"middle_frame.jpg\", \"Middle Frame\"),\n        (\"peak_frame.jpg\", \"Peak Traffic Frame\")\n    ]\n    for path, title in image_paths:\n        if os.path.exists(path):\n            try:\n                img = ReportLabImage(path, width=5*inch, height=3*inch)\n                elements.append(Paragraph(title, normal_style))\n                elements.append(img)\n                elements.append(Spacer(1, 0.1 * inch))\n            except Exception as e:\n                print(f\"Error adding image {path} to PDF: {e}\")\n                elements.append(Paragraph(f\"{title} not available.\", normal_style))\n                elements.append(Spacer(1, 0.1 * inch))\n        else:\n            print(f\"Warning: Image {path} not found.\")\n            elements.append(Paragraph(f\"{title} not available.\", normal_style))\n            elements.append(Spacer(1, 0.1 * inch))\n\n    # Note\n    elements.append(Paragraph(\"Note: This PDF contains the complete vehicle detection report.\", normal_style))\n\n    # Build PDF\n    pdf.build(elements)\n    print(\"PDF report generated as 'report.pdf'.\")\nexcept Exception as e:\n    print(f\"Error generating PDF: {e}\")\n\n# Updated answer_question function\ndef answer_question(question, vehicle_counts, frame_vehicle_counts, fps, total_frames, average_counts, max_time_sec, max_vehicles, context, flan_t5, bart, average_speeds, congestion_indices, emergency_alerts):\n    question = question.lower().strip()\n    if not question:\n        return \"Please ask a valid question.\"\n\n    # Time-range query handling\n    time_range_match = re.search(r\"from (\\d+(?:\\.\\d+)?)\\s*(?:to|s(?:econd)?s?\\s*to)\\s*(\\d+(?:\\.\\d+)?)\\s*s(?:econd)?s?\", question)\n    if time_range_match:\n        start_time = float(time_range_match.group(1))\n        end_time = float(time_range_match.group(2))\n        if start_time >= end_time or end_time > total_frames / fps:\n            return f\"Invalid time range: {start_time}s to {end_time}s. Video duration is {total_frames/fps:.2f}s.\"\n        start_frame = int(start_time * fps)\n        end_frame = min(int(end_time * fps), total_frames - 1)\n        range_counts = defaultdict(int)\n        for frame_counts in frame_vehicle_counts[start_frame:end_frame + 1]:\n            for vtype, count in frame_counts.items():\n                range_counts[vtype] += count\n        counts_str = \", \".join([f\"{count} {vtype}(s)\" for vtype, count in range_counts.items()])\n        return f\"From {start_time}s to {end_time}s: {counts_str or 'no vehicles'}.\"\n\n    # Specific time query\n    time_match = re.search(r\"at (\\d+(?:\\.\\d+)?) seconds?\", question)\n    if time_match:\n        time_sec = float(time_match.group(1))\n        frame_index = min(int(time_sec * fps), total_frames - 1)\n        if frame_index >= total_frames:\n            return f\"Time {time_sec}s exceeds video duration ({total_frames/fps:.2f}s).\"\n        frame_counts = frame_vehicle_counts[frame_index]\n        for vtype in vehicle_counts.keys():\n            if vtype.lower() in question:\n                count = frame_counts.get(vtype, 0)\n                return f\"At {time_sec} seconds, there were {count} {vtype}(s).\"\n        counts_str = \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n        return f\"At {time_sec} seconds: {counts_str or 'no vehicles'}.\"\n\n    # Other rule-based answers\n    if \"beginning\" in question or \"start\" in question:\n        frame_counts = frame_vehicle_counts[0]\n        counts_str = \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n        return f\"At the start: {counts_str or 'no vehicles'}.\"\n\n    if \"end\" in question or \"last\" in question:\n        frame_counts = frame_vehicle_counts[-1]\n        counts_str = \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n        return f\"At the end: {counts_str or 'no vehicles'}.\"\n\n    if \"middle frame\" in question or \"middle of the video\" in question:\n        frame_counts = frame_vehicle_counts[middle_index]\n        counts_str = \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n        return f\"In the middle frame at {middle_index/fps:.2f} seconds: {counts_str or 'no vehicles'}.\"\n\n    if \"peak\" in question and \"time\" in question:\n        return f\"Peak traffic was at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\"\n\n    if \"types of vehicles\" in question or \"vehicle types\" in question:\n        types = list(vehicle_counts.keys())\n        types_str = \", \".join(types[:-1]) + \" and \" + types[-1] if len(types) > 1 else types[0]\n        return f\"Vehicle types detected: {types_str}.\"\n\n    if \"average speed\" in question:\n        for vtype in vehicle_counts.keys():\n            if vtype.lower() in question:\n                speed = average_speeds.get(vtype, 0)\n                return f\"The average speed of {vtype}s was {speed:.2f} pixels per second.\"\n        speeds_str = \", \".join([f\"{vtype}: {speed:.2f} pixels/s\" for vtype, speed in average_speeds.items()])\n        return f\"Average speeds: {speeds_str or 'none'}.\"\n\n    if \"average\" in question:\n        for vtype in vehicle_counts.keys():\n            if vtype.lower() in question:\n                avg = average_counts.get(vtype, 0)\n                return f\"Average {vtype}s per frame: {avg:.2f}.\"\n        averages_str = \", \".join([f\"{vtype}: {avg:.2f}\" for vtype, avg in average_counts.items()])\n        return f\"Average vehicles per frame: {averages_str or 'none'}.\"\n\n    if \"how many\" in question and \"total\" in question:\n        total = sum(vehicle_counts.values())\n        return f\"Total unique vehicles detected: {total}.\"\n\n    if \"congestion\" in question or \"congested\" in question:\n        level = \"high\" if average_congestion > 2 else \"moderate\" if average_congestion > 1 else \"low\"\n        return f\"The average congestion index was {average_congestion:.2f}, indicating {level} congestion.\"\n\n    if \"emergency\" in question or \"alerts\" in question:\n        return f\"Emergency alerts: {'; '.join(emergency_alerts) if emergency_alerts else 'None detected'}.\"\n\n    # Ensemble model pipeline for creative/exploratory questions\n    is_exploratory = any(kw in question for kw in [\"describe\", \"tell me\", \"what can you say\", \"summarize\", \"activity\", \"explain\"])\n    is_hypothetical = any(kw in question for kw in [\"would\", \"might\", \"could\", \"cause\"])\n\n    prompt = f\"Context:\\n{context}\\nQuestion: {question}\\n\"\n\n    if is_exploratory:\n        prompt += (\n            \"Instruction: Provide a detailed and creative description of the traffic scene based on the provided data. \"\n            \"Mention vehicle types, counts, speeds, temporal trends, and any notable events (e.g., emergency alerts). \"\n            \"Start with 'The video depicts...' and weave a narrative that paints a vivid picture of the scene.\\nAnswer:\"\n        )\n        try:\n            if flan_t5:\n                flan_response = flan_t5(prompt, max_new_tokens=200)[0]['generated_text'].strip()\n                if bart:\n                    bart_prompt = f\"Summarize and enhance this description creatively:\\n{flan_response}\"\n                    response = bart(bart_prompt, max_length=150)[0]['summary_text'].strip()\n                else:\n                    response = flan_response\n            else:\n                response = \"Chatbot unavailable. Summary: \" + \", \".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()]) + \".\"\n        except Exception as e:\n            response = f\"Error: {e}. Summary: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])}.\"\n    elif is_hypothetical:\n        prompt += (\n            \"Instruction: Speculate on possible causes or implications of the traffic data. \"\n            \"Discuss potential reasons for peak traffic, emergency alerts, or how this data could inform traffic management or urban planning. \"\n            \"Start with 'Based on the data...' and provide insightful analysis.\\nAnswer:\"\n        )\n        try:\n            if flan_t5:\n                flan_response = flan_t5(prompt, max_new_tokens=200)[0]['generated_text'].strip()\n                if bart:\n                    bart_prompt = f\"Summarize and enhance this analysis creatively:\\n{flan_response}\"\n                    response = bart(bart_prompt, max_length=150)[0]['summary_text'].strip()\n                else:\n                    response = flan_response\n            else:\n                response = \"Chatbot unavailable. Summary: \" + \", \".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()]) + \".\"\n        except Exception as e:\n            response = f\"Error: {e}. Summary: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])}.\"\n    else:\n        prompt += \"Instruction: Answer the question concisely using the provided data. If the question cannot be answered, state that.\\nAnswer:\"\n        try:\n            if flan_t5:\n                response = flan_t5(prompt, max_new_tokens=100)[0]['generated_text'].strip()\n            else:\n                response = \"Chatbot unavailable. Summary: \" + \", \".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()]) + \".\"\n        except Exception as e:\n            response = f\"Error: {e}. Summary: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])}.\"\n\n    return response\n\n# Dynamic chat suggestions\nsuggestions = [\n    \"How many cars were detected in total?\",\n    \"How many ambulances were in the video?\",\n    \"How many vehicles from 2 to 5 seconds?\",\n    \"What was the peak traffic time?\",\n    \"What types of vehicles were detected?\",\n    \"Describe the traffic in the video.\",\n    \"Explain the traffic flow and trends.\",\n    \"What was happening in the middle of the video?\",\n    \"What might have caused the peak traffic?\",\n    \"How could this data help traffic management?\",\n    \"What was the average speed of cars?\",\n    \"How congested was the traffic?\",\n    \"Summarize the vehicle activity in the video.\"\n]\nif emergency_alerts:\n    suggestions.append(\"When were ambulances most active?\")\n\n# Chat loop\nprint(\"\\nStarting chat interaction...\")\nprint(\"Video processing complete. Ask any question about the video (type 'exit' to quit).\")\nprint(\"Suggested questions:\")\nfor i, suggestion in enumerate(suggestions, 1):\n    print(f\"{i}. {suggestion}\")\nwhile True:\n    try:\n        user_query = input(\"Ask a question: \")\n        if user_query.lower().strip() == \"exit\":\n            print(\"Goodbye.\")\n            break\n        response = answer_question(user_query, vehicle_counts, frame_vehicle_counts, fps, total_frames, average_counts, max_time_sec, max_vehicles, context, flan_t5, bart, average_speeds, congestion_indices, emergency_alerts)\n        print(\"\\nAnswer:\")\n        print(response)\n        print()\n    except KeyboardInterrupt:\n        print(\"\\nChat interrupted. Type 'exit' to quit or continue asking questions.\")\n    except Exception as e:\n        print(f\"Error in chat loop: {e}\")\n        print(\"Please try again or type 'exit' to quit.\")\n\n# Cleanup\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"kCmBvaKHaOjq","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ce272bd753214c4581a663d1352de470","fec4f3a6eec24ee28c5f14b4b1a4af51","db18797e4b6648c888328a7a090d5ead","ffb5dc2afc5d4ea797503b55c1f60d72","ae9f0e3238f4441dac8fb7409fe7e9ca","098716c4cb9c417fbb4300f953b0611d","ee4e7b3f53a947c8a83e0c1f5120b88a","3c72237289e749eba4a7efe8094ee87e","b3bdbf78c05b429baa9865049d3fa24d","82789f95bfe3423cbb3a25a1ef205669","a822574493f84e0aabc213ec46afd117","86dd7f0cba3b48699084fdf2a96247c9","e05cc23f121641fdad20db5e6de206b2","41714b086cdb4f8bb952d88cb7e2dd40","b1d524c540874729af68aa497e7357d4","cf2c8651dd694a59a1807eac671c3206","1c63101885884b3791c41bb36e7802fe","996bda6fdef94f57b80913e213f1b73a","4ccdfe10c1ca40cc91227821740aee15","50d3bf0bf12c4237bf55e327c330dadd","ecd796a699654bdfb1de9a3370410f2f","b1d6d2b3220b48d1ae08f1de92cb4694","21499c6654e84747836989f8a5cb3341","75c39fe1fc95439db2c05780d17a670d","b8303f3f282346c59ac54fedc1f0a869","cf0af215c75042ffac849cffc787f94f","e6f057248c714a9b8b3f666bc86d38ec","1c57704e3fe64a339407c15eddc5d679","0457d42601f1487db124a6749420afdc","834bf2ced6ab42e39b2d6a762d60da14","cd1f0d79716e4838920062323f13a55a","699878b8d2df4b03b7b2395c61b58027","b732d26a7e554eb9b18c319b34b8607b","e94162eac57544aa9f8c10c79771b05f","96a9d9eb6e754379b87811834e760d5d","2384453afe254b518e30ec3c2fc94952","569ab38150214f4aad9ef019d4f98b52","1ea083a00bb34126848df779768c8972","8de0d17a9e2e416f86b2552860fadf81","20230f87c44f43dea784a6a51d3ed7c4","655d1fcfdda1461a80252468a57bfc50","6e274f5418954492a98154057e059592","2039bc802db749a49e50d3d6b084a2bf","f65d6fb15bc34937b64ca3ef20388571","8286ff3f92a942068ffc6be599ae5064","23e71c6333624c44b31a9141585ef837","6ef7e66074ff4b959d8168c4595d36ca","ee7ad369e2a549b8aceb2520a33222b6","82cff09161984fb8b30189706a1f2b98","a00e4c2a89aa4419a11e1178c19eb45b","8b38eed874dc497f8c337d1d2cb7b799","f64c23c26a8f4e48b07c79b79667b7fc","a6107a2ee9b84ed78ba733f506d6f39b","74b6aedf17fd4759a6b4487a5bf0b40f","4fa6657de7894c898eb322f0f19b950b","b315f15b7a894d83bb8e31f52931276b","395239a18aad4603baf4faac7b521161","3b27c3ed61a44a31982dcd720ff42453","367649468c3c40bcb88eb821a229ee93","80ae2fe68489447bbb365affc976a15e","bc96ba90e41d4dd18863eeefd840502b","66ab0df0c6db4ea18eeaaefbf6229987","544c49ae0d8c4835a8cae7a75e8ed6e3","3bcc2edcbd0944e1a0b0853bad38524d","41ce865046724eb286594c902afe5861","9cc9846554d046a78b80dfd8edffa91d","07ed84259ec64b2bb95b76a8010bc6bd","e2099dae21b44c09a78b947f618666ef","5789d402bea1468f94fbd1069fe6d4cc","e3a41a8786e24aad8890e96e1a8bfe6f","07c81bd2c5534e4a9b8487170993c95d","4734d02ab98f4e33a86d4962e3bf167a","f211367c35ad4a998bbc2199a9bc7c11","bddde1a956f7446692d70dae4344d79e","fb41bc8dc71d4a4890249d7933d34098","fcd8d3a102f844d49ccbe4a8071adf51","11ddbc6a2fe945718a84ef589eff3a40"]},"outputId":"3cd9c60e-6e33-4d8e-9286-e3beb09d54ee","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:31:08.519032Z","iopub.execute_input":"2025-04-20T07:31:08.519361Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745134279.308554      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745134279.386150      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"YOLO model loaded successfully.\nProcessing video...\n\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\nCollecting lap>=0.5.12\n  Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from lap>=0.5.12) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->lap>=0.5.12) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nDownloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 30.9 MB/s eta 0:00:00\nInstalling collected packages: lap\nSuccessfully installed lap-0.5.12\n\n\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 3.1s, installed 1 package: ['lap>=0.5.12']\n\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n\n\n0: 480x640 1 Bus, 8 Cars, 38.6ms\nSpeed: 10.5ms preprocess, 38.6ms inference, 323.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.8ms\nSpeed: 2.4ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.9ms\nSpeed: 2.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.8ms\nSpeed: 2.4ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.7ms\nSpeed: 2.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 7.1ms\nSpeed: 2.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.6ms\nSpeed: 2.0ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.9ms\nSpeed: 2.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.9ms\nSpeed: 2.3ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 7.1ms\nSpeed: 2.4ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.1ms\nSpeed: 2.3ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 8 Cars, 6.8ms\nSpeed: 2.3ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.9ms\nSpeed: 2.4ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.5ms\nSpeed: 2.0ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 7.7ms\nSpeed: 2.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.7ms\nSpeed: 2.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 9 Cars, 6.4ms\nSpeed: 2.4ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.8ms\nSpeed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.6ms\nSpeed: 2.4ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.6ms\nSpeed: 2.8ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.3ms\nSpeed: 2.3ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.1ms\nSpeed: 2.2ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 7.0ms\nSpeed: 2.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.1ms\nSpeed: 2.3ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.3ms\nSpeed: 2.1ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.8ms\nSpeed: 2.5ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.0ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.5ms\nSpeed: 2.4ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.4ms\nSpeed: 2.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.6ms\nSpeed: 2.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.7ms\nSpeed: 2.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 10 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 7.0ms\nSpeed: 2.9ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.3ms\nSpeed: 2.3ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.1ms\nSpeed: 2.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.7ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.8ms\nSpeed: 2.5ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.2ms\nSpeed: 2.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.4ms\nSpeed: 2.3ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.4ms\nSpeed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.9ms\nSpeed: 2.2ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.9ms\nSpeed: 2.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.0ms\nSpeed: 2.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.2ms\nSpeed: 2.2ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.1ms\nSpeed: 2.4ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 7.0ms\nSpeed: 2.2ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 7.0ms\nSpeed: 2.3ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.3ms\nSpeed: 2.2ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.2ms\nSpeed: 2.4ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.8ms\nSpeed: 2.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.7ms\nSpeed: 3.1ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.7ms\nSpeed: 2.4ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.6ms\nSpeed: 2.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.1ms\nSpeed: 2.3ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.3ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 13.3ms\nSpeed: 2.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 11.2ms\nSpeed: 2.7ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.1ms\nSpeed: 2.1ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.8ms\nSpeed: 2.3ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.4ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 7.4ms\nSpeed: 2.1ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.1ms\nSpeed: 2.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 6.2ms\nSpeed: 2.1ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 6.1ms\nSpeed: 2.2ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 8.0ms\nSpeed: 3.1ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 7.1ms\nSpeed: 2.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 6.7ms\nSpeed: 2.4ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.1ms\nSpeed: 2.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.8ms\nSpeed: 2.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.7ms\nSpeed: 2.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.0ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.3ms\nSpeed: 2.3ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.9ms\nSpeed: 2.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.8ms\nSpeed: 2.4ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.4ms\nSpeed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.4ms\nSpeed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.3ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.0ms\nSpeed: 2.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.4ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.8ms\nSpeed: 2.3ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 7.7ms\nSpeed: 2.2ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.6ms\nSpeed: 2.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.8ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.3ms\nSpeed: 2.3ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.2ms\nSpeed: 2.4ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.3ms\nSpeed: 2.1ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.6ms\nSpeed: 2.0ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.4ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.0ms\nSpeed: 2.3ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.4ms\nSpeed: 2.0ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.9ms\nSpeed: 2.1ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.1ms\nSpeed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 12.2ms\nSpeed: 2.0ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.8ms\nSpeed: 2.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.7ms\nSpeed: 2.3ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.8ms\nSpeed: 2.2ms preprocess, 6.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.2ms\nSpeed: 2.4ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.9ms\nSpeed: 2.2ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.1ms\nSpeed: 2.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.0ms\nSpeed: 2.4ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.3ms\nSpeed: 2.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 13 Cars, 7.0ms\nSpeed: 2.4ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.9ms\nSpeed: 2.2ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.1ms\nSpeed: 2.4ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.0ms\nSpeed: 2.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.6ms\nSpeed: 2.4ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.1ms\nSpeed: 2.2ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.4ms\nSpeed: 2.2ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 7.3ms\nSpeed: 2.2ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.8ms\nSpeed: 2.3ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.7ms\nSpeed: 2.2ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.6ms\nSpeed: 2.3ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.0ms\nSpeed: 2.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 8.2ms\nSpeed: 3.0ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.3ms\nSpeed: 2.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.8ms\nSpeed: 2.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.0ms\nSpeed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.0ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.3ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.8ms\nSpeed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.3ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 2 Buss, 11 Cars, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.9ms\nSpeed: 2.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.2ms\nSpeed: 2.4ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.1ms\nSpeed: 2.3ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.4ms\nSpeed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.4ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.3ms\nSpeed: 2.3ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.9ms\nSpeed: 3.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 2 Buss, 11 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 2 Buss, 11 Cars, 6.7ms\nSpeed: 2.0ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 8.0ms\nSpeed: 2.6ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.7ms\nSpeed: 2.4ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.4ms\nSpeed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.3ms\nSpeed: 2.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.5ms\nSpeed: 2.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.4ms\nSpeed: 2.0ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.4ms\nSpeed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.8ms\nSpeed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.8ms\nSpeed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.8ms\nSpeed: 2.6ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.2ms\nSpeed: 2.3ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.4ms\nSpeed: 2.3ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.8ms\nSpeed: 2.1ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.3ms\nSpeed: 2.2ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.0ms\nSpeed: 2.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 11 Cars, 6.2ms\nSpeed: 2.2ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 7.1ms\nSpeed: 2.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 7.0ms\nSpeed: 2.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 7.7ms\nSpeed: 2.3ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 6.7ms\nSpeed: 2.1ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 6.6ms\nSpeed: 2.3ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 7.1ms\nSpeed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 6.6ms\nSpeed: 2.2ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 10 Cars, 7.6ms\nSpeed: 2.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.9ms\nSpeed: 2.8ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.2ms\nSpeed: 2.2ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.0ms\nSpeed: 2.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.4ms\nSpeed: 2.3ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 7.3ms\nSpeed: 2.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.7ms\nSpeed: 2.4ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.9ms\nSpeed: 2.2ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.7ms\nSpeed: 2.2ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.6ms\nSpeed: 2.3ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.4ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 8.5ms\nSpeed: 2.9ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.7ms\nSpeed: 2.5ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.9ms\nSpeed: 2.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 7.0ms\nSpeed: 2.2ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.4ms\nSpeed: 2.0ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.5ms\nSpeed: 2.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.3ms\nSpeed: 2.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 14 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 14 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 7.1ms\nSpeed: 2.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 7.1ms\nSpeed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.2ms\nSpeed: 2.4ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 14 Cars, 6.7ms\nSpeed: 2.2ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 15 Cars, 6.6ms\nSpeed: 2.1ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 15 Cars, 6.7ms\nSpeed: 2.2ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 15 Cars, 6.4ms\nSpeed: 2.4ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 15 Cars, 6.8ms\nSpeed: 2.3ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 15 Cars, 6.7ms\nSpeed: 2.2ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.0ms\nSpeed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 13 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 13 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.8ms\nSpeed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 6.7ms\nSpeed: 2.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 15 Cars, 7.2ms\nSpeed: 2.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 13 Cars, 7.2ms\nSpeed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 13 Cars, 6.5ms\nSpeed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 13 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 13 Cars, 7.0ms\nSpeed: 2.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Bus, 12 Cars, 6.5ms\nSpeed: 2.3ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 7.0ms\nSpeed: 2.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.9ms\nSpeed: 2.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.6ms\nSpeed: 2.0ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.3ms\nSpeed: 2.2ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 7.3ms\nSpeed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 6.5ms\nSpeed: 2.1ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 8.5ms\nSpeed: 2.8ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 12 Cars, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 6.6ms\nSpeed: 2.3ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 1 Van, 11 Cars, 7.1ms\nSpeed: 2.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\nVideo processing complete: 301 frames processed.\nCalculating average speeds...\nMiddle frame saved as 'middle_frame.jpg'.\nPeak frame saved as 'peak_frame.jpg'.\nLoading language models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b515f352cfb4432b2277e34b4b9dc32"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e2fb283e65f44d0bfdf984d7384a8bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e36acf9f907493dad4832f826d2870b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0525e544e54d16947183c7c29a37f4"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58bd86197e454f6fa521aab0c87414eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ba017891214934a2efe5d2c64cddc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88256730ff534e61b818033d27255081"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Flan-T5-Large loaded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd6ce00ae73442d2b140628a98bb4a74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86f19972a2c44eeaaa88a09b51a8aa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e76bb6c75ab4761beceb6a51f43bf94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbfb2693146c4a00ba70b0828e3a7e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801a3ed23cc24a15a86aa20a86667d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ef96d9d8964f618184ccac0c9ce346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c4b9e86b94b4d16acd3d5c0f2893c5d"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"BART-Large loaded for creative summaries.\nGenerating traffic density heatmap...\nHeatmap saved as 'heatmap.png'.\nExporting data to CSV...\nTraffic data exported to 'traffic_data.csv'.\nGenerating text report...\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 200, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n","output_type":"stream"},{"name":"stdout","text":"Text report generated with Flan-T5.\nReport enhanced with BART summarization.\n\n--- Generated Report ---\n\nSummarize the following report creatively, focusing on the scene and implications:The report describes the traffic conditions, vehicle types, trends, and possible implications.\nGenerating PDF report...\nPDF report generated as 'report.pdf'.\n\nStarting chat interaction...\nVideo processing complete. Ask any question about the video (type 'exit' to quit).\nSuggested questions:\n1. How many cars were detected in total?\n2. How many ambulances were in the video?\n3. How many vehicles from 2 to 5 seconds?\n4. What was the peak traffic time?\n5. What types of vehicles were detected?\n6. Describe the traffic in the video.\n7. Explain the traffic flow and trends.\n8. What was happening in the middle of the video?\n9. What might have caused the peak traffic?\n10. How could this data help traffic management?\n11. What was the average speed of cars?\n12. How congested was the traffic?\n13. Summarize the vehicle activity in the video.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  give me video description\n"},{"name":"stdout","text":"\nAnswer:\nVideo duration: 10.03 seconds.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  what are the vehicles in the video \n"},{"name":"stdout","text":"\nAnswer:\nCar: 18, Bus: 1 (total: 19)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  how many vehicles on 4th second\n"},{"name":"stdout","text":"\nAnswer:\n4\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  how many vechicles between 4 to 7 seconds\n"},{"name":"stdout","text":"\nAnswer:\n5 vehicles at 1.33 seconds\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  describe the vehicles timings\n"},{"name":"stdout","text":"\nAnswer:\nSummarize and enhance this description creatively:The video depicts a city street with a mix of cars, buses, and cars. The video begins with a black screen with a white background. Then, the video then transitions to a white screen with an image of a car and a bus on a street with cars on the street. Then the video transitions to an image with a car on the road. The music then fades out and fades back in. The song then fades in and out. The game then fades to black. The vids then fade out.The video then transition to black screen. The photo then transitions. The Video then transitions:A. Thevideo then transitions,A-B-C-D-E-\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  Summarize the vehicle activity in the video\n"},{"name":"stderr","text":"Your max_length is set to 150, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n","output_type":"stream"},{"name":"stdout","text":"\nAnswer:\nSummarize and enhance this description creatively:The video depicts a city street with a mix of cars, buses, and cars.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  How many ambulances were in the video?\n"},{"name":"stdout","text":"\nAnswer:\nNo ambulances were in the video.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  Describe the traffic in the video.\n"},{"name":"stderr","text":"Your max_length is set to 150, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n","output_type":"stream"},{"name":"stdout","text":"\nAnswer:\nSummarize and enhance this description creatively:The video depicts a city street with a mix of cars, buses, and cars. The video was shot at a speed of 30.00 fps.\n\n","output_type":"stream"}],"execution_count":null}]}