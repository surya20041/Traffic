{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a9cb3cadd594aa1997683768f0f8b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b424abc465a4fb1b0801f348ab882f8",
              "IPY_MODEL_727adb140f1745e69ebada843d1f7cf7",
              "IPY_MODEL_44f2431700364dd990ad71da64bd1215"
            ],
            "layout": "IPY_MODEL_3384d992eff7481da05e681a85557aec"
          }
        },
        "8b424abc465a4fb1b0801f348ab882f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d562147863491aaf11a62817e4513f",
            "placeholder": "​",
            "style": "IPY_MODEL_cfe5d9576b5d42f6b64899761e61b838",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "727adb140f1745e69ebada843d1f7cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b643e365d99a4a678247499dabb10524",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f690e8598b984531bd3a5520fa35ce2b",
            "value": 5
          }
        },
        "44f2431700364dd990ad71da64bd1215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f69874be53d4ea08dbbfacac3f02b40",
            "placeholder": "​",
            "style": "IPY_MODEL_56ba9ae7980b47509052e4b4d59991a7",
            "value": " 5/5 [00:08&lt;00:00,  1.49s/it]"
          }
        },
        "3384d992eff7481da05e681a85557aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d562147863491aaf11a62817e4513f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfe5d9576b5d42f6b64899761e61b838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b643e365d99a4a678247499dabb10524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f690e8598b984531bd3a5520fa35ce2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f69874be53d4ea08dbbfacac3f02b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ba9ae7980b47509052e4b4d59991a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hdBDg-UV9S2",
        "outputId": "9646ba4a-c876-43b5-93bb-216cbc4e10fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Inference_API_KEY_8B` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Inference_API_KEY_8B`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rouqQSM0acVe",
        "outputId": "189904ee-1ee8-41da-d5b9-4990d7276827"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.111-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Collecting py-cpuinfo (from ultralytics)\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.111-py3-none-any.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.8/978.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: py-cpuinfo, opencv-python, ultralytics-thop, ultralytics\n",
            "Successfully installed opencv-python-4.11.0.86 py-cpuinfo-9.0.0 ultralytics-8.3.111 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python numpy ultralytics transformers pillow matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDQ9BsGKsEH7",
        "outputId": "b53010a1-91bf-4b87-ac28-7bf713eee103"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.111)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import HTML, display\n",
        "import base64\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Install bitsandbytes for 8-bit quantization\n",
        "try:\n",
        "    import bitsandbytes\n",
        "except ImportError:\n",
        "    print(\"Installing bitsandbytes...\")\n",
        "    !pip install bitsandbytes\n",
        "    import bitsandbytes\n",
        "\n",
        "# Load YOLO model\n",
        "model_path = r\"/content/best.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Video setup\n",
        "video_path = r\"/content/test_10s.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "middle_index = total_frames // 2 if total_frames > 0 else -1\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "selected_frame = None\n",
        "peak_frame = None\n",
        "vehicle_counts = {}\n",
        "track_id_to_class = {}\n",
        "frame_vehicle_counts = []\n",
        "frame_index = 0\n",
        "max_vehicles = 0\n",
        "max_frame_index = 0\n",
        "emergency_alerts = []\n",
        "track_positions = defaultdict(list)  # For speed estimation\n",
        "average_speeds = {}\n",
        "congestion_indices = []\n",
        "\n",
        "# Process video\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "    frame = cv2.resize(frame, (700, 500))\n",
        "    results = model.track(frame, persist=True)\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "    confidences = results[0].boxes.conf.cpu().numpy()\n",
        "    classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "    track_ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else []\n",
        "\n",
        "    # Unique vehicle counts and speed estimation\n",
        "    current_frame_counts = defaultdict(int)\n",
        "    for conf, cls, track_id, box in zip(confidences, classes, track_ids, boxes):\n",
        "        if conf < 0.5:\n",
        "            continue\n",
        "        label = results[0].names[cls]\n",
        "        if track_id not in track_id_to_class:\n",
        "            track_id_to_class[track_id] = label\n",
        "            vehicle_counts[label] = vehicle_counts.get(label, 0) + 1\n",
        "        current_frame_counts[label] += 1\n",
        "        # Track position for speed (center of bounding box)\n",
        "        center_x = (box[0] + box[2]) / 2\n",
        "        center_y = (box[1] + box[3]) / 2\n",
        "        track_positions[track_id].append((frame_index, center_x, center_y))\n",
        "\n",
        "    frame_vehicle_counts.append(dict(current_frame_counts))\n",
        "\n",
        "    # Emergency vehicle alerts\n",
        "    if current_frame_counts.get(\"Ambulance\", 0) > 1:\n",
        "        emergency_alerts.append(f\"High ambulance activity at {frame_index/fps:.2f}s: {current_frame_counts['Ambulance']} ambulances\")\n",
        "\n",
        "    # Congestion index\n",
        "    total_in_frame = sum(current_frame_counts.values())\n",
        "    congestion_index = total_in_frame / 5.0  # Normalize (5 vehicles = index 1.0)\n",
        "    congestion_indices.append(congestion_index)\n",
        "    if total_in_frame > max_vehicles:\n",
        "        max_vehicles = total_in_frame\n",
        "        max_frame_index = frame_index\n",
        "        peak_frame = results[0].plot()\n",
        "\n",
        "    # Save middle frame\n",
        "    if frame_index == middle_index:\n",
        "        selected_frame = results[0].plot()\n",
        "\n",
        "    frame_index += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Calculate average speeds\n",
        "for track_id, positions in track_positions.items():\n",
        "    label = track_id_to_class[track_id]\n",
        "    if len(positions) < 2:\n",
        "        continue\n",
        "    total_speed = 0\n",
        "    count = 0\n",
        "    for i in range(1, len(positions)):\n",
        "        frame_diff = positions[i][0] - positions[i-1][0]\n",
        "        if frame_diff == 0:\n",
        "            continue\n",
        "        dx = positions[i][1] - positions[i-1][1]\n",
        "        dy = positions[i][2] - positions[i-1][2]\n",
        "        distance = np.sqrt(dx**2 + dy**2)\n",
        "        time = frame_diff / fps\n",
        "        speed = distance / time  # Pixels per second\n",
        "        total_speed += speed\n",
        "        count += 1\n",
        "    if count > 0:\n",
        "        avg_speed = total_speed / count\n",
        "        average_speeds[label] = average_speeds.get(label, 0) + avg_speed\n",
        "        average_speeds[f\"{label}_count\"] = average_speeds.get(f\"{label}_count\", 0) + 1\n",
        "\n",
        "for label in vehicle_counts.keys():\n",
        "    count_key = f\"{label}_count\"\n",
        "    if count_key in average_speeds:\n",
        "        average_speeds[label] = average_speeds[label] / average_speeds[count_key]\n",
        "        del average_speeds[count_key]\n",
        "\n",
        "# Save annotated frames\n",
        "if selected_frame is not None:\n",
        "    cv2.imwrite(\"middle_frame.jpg\", selected_frame)\n",
        "    print(\"Middle frame saved as 'middle_frame.jpg'.\")\n",
        "else:\n",
        "    print(\"Warning: Middle frame not saved.\")\n",
        "if peak_frame is not None:\n",
        "    cv2.imwrite(\"peak_frame.jpg\", peak_frame)\n",
        "    print(\"Peak frame saved as 'peak_frame.jpg'.\")\n",
        "else:\n",
        "    print(\"Warning: Peak frame not saved.\")\n",
        "\n",
        "# Convert middle frame to PIL for report generation\n",
        "if selected_frame is not None:\n",
        "    selected_frame_rgb = cv2.cvtColor(selected_frame, cv2.COLOR_BGR2RGB)\n",
        "    selected_frame_pil = Image.fromarray(selected_frame_rgb)\n",
        "else:\n",
        "    print(\"Warning: No middle frame selected for PIL conversion.\")\n",
        "    selected_frame_pil = None\n",
        "\n",
        "# Load report pipeline\n",
        "try:\n",
        "    pipe = pipeline(\"image-text-to-text\", model=\"meta-llama/Llama-3.2-11B-Vision\")\n",
        "except Exception as e:\n",
        "    print(f\"Vision model error: {e}\")\n",
        "    try:\n",
        "        pipe = pipeline(\"text2text-generation\", model=\"meta-llama/Llama-3.2-11B\")\n",
        "    except Exception as e:\n",
        "        print(f\"Llama error: {e}\")\n",
        "        pipe = None\n",
        "\n",
        "# Load chatbot model (Flan-T5-Large with 8-bit quantization)\n",
        "try:\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"google/flan-t5-large\",\n",
        "        device_map=\"auto\",\n",
        "        load_in_8bit=True\n",
        "    )\n",
        "    chatbot = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"Flan-T5-Large loaded for chatbot.\")\n",
        "except Exception as e:\n",
        "    print(f\"Chatbot model error: {e}\")\n",
        "    print(\"Ensure you are using a GPU runtime (e.g., T4 or V100) and have installed bitsandbytes.\")\n",
        "    chatbot = None\n",
        "\n",
        "# Calculate averages and congestion\n",
        "average_counts = {}\n",
        "for vtype in vehicle_counts.keys():\n",
        "    total = sum(frame_counts.get(vtype, 0) for frame_counts in frame_vehicle_counts)\n",
        "    average_counts[vtype] = total / len(frame_vehicle_counts) if frame_vehicle_counts else 0\n",
        "max_time_sec = max_frame_index / fps if fps > 0 else 0\n",
        "average_congestion = np.mean(congestion_indices) if congestion_indices else 0\n",
        "\n",
        "# Generate traffic density heatmap\n",
        "times = [i / fps for i in range(len(frame_vehicle_counts))]\n",
        "total_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in frame_vehicle_counts]\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap([total_vehicles_per_frame], cmap=\"YlOrRd\", xticklabels=50, cbar_kws={'label': 'Vehicle Count'})\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Traffic Density Heatmap\")\n",
        "plt.xticks(ticks=np.linspace(0, len(times)-1, 5), labels=[f\"{t:.1f}\" for t in np.linspace(0, max(times), 5)])\n",
        "plt.savefig(\"heatmap.png\")\n",
        "plt.show()\n",
        "print(\"Heatmap saved as 'heatmap.png'.\")\n",
        "\n",
        "# Export to CSV\n",
        "csv_data = {\n",
        "    \"Frame\": list(range(len(frame_vehicle_counts))),\n",
        "    \"Time (s)\": times,\n",
        "    \"Total Vehicles\": total_vehicles_per_frame,\n",
        "    \"Congestion Index\": congestion_indices\n",
        "}\n",
        "for vtype in vehicle_counts.keys():\n",
        "    csv_data[vtype] = [frame_counts.get(vtype, 0) for frame_counts in frame_vehicle_counts]\n",
        "df = pd.DataFrame(csv_data)\n",
        "df.to_csv(\"traffic_data.csv\", index=False)\n",
        "print(\"Traffic data exported to 'traffic_data.csv'.\")\n",
        "\n",
        "# Generate text report\n",
        "report_prompt = (\n",
        "    f\"Vehicle Detection Report:\\n\"\n",
        "    f\"Unique vehicle counts across all frames:\\n\"\n",
        ")\n",
        "for vtype, count in vehicle_counts.items():\n",
        "    report_prompt += f\"{vtype}: {count}\\n\"\n",
        "report_prompt += \"Additional Insights:\\n\"\n",
        "report_prompt += \"Average vehicles per frame:\\n\"\n",
        "for vtype, avg in average_counts.items():\n",
        "    report_prompt += f\"{vtype}: {avg:.2f}\\n\"\n",
        "report_prompt += f\"Average speeds (pixels/s):\\n\"\n",
        "for vtype, speed in average_speeds.items():\n",
        "    report_prompt += f\"{vtype}: {speed:.2f}\\n\"\n",
        "report_prompt += f\"Peak traffic at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\\n\"\n",
        "report_prompt += f\"Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high).\\n\"\n",
        "if emergency_alerts:\n",
        "    report_prompt += \"Emergency Alerts:\\n\" + \"\\n\".join(emergency_alerts) + \"\\n\"\n",
        "if frame_vehicle_counts and middle_index < len(frame_vehicle_counts):\n",
        "    selected_counts = frame_vehicle_counts[middle_index]\n",
        "    report_prompt += \"Selected middle frame shows:\\n\"\n",
        "    for vtype, count in selected_counts.items():\n",
        "        report_prompt += f\"{vtype}: {count}\\n\"\n",
        "report_prompt += \"Generate a detailed summary based on this data and describe the scene in the selected frame.\"\n",
        "\n",
        "try:\n",
        "    if selected_frame_pil and pipe and \"image-text-to-text\" in str(pipe):\n",
        "        result = pipe(image=selected_frame_pil, text=report_prompt, max_new_tokens=300)\n",
        "        report_text = result[0]['generated_text']\n",
        "    elif pipe:\n",
        "        result = pipe(text=report_prompt, max_new_tokens=300)\n",
        "        report_text = result[0]['generated_text']\n",
        "    else:\n",
        "        report_text = \"Report failed. Counts:\\n\" + \"\\n\".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()])\n",
        "except Exception as e:\n",
        "    print(f\"Report error: {e}\")\n",
        "    report_text = \"Report failed. Counts:\\n\" + \"\\n\".join([f\"{v}: {c}\" for v, c in vehicle_counts.items()])\n",
        "\n",
        "print(\"\\n--- Generated Report ---\\n\")\n",
        "print(report_text)\n",
        "\n",
        "# Visualization Dashboard\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))\n",
        "ax1.plot(times, total_vehicles_per_frame, color='blue')\n",
        "ax1.set_xlabel('Time (seconds)')\n",
        "ax1.set_ylabel('Total Vehicles')\n",
        "ax1.set_title('Total Vehicles Over Time')\n",
        "ax1.grid(True)\n",
        "\n",
        "vehicle_types = list(vehicle_counts.keys())\n",
        "for vtype in vehicle_types:\n",
        "    counts = [frame_counts.get(vtype, 0) for frame_counts in frame_vehicle_counts]\n",
        "    ax2.plot(times, counts, label=vtype)\n",
        "ax2.set_xlabel('Time (seconds)')\n",
        "ax2.set_ylabel('Vehicle Count')\n",
        "ax2.set_title('Vehicle Counts by Type Over Time')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "ax3.bar(vehicle_types, [vehicle_counts[vtype] for vtype in vehicle_types], color='green')\n",
        "ax3.set_xlabel('Vehicle Type')\n",
        "ax3.set_ylabel('Unique Count')\n",
        "ax3.set_title('Unique Vehicle Counts')\n",
        "ax3.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"dashboard.png\")\n",
        "plt.show()\n",
        "print(\"Dashboard saved as 'dashboard.png'.\")\n",
        "\n",
        "# Encode images as base64\n",
        "def encode_image_to_base64(image_path):\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "            print(f\"Successfully encoded {image_path} to base64.\")\n",
        "            return encoded\n",
        "    print(f\"Warning: {image_path} not found for base64 encoding.\")\n",
        "    return None\n",
        "\n",
        "dashboard_b64 = encode_image_to_base64(\"dashboard.png\")\n",
        "middle_frame_b64 = encode_image_to_base64(\"middle_frame.jpg\")\n",
        "peak_frame_b64 = encode_image_to_base64(\"peak_frame.jpg\")\n",
        "heatmap_b64 = encode_image_to_base64(\"heatmap.png\")\n",
        "\n",
        "# Generate enhanced HTML report\n",
        "report_text_html = report_text.replace('\\n', '<br>')\n",
        "html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Vehicle Detection Report</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "        h1, h2 {{ color: #333; cursor: pointer; }}\n",
        "        table {{ border-collapse: collapse; width: 50%; margin: 20px 0; }}\n",
        "        th, td {{ border: 1px solid #ccc; padding: 8px; text-align: left; }}\n",
        "        th {{ background-color: #f2f2f2; }}\n",
        "        img {{ max-width: 100%; height: auto; margin: 10px 0; }}\n",
        "        .section {{ margin-bottom: 20px; }}\n",
        "        .collapsible {{ display: none; }}\n",
        "        .collapsible.show {{ display: block; }}\n",
        "    </style>\n",
        "    <script>\n",
        "        function toggleSection(id) {{\n",
        "            var content = document.getElementById(id);\n",
        "            content.classList.toggle('show');\n",
        "        }}\n",
        "    </script>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Vehicle Detection Report</h1>\n",
        "    <div class=\"section\">\n",
        "        <h2 onclick=\"toggleSection('summary')\">Summary</h2>\n",
        "        <div id=\"summary\" class=\"collapsible show\">\n",
        "            <p>{report_text_html}</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <h2 onclick=\"toggleSection('stats')\">Vehicle Statistics</h2>\n",
        "        <div id=\"stats\" class=\"collapsible\">\n",
        "            <table>\n",
        "                <tr><th>Vehicle Type</th><th>Unique Count</th><th>Average per Frame</th><th>Middle Frame Count</th><th>Average Speed (pixels/s)</th></tr>\n",
        "\"\"\"\n",
        "for vtype in vehicle_counts.keys():\n",
        "    unique_count = vehicle_counts.get(vtype, 0)\n",
        "    avg_count = average_counts.get(vtype, 0)\n",
        "    middle_count = frame_vehicle_counts[middle_index].get(vtype, 0) if middle_index < len(frame_vehicle_counts) else 0\n",
        "    speed = average_speeds.get(vtype, 0)\n",
        "    html_content += f\"<tr><td>{vtype}</td><td>{unique_count}</td><td>{avg_count:.2f}</td><td>{middle_count}</td><td>{speed:.2f}</td></tr>\"\n",
        "html_content += \"</table></div></div>\"\n",
        "html_content += f\"\"\"\n",
        "    <div class=\"section\">\n",
        "        <h2 onclick=\"toggleSection('insights')\">Key Insights</h2>\n",
        "        <div id=\"insights\" class=\"collapsible\">\n",
        "            <p>Peak traffic occurred at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.</p>\n",
        "            <p>Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high).</p>\n",
        "\"\"\"\n",
        "if emergency_alerts:\n",
        "    html_content += \"<p>Emergency Alerts:</p><ul>\" + \"\".join([f\"<li>{alert}</li>\" for alert in emergency_alerts]) + \"</ul>\"\n",
        "html_content += \"</div></div>\"\n",
        "html_content += f\"\"\"\n",
        "    <div class=\"section\">\n",
        "        <h2 onclick=\"toggleSection('visuals')\">Visualizations</h2>\n",
        "        <div id=\"visuals\" class=\"collapsible\">\n",
        "\"\"\"\n",
        "if dashboard_b64:\n",
        "    html_content += f'<img src=\"data:image/png;base64,{dashboard_b64}\" alt=\"Visualization Dashboard\">'\n",
        "else:\n",
        "    html_content += \"<p>Dashboard image not available.</p>\"\n",
        "if heatmap_b64:\n",
        "    html_content += f'<img src=\"data:image/png;base64,{heatmap_b64}\" alt=\"Traffic Density Heatmap\">'\n",
        "else:\n",
        "    html_content += \"<p>Heatmap image not available.</p>\"\n",
        "if middle_frame_b64:\n",
        "    html_content += f'<img src=\"data:image/jpeg;base64,{middle_frame_b64}\" alt=\"Middle Frame\">'\n",
        "else:\n",
        "    html_content += \"<p>Middle frame image not available.</p>\"\n",
        "if peak_frame_b64:\n",
        "    html_content += f'<img src=\"data:image/jpeg;base64,{peak_frame_b64}\" alt=\"Peak Traffic Frame\">'\n",
        "else:\n",
        "    html_content += \"<p>Peak frame image not available.</p>\"\n",
        "html_content += \"\"\"\n",
        "        </div>\n",
        "    </div>\n",
        "    <div class=\"section\">\n",
        "        <p><strong>Note:</strong> This is an HTML report. Do not copy-paste into a Python cell, as it will cause a syntax error. View it here or open 'report.html' in a browser.</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "with open(\"report.html\", \"w\") as f:\n",
        "    f.write(html_content)\n",
        "print(\"\\nHTML report generated as 'report.html' and displayed below.\")\n",
        "print(\"Note: If the report doesn't display, open 'report.html' in a browser or check for errors above.\")\n",
        "\n",
        "# Display HTML report in Colab\n",
        "try:\n",
        "    display(HTML(html_content))\n",
        "except Exception as e:\n",
        "    print(f\"Error displaying HTML: {e}\")\n",
        "    print(\"Please open 'report.html' in a browser to view the report.\")\n",
        "\n",
        "# Create optimized context for chatbot\n",
        "context = (\n",
        "    f\"Video Analysis: {total_frames} frames, {(total_frames/fps):.2f}s, {fps:.2f} FPS.\\n\"\n",
        "    f\"Unique vehicles: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])} (total: {sum(vehicle_counts.values())}).\\n\"\n",
        "    f\"Average per frame: {', '.join([f'{v}: {c:.2f}' for v, c in average_counts.items()])}.\\n\"\n",
        "    f\"Average speeds (pixels/s): {', '.join([f'{v}: {s:.2f}' for v, s in average_speeds.items()])}.\\n\"\n",
        "    f\"Peak traffic: {max_vehicles} vehicles at {max_time_sec:.2f}s.\\n\"\n",
        "    f\"Middle frame ({middle_index/fps:.2f}s): {', '.join([f'{v}: {c}' for v, c in frame_vehicle_counts[middle_index].items()]) if middle_index < len(frame_vehicle_counts) else 'no vehicles'}.\\n\"\n",
        "    f\"Vehicle types: {', '.join(vehicle_counts.keys())}.\\n\"\n",
        "    f\"Traffic: {'Heavy' if sum(vehicle_counts.values()) > 30 else 'Moderate' if sum(vehicle_counts.values()) > 15 else 'Light'}.\\n\"\n",
        "    f\"Congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high).\\n\"\n",
        ")\n",
        "if emergency_alerts:\n",
        "    context += f\"Emergency alerts: {'; '.join(emergency_alerts)}.\\n\"\n",
        "\n",
        "# Updated answer_question function\n",
        "def answer_question(question, vehicle_counts, frame_vehicle_counts, fps, total_frames, average_counts, max_time_sec, max_vehicles, context, chatbot, average_speeds, congestion_indices, emergency_alerts):\n",
        "    question = question.lower().strip()\n",
        "    if not question:\n",
        "        return \"Please ask a valid question.\"\n",
        "\n",
        "    # Rule-based answers\n",
        "    time_match = re.search(r\"at (\\d+) seconds\", question)\n",
        "    if time_match:\n",
        "        time_sec = int(time_match.group(1))\n",
        "        if time_sec * fps >= total_frames:\n",
        "            return f\"Time {time_sec} seconds exceeds video duration ({total_frames/fps:.2f}s).\"\n",
        "        frame_index = min(int(time_sec * fps), total_frames - 1)\n",
        "        frame_counts = frame_vehicle_counts[frame_index]\n",
        "        for vtype in vehicle_counts.keys():\n",
        "            if vtype.lower() in question:\n",
        "                count = frame_counts.get(vtype, 0)\n",
        "                return f\"At {time_sec} seconds, there were {count} {vtype}(s).\"\n",
        "        if \"traffic\" in question or \"how many\" in question:\n",
        "            counts_str = \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n",
        "            return f\"At {time_sec} seconds: {counts_str or 'no vehicles'}.\"\n",
        "        total = sum(frame_counts.values())\n",
        "        return f\"At {time_sec} seconds, there were {total} vehicles.\"\n",
        "\n",
        "    if \"peak\" in question:\n",
        "        return f\"Peak traffic was at {max_time_sec:.2f} seconds with {max_vehicles} vehicles.\"\n",
        "\n",
        "    if \"average\" in question and \"speed\" in question:\n",
        "        for vtype in vehicle_counts.keys():\n",
        "            if vtype.lower() in question:\n",
        "                speed = average_speeds.get(vtype, 0)\n",
        "                return f\"Average speed of {vtype}s: {speed:.2f} pixels/s.\"\n",
        "        speeds_str = \", \".join([f\"{vtype}: {speed:.2f} pixels/s\" for vtype, speed in average_speeds.items()])\n",
        "        return f\"Average speeds: {speeds_str or 'none'}.\"\n",
        "\n",
        "    if \"average\" in question:\n",
        "        for vtype in vehicle_counts.keys():\n",
        "            if vtype.lower() in question:\n",
        "                avg = average_counts.get(vtype, 0)\n",
        "                return f\"Average {vtype}s per frame: {avg:.2f}.\"\n",
        "        averages_str = \", \".join([f\"{vtype}: {avg:.2f}\" for vtype, avg in average_counts.items()])\n",
        "        return f\"Average vehicles per frame: {averages_str or 'none'}.\"\n",
        "\n",
        "    if re.search(r\"how many (total |)vehicles\", question) or \"total number\" in question:\n",
        "        total = sum(vehicle_counts.values())\n",
        "        return f\"Total unique vehicles: {total}.\"\n",
        "\n",
        "    for vtype, count in vehicle_counts.items():\n",
        "        type_lower = vtype.lower()\n",
        "        if any(phrase in question for phrase in [f\"how many {type_lower}\", f\"number of {type_lower}\", f\"how many {type_lower}s\", f\"{type_lower}s in the video\", f\"{type_lower} in the video\"]):\n",
        "            return f\"Unique {vtype}(s) detected: {count}.\"\n",
        "\n",
        "    if \"most common\" in question or \"most frequent\" in question:\n",
        "        if vehicle_counts:\n",
        "            most_common = max(vehicle_counts.items(), key=lambda x: x[1])\n",
        "            return f\"Most common vehicle: {most_common[0]} ({most_common[1]} instances).\"\n",
        "        return \"No vehicles detected.\"\n",
        "\n",
        "    if \"what types\" in question or \"vehicle types\" in question or \"which vehicles\" in question:\n",
        "        if vehicle_counts:\n",
        "            types = list(vehicle_counts.keys())\n",
        "            types_str = \", \".join(types[:-1]) + \" and \" + types[-1] if len(types) > 1 else types[0]\n",
        "            return f\"Vehicle types: {types_str}.\"\n",
        "        return \"No vehicles detected.\"\n",
        "\n",
        "    if \"traffic\" in question and (\"condition\" in question or \"overall\" in question):\n",
        "        total = sum(vehicle_counts.values())\n",
        "        condition = \"heavy\" if total > 30 else \"moderate\" if total > 15 else \"light\"\n",
        "        return f\"Traffic was {condition} with {total} unique vehicles.\"\n",
        "\n",
        "    if \"congestion\" in question or \"congested\" in question:\n",
        "        avg_congestion = np.mean(congestion_indices) if congestion_indices else 0\n",
        "        level = \"high\" if avg_congestion > 2 else \"moderate\" if avg_congestion > 1 else \"low\"\n",
        "        return f\"Average congestion index: {avg_congestion:.2f} ({level}).\"\n",
        "\n",
        "    if \"emergency\" in question or \"alerts\" in question or \"ambulance activity\" in question:\n",
        "        if emergency_alerts:\n",
        "            return f\"Emergency alerts: {'; '.join(emergency_alerts)}.\"\n",
        "        return \"No emergency alerts detected.\"\n",
        "\n",
        "    # Chatbot for open-ended and hypothetical questions\n",
        "    if chatbot:\n",
        "        try:\n",
        "            is_exploratory = any(kw in question for kw in [\"describe\", \"tell me\", \"what can you say\", \"interesting\", \"summarize\", \"activity\", \"happening\"])\n",
        "            is_hypothetical = any(kw in question for kw in [\"would\", \"might\", \"could\", \"help\"])\n",
        "\n",
        "            if is_exploratory:\n",
        "                prompt = (\n",
        "                    f\"{context}\\n\"\n",
        "                    f\"Instruction: Describe the traffic in detail, including vehicle types, counts, speeds, or trends. Start with 'The video shows...'.\\n\"\n",
        "                    f\"Example:\\n\"\n",
        "                    f\"Q: What was happening in the middle?\\n\"\n",
        "                    f\"A: The video shows 3 cars and 1 ambulance in the middle frame at 5s, moving at 50 pixels/s, with moderate traffic.\\n\"\n",
        "                    f\"Question: {question}\\nAnswer:\"\n",
        "                )\n",
        "            elif is_hypothetical:\n",
        "                prompt = (\n",
        "                    f\"{context}\\n\"\n",
        "                    f\"Instruction: Reason about the traffic data to answer speculatively, discussing causes, impacts, or applications. Start with 'Based on the data...'.\\n\"\n",
        "                    f\"Example:\\n\"\n",
        "                    f\"Q: What might cause a peak?\\n\"\n",
        "                    f\"A: Based on the data, a peak of 4 vehicles at 3.2s could be due to a traffic signal.\\n\"\n",
        "                    f\"Question: {question}\\nAnswer:\"\n",
        "                )\n",
        "            else:\n",
        "                prompt = (\n",
        "                    f\"{context}\\n\"\n",
        "                    f\"Instruction: Answer concisely using the data, focusing on counts, peak, speeds, or congestion.\\n\"\n",
        "                    f\"Example:\\n\"\n",
        "                    f\"Q: What was the traffic like?\\n\"\n",
        "                    f\"A: The traffic was moderate with 9 vehicles, peaking at 4 vehicles at 3.2s, congestion index 0.8.\\n\"\n",
        "                    f\"Question: {question}\\nAnswer:\"\n",
        "                )\n",
        "\n",
        "            response = chatbot(prompt, max_length=80, num_return_sequences=1)[0]['generated_text'].strip()\n",
        "            if response and len(response) > 10 and not response.startswith((\"Instruction:\", \"Example:\", \"Question:\")):\n",
        "                sentences = re.split(r'(?<=[.!?])\\s+', response)\n",
        "                return \" \".join(sentences[:min(3, len(sentences))]).strip()\n",
        "            else:\n",
        "                simplified_prompt = f\"{context}\\nQuestion: {question}\\nAnswer concisely:\"\n",
        "                response = chatbot(simplified_prompt, max_length=80, num_return_sequences=1)[0]['generated_text'].strip()\n",
        "                if response and len(response) > 10:\n",
        "                    sentences = re.split(r'(?<=[.!?])\\s+', response)\n",
        "                    return \" \".join(sentences[:min(3, len(sentences))]).strip()\n",
        "                print(f\"Debug: Invalid chatbot response for: {question} (response: {response})\")\n",
        "                return f\"Please rephrase your question. Summary: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])}.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Debug: Chatbot error for '{question}': {e}\")\n",
        "            return f\"Error processing question. Summary: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])}.\"\n",
        "    return f\"No chatbot available. Summary: {', '.join([f'{v}: {c}' for v, c in vehicle_counts.items()])}.\"\n",
        "\n",
        "# Dynamic chat suggestions\n",
        "suggestions = [\n",
        "    \"How many cars were detected in total?\",\n",
        "    \"How many ambulances were in the video?\",\n",
        "    \"How many vehicles at 5 seconds?\",\n",
        "    \"What was the peak traffic time?\",\n",
        "    \"What types of vehicles were detected?\",\n",
        "    \"Tell me about the vehicles in the video.\",\n",
        "    \"What can you say about the traffic flow?\",\n",
        "    \"What was happening in the middle of the video?\",\n",
        "    \"Was there anything interesting in the video?\",\n",
        "    \"What might cause a peak like this?\",\n",
        "    \"How could this data help traffic management?\",\n",
        "    \"What was the average speed of cars?\",\n",
        "    \"How congested was the traffic?\"\n",
        "]\n",
        "if emergency_alerts:\n",
        "    suggestions.append(\"When were ambulances most active?\")\n",
        "\n",
        "# Chat loop with guidance\n",
        "print(\"\\nVideo processing complete. Ask any question about the video (type 'exit' to quit).\")\n",
        "print(\"Suggested questions:\")\n",
        "for i, suggestion in enumerate(suggestions, 1):\n",
        "    print(f\"{i}. {suggestion}\")\n",
        "while True:\n",
        "    user_query = input(\"Ask a question: \")\n",
        "    if user_query.lower().strip() == \"exit\":\n",
        "        print(\"Goodbye.\")\n",
        "        break\n",
        "    response = answer_question(user_query, vehicle_counts, frame_vehicle_counts, fps, total_frames, average_counts, max_time_sec, max_vehicles, context, chatbot, average_speeds, congestion_indices, emergency_alerts)\n",
        "    print(\"\\nAnswer:\")\n",
        "    print(response)\n",
        "    print()"
      ],
      "metadata": {
        "id": "kCmBvaKHaOjq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5a9cb3cadd594aa1997683768f0f8b0c",
            "8b424abc465a4fb1b0801f348ab882f8",
            "727adb140f1745e69ebada843d1f7cf7",
            "44f2431700364dd990ad71da64bd1215",
            "3384d992eff7481da05e681a85557aec",
            "17d562147863491aaf11a62817e4513f",
            "cfe5d9576b5d42f6b64899761e61b838",
            "b643e365d99a4a678247499dabb10524",
            "f690e8598b984531bd3a5520fa35ce2b",
            "5f69874be53d4ea08dbbfacac3f02b40",
            "56ba9ae7980b47509052e4b4d59991a7"
          ]
        },
        "outputId": "986f65b0-a862-4cd1-d148-2ec0942c21d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 104.3ms\n",
            "Speed: 2.7ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 93.0ms\n",
            "Speed: 2.3ms preprocess, 93.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 85.1ms\n",
            "Speed: 2.4ms preprocess, 85.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 83.5ms\n",
            "Speed: 2.4ms preprocess, 83.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 83.8ms\n",
            "Speed: 2.4ms preprocess, 83.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 84.8ms\n",
            "Speed: 2.3ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 83.3ms\n",
            "Speed: 2.4ms preprocess, 83.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 41.1ms\n",
            "Speed: 2.4ms preprocess, 41.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 84.2ms\n",
            "Speed: 2.3ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 84.1ms\n",
            "Speed: 2.4ms preprocess, 84.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 83.3ms\n",
            "Speed: 2.4ms preprocess, 83.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 83.9ms\n",
            "Speed: 2.4ms preprocess, 83.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 40.5ms\n",
            "Speed: 2.4ms preprocess, 40.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 86.2ms\n",
            "Speed: 2.5ms preprocess, 86.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 83.4ms\n",
            "Speed: 2.3ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 84.2ms\n",
            "Speed: 2.3ms preprocess, 84.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 40.7ms\n",
            "Speed: 2.6ms preprocess, 40.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 84.0ms\n",
            "Speed: 2.3ms preprocess, 84.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 85.5ms\n",
            "Speed: 2.3ms preprocess, 85.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 8 Cars, 84.7ms\n",
            "Speed: 2.5ms preprocess, 84.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 82.1ms\n",
            "Speed: 2.4ms preprocess, 82.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 41.1ms\n",
            "Speed: 2.4ms preprocess, 41.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 82.3ms\n",
            "Speed: 2.4ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 84.8ms\n",
            "Speed: 2.3ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 83.4ms\n",
            "Speed: 2.5ms preprocess, 83.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 82.9ms\n",
            "Speed: 2.3ms preprocess, 82.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 82.8ms\n",
            "Speed: 2.4ms preprocess, 82.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 79.8ms\n",
            "Speed: 2.5ms preprocess, 79.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 9 Cars, 81.8ms\n",
            "Speed: 2.4ms preprocess, 81.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 83.9ms\n",
            "Speed: 2.6ms preprocess, 83.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 41.6ms\n",
            "Speed: 2.5ms preprocess, 41.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 81.5ms\n",
            "Speed: 2.6ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 82.1ms\n",
            "Speed: 2.6ms preprocess, 82.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 83.4ms\n",
            "Speed: 2.5ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 88.1ms\n",
            "Speed: 2.3ms preprocess, 88.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 84.2ms\n",
            "Speed: 2.3ms preprocess, 84.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.3ms\n",
            "Speed: 2.6ms preprocess, 84.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 80.7ms\n",
            "Speed: 2.5ms preprocess, 80.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 43.9ms\n",
            "Speed: 2.6ms preprocess, 43.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 82.8ms\n",
            "Speed: 2.5ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 81.9ms\n",
            "Speed: 2.5ms preprocess, 81.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 84.0ms\n",
            "Speed: 2.6ms preprocess, 84.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 43.7ms\n",
            "Speed: 2.7ms preprocess, 43.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 86.2ms\n",
            "Speed: 3.1ms preprocess, 86.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.5ms\n",
            "Speed: 2.6ms preprocess, 84.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.2ms\n",
            "Speed: 2.7ms preprocess, 84.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 81.7ms\n",
            "Speed: 2.6ms preprocess, 81.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.3ms\n",
            "Speed: 2.8ms preprocess, 84.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 82.0ms\n",
            "Speed: 2.5ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 84.3ms\n",
            "Speed: 2.5ms preprocess, 84.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 83.7ms\n",
            "Speed: 2.5ms preprocess, 83.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 10 Cars, 83.6ms\n",
            "Speed: 2.5ms preprocess, 83.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.4ms\n",
            "Speed: 2.6ms preprocess, 84.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 40.5ms\n",
            "Speed: 2.5ms preprocess, 40.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 81.0ms\n",
            "Speed: 2.5ms preprocess, 81.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.9ms\n",
            "Speed: 2.7ms preprocess, 85.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.9ms\n",
            "Speed: 2.6ms preprocess, 82.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.8ms\n",
            "Speed: 2.7ms preprocess, 83.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.7ms\n",
            "Speed: 2.5ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.5ms\n",
            "Speed: 2.5ms preprocess, 84.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 81.4ms\n",
            "Speed: 2.6ms preprocess, 81.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.4ms\n",
            "Speed: 2.7ms preprocess, 85.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 41.3ms\n",
            "Speed: 2.6ms preprocess, 41.3ms inference, 40.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.7ms\n",
            "Speed: 2.5ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.5ms\n",
            "Speed: 2.4ms preprocess, 82.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.2ms\n",
            "Speed: 2.6ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 40.8ms\n",
            "Speed: 2.7ms preprocess, 40.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 86.1ms\n",
            "Speed: 2.7ms preprocess, 86.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.8ms\n",
            "Speed: 2.5ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.6ms\n",
            "Speed: 2.7ms preprocess, 82.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.3ms\n",
            "Speed: 2.5ms preprocess, 85.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 87.1ms\n",
            "Speed: 2.6ms preprocess, 87.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.8ms\n",
            "Speed: 2.6ms preprocess, 83.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.6ms\n",
            "Speed: 2.7ms preprocess, 83.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 82.9ms\n",
            "Speed: 2.6ms preprocess, 82.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 80.5ms\n",
            "Speed: 2.5ms preprocess, 80.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 83.5ms\n",
            "Speed: 2.5ms preprocess, 83.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 83.8ms\n",
            "Speed: 2.5ms preprocess, 83.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 83.1ms\n",
            "Speed: 2.6ms preprocess, 83.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 43.1ms\n",
            "Speed: 2.7ms preprocess, 43.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 80.7ms\n",
            "Speed: 2.8ms preprocess, 80.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 82.9ms\n",
            "Speed: 2.5ms preprocess, 82.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.1ms\n",
            "Speed: 2.5ms preprocess, 85.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.2ms\n",
            "Speed: 2.5ms preprocess, 84.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.9ms\n",
            "Speed: 2.6ms preprocess, 83.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 86.6ms\n",
            "Speed: 2.6ms preprocess, 86.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.7ms\n",
            "Speed: 2.5ms preprocess, 82.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 81.9ms\n",
            "Speed: 2.5ms preprocess, 81.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.3ms\n",
            "Speed: 2.6ms preprocess, 82.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.9ms\n",
            "Speed: 2.5ms preprocess, 83.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.7ms\n",
            "Speed: 2.5ms preprocess, 82.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.3ms\n",
            "Speed: 2.6ms preprocess, 85.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 41.0ms\n",
            "Speed: 2.6ms preprocess, 41.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.9ms\n",
            "Speed: 2.7ms preprocess, 84.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.8ms\n",
            "Speed: 2.5ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.4ms\n",
            "Speed: 2.5ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.5ms\n",
            "Speed: 2.6ms preprocess, 84.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 41.7ms\n",
            "Speed: 2.7ms preprocess, 41.7ms inference, 43.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.9ms\n",
            "Speed: 2.5ms preprocess, 82.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.7ms\n",
            "Speed: 2.5ms preprocess, 83.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.1ms\n",
            "Speed: 2.6ms preprocess, 83.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 39.9ms\n",
            "Speed: 43.7ms preprocess, 39.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.8ms\n",
            "Speed: 2.6ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 84.3ms\n",
            "Speed: 2.5ms preprocess, 84.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 85.7ms\n",
            "Speed: 2.6ms preprocess, 85.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 83.1ms\n",
            "Speed: 2.7ms preprocess, 83.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 82.2ms\n",
            "Speed: 2.6ms preprocess, 82.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 81.8ms\n",
            "Speed: 3.0ms preprocess, 81.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 80.6ms\n",
            "Speed: 2.6ms preprocess, 80.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 Cars, 83.2ms\n",
            "Speed: 2.5ms preprocess, 83.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 44.0ms\n",
            "Speed: 2.8ms preprocess, 44.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 80.8ms\n",
            "Speed: 2.5ms preprocess, 80.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.5ms\n",
            "Speed: 2.5ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.2ms\n",
            "Speed: 2.7ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.5ms\n",
            "Speed: 2.7ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.4ms\n",
            "Speed: 2.6ms preprocess, 85.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.3ms\n",
            "Speed: 2.6ms preprocess, 83.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.4ms\n",
            "Speed: 2.7ms preprocess, 82.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.3ms\n",
            "Speed: 2.7ms preprocess, 84.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 80.2ms\n",
            "Speed: 2.5ms preprocess, 80.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.9ms\n",
            "Speed: 2.7ms preprocess, 83.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.9ms\n",
            "Speed: 2.4ms preprocess, 84.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.3ms\n",
            "Speed: 2.8ms preprocess, 85.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 40.2ms\n",
            "Speed: 2.6ms preprocess, 40.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 86.6ms\n",
            "Speed: 2.7ms preprocess, 86.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 81.8ms\n",
            "Speed: 2.4ms preprocess, 81.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.0ms\n",
            "Speed: 2.5ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.4ms\n",
            "Speed: 2.8ms preprocess, 85.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.9ms\n",
            "Speed: 2.5ms preprocess, 83.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 86.0ms\n",
            "Speed: 2.7ms preprocess, 86.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 83.2ms\n",
            "Speed: 2.7ms preprocess, 83.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.7ms\n",
            "Speed: 2.7ms preprocess, 84.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 82.7ms\n",
            "Speed: 2.7ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 85.4ms\n",
            "Speed: 2.8ms preprocess, 85.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 42.3ms\n",
            "Speed: 2.8ms preprocess, 42.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 86.8ms\n",
            "Speed: 2.5ms preprocess, 86.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 86.7ms\n",
            "Speed: 2.8ms preprocess, 86.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 86.4ms\n",
            "Speed: 2.6ms preprocess, 86.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 85.3ms\n",
            "Speed: 2.7ms preprocess, 85.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 85.7ms\n",
            "Speed: 2.7ms preprocess, 85.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 87.3ms\n",
            "Speed: 2.5ms preprocess, 87.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 42.8ms\n",
            "Speed: 2.7ms preprocess, 42.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 85.3ms\n",
            "Speed: 2.6ms preprocess, 85.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.1ms\n",
            "Speed: 2.7ms preprocess, 83.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 82.3ms\n",
            "Speed: 2.7ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 82.9ms\n",
            "Speed: 2.7ms preprocess, 82.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 86.7ms\n",
            "Speed: 2.7ms preprocess, 86.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 82.9ms\n",
            "Speed: 2.8ms preprocess, 82.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 84.9ms\n",
            "Speed: 2.5ms preprocess, 84.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 82.5ms\n",
            "Speed: 2.6ms preprocess, 82.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 41.9ms\n",
            "Speed: 2.5ms preprocess, 41.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 87.8ms\n",
            "Speed: 2.7ms preprocess, 87.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.5ms\n",
            "Speed: 2.5ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.6ms\n",
            "Speed: 2.6ms preprocess, 83.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.2ms\n",
            "Speed: 2.6ms preprocess, 85.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 40.7ms\n",
            "Speed: 2.7ms preprocess, 40.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 86.5ms\n",
            "Speed: 2.7ms preprocess, 86.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.1ms\n",
            "Speed: 2.5ms preprocess, 84.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.4ms\n",
            "Speed: 2.5ms preprocess, 85.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 42.4ms\n",
            "Speed: 2.6ms preprocess, 42.4ms inference, 44.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 13 Cars, 83.7ms\n",
            "Speed: 2.4ms preprocess, 83.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.9ms\n",
            "Speed: 2.6ms preprocess, 82.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.8ms\n",
            "Speed: 2.6ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 41.9ms\n",
            "Speed: 2.7ms preprocess, 41.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 88.3ms\n",
            "Speed: 2.6ms preprocess, 88.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 82.8ms\n",
            "Speed: 2.6ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.6ms\n",
            "Speed: 2.6ms preprocess, 84.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.2ms\n",
            "Speed: 2.7ms preprocess, 85.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.5ms\n",
            "Speed: 2.6ms preprocess, 84.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 86.3ms\n",
            "Speed: 2.6ms preprocess, 86.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 82.6ms\n",
            "Speed: 2.8ms preprocess, 82.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 85.6ms\n",
            "Speed: 2.7ms preprocess, 85.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 81.9ms\n",
            "Speed: 2.6ms preprocess, 81.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 83.5ms\n",
            "Speed: 2.6ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 86.8ms\n",
            "Speed: 2.6ms preprocess, 86.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 85.1ms\n",
            "Speed: 2.6ms preprocess, 85.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 40.8ms\n",
            "Speed: 2.5ms preprocess, 40.8ms inference, 41.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 87.3ms\n",
            "Speed: 2.6ms preprocess, 87.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.5ms\n",
            "Speed: 2.5ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.3ms\n",
            "Speed: 2.6ms preprocess, 85.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.1ms\n",
            "Speed: 2.5ms preprocess, 82.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 84.4ms\n",
            "Speed: 2.5ms preprocess, 84.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.6ms\n",
            "Speed: 2.4ms preprocess, 83.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.3ms\n",
            "Speed: 2.5ms preprocess, 83.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 41.1ms\n",
            "Speed: 2.6ms preprocess, 41.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.2ms\n",
            "Speed: 2.5ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 84.9ms\n",
            "Speed: 2.7ms preprocess, 84.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.7ms\n",
            "Speed: 2.7ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 81.4ms\n",
            "Speed: 2.5ms preprocess, 81.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 80.2ms\n",
            "Speed: 2.6ms preprocess, 80.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.5ms\n",
            "Speed: 2.6ms preprocess, 83.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 Buss, 11 Cars, 83.9ms\n",
            "Speed: 2.4ms preprocess, 83.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 84.4ms\n",
            "Speed: 2.4ms preprocess, 84.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 41.8ms\n",
            "Speed: 2.6ms preprocess, 41.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 86.3ms\n",
            "Speed: 2.5ms preprocess, 86.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.1ms\n",
            "Speed: 2.5ms preprocess, 85.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 84.2ms\n",
            "Speed: 2.4ms preprocess, 84.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.7ms\n",
            "Speed: 2.5ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.9ms\n",
            "Speed: 2.5ms preprocess, 82.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 86.4ms\n",
            "Speed: 2.5ms preprocess, 86.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 86.7ms\n",
            "Speed: 2.3ms preprocess, 86.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.2ms\n",
            "Speed: 2.5ms preprocess, 85.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 40.5ms\n",
            "Speed: 2.3ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 Buss, 11 Cars, 85.5ms\n",
            "Speed: 2.5ms preprocess, 85.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 Buss, 11 Cars, 83.8ms\n",
            "Speed: 2.3ms preprocess, 83.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.0ms\n",
            "Speed: 2.5ms preprocess, 83.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 41.3ms\n",
            "Speed: 2.5ms preprocess, 41.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 84.3ms\n",
            "Speed: 2.4ms preprocess, 84.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.1ms\n",
            "Speed: 2.3ms preprocess, 85.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.8ms\n",
            "Speed: 2.5ms preprocess, 85.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 80.3ms\n",
            "Speed: 2.4ms preprocess, 80.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 87.1ms\n",
            "Speed: 2.7ms preprocess, 87.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 83.4ms\n",
            "Speed: 2.4ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 85.0ms\n",
            "Speed: 2.6ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 83.4ms\n",
            "Speed: 2.4ms preprocess, 83.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 40.2ms\n",
            "Speed: 2.4ms preprocess, 40.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 81.9ms\n",
            "Speed: 2.4ms preprocess, 81.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.7ms\n",
            "Speed: 2.6ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 81.2ms\n",
            "Speed: 2.5ms preprocess, 81.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.4ms\n",
            "Speed: 2.6ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 40.4ms\n",
            "Speed: 2.3ms preprocess, 40.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 84.3ms\n",
            "Speed: 2.5ms preprocess, 84.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 85.0ms\n",
            "Speed: 2.9ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.9ms\n",
            "Speed: 2.3ms preprocess, 83.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 44.9ms\n",
            "Speed: 2.8ms preprocess, 44.9ms inference, 40.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 85.4ms\n",
            "Speed: 2.4ms preprocess, 85.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 86.5ms\n",
            "Speed: 2.4ms preprocess, 86.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.8ms\n",
            "Speed: 2.5ms preprocess, 83.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 81.0ms\n",
            "Speed: 2.4ms preprocess, 81.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 11 Cars, 87.5ms\n",
            "Speed: 2.4ms preprocess, 87.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 87.9ms\n",
            "Speed: 2.5ms preprocess, 87.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 96.2ms\n",
            "Speed: 2.5ms preprocess, 96.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 87.9ms\n",
            "Speed: 2.6ms preprocess, 87.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 87.0ms\n",
            "Speed: 2.4ms preprocess, 87.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 41.6ms\n",
            "Speed: 44.7ms preprocess, 41.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 87.4ms\n",
            "Speed: 2.5ms preprocess, 87.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 86.3ms\n",
            "Speed: 2.5ms preprocess, 86.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 83.0ms\n",
            "Speed: 2.8ms preprocess, 83.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 10 Cars, 82.7ms\n",
            "Speed: 2.5ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.4ms\n",
            "Speed: 2.4ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 82.3ms\n",
            "Speed: 2.6ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 84.6ms\n",
            "Speed: 2.5ms preprocess, 84.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 84.3ms\n",
            "Speed: 2.3ms preprocess, 84.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 43.0ms\n",
            "Speed: 2.4ms preprocess, 43.0ms inference, 43.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 81.9ms\n",
            "Speed: 2.6ms preprocess, 81.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.9ms\n",
            "Speed: 2.4ms preprocess, 83.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.6ms\n",
            "Speed: 2.5ms preprocess, 83.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 84.4ms\n",
            "Speed: 2.3ms preprocess, 84.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.8ms\n",
            "Speed: 2.4ms preprocess, 85.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.7ms\n",
            "Speed: 2.5ms preprocess, 83.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 86.0ms\n",
            "Speed: 2.5ms preprocess, 86.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 82.9ms\n",
            "Speed: 2.5ms preprocess, 82.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 40.9ms\n",
            "Speed: 2.4ms preprocess, 40.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 88.1ms\n",
            "Speed: 2.5ms preprocess, 88.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 81.2ms\n",
            "Speed: 2.5ms preprocess, 81.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.5ms\n",
            "Speed: 2.3ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 83.3ms\n",
            "Speed: 2.4ms preprocess, 83.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 85.5ms\n",
            "Speed: 2.4ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.5ms\n",
            "Speed: 2.5ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.5ms\n",
            "Speed: 2.5ms preprocess, 83.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 86.0ms\n",
            "Speed: 2.6ms preprocess, 86.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 41.7ms\n",
            "Speed: 2.5ms preprocess, 41.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 82.8ms\n",
            "Speed: 2.5ms preprocess, 82.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.4ms\n",
            "Speed: 2.5ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 82.5ms\n",
            "Speed: 2.4ms preprocess, 82.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 84.6ms\n",
            "Speed: 2.4ms preprocess, 84.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 14 Cars, 82.7ms\n",
            "Speed: 2.4ms preprocess, 82.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 14 Cars, 82.7ms\n",
            "Speed: 2.5ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 83.9ms\n",
            "Speed: 2.5ms preprocess, 83.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 82.7ms\n",
            "Speed: 2.3ms preprocess, 82.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 40.6ms\n",
            "Speed: 2.3ms preprocess, 40.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 84.9ms\n",
            "Speed: 2.3ms preprocess, 84.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 14 Cars, 83.0ms\n",
            "Speed: 2.4ms preprocess, 83.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 15 Cars, 85.7ms\n",
            "Speed: 2.3ms preprocess, 85.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 15 Cars, 83.0ms\n",
            "Speed: 2.4ms preprocess, 83.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 15 Cars, 88.5ms\n",
            "Speed: 2.6ms preprocess, 88.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 15 Cars, 83.7ms\n",
            "Speed: 2.4ms preprocess, 83.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 15 Cars, 84.0ms\n",
            "Speed: 2.4ms preprocess, 84.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.8ms\n",
            "Speed: 2.4ms preprocess, 83.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 13 Cars, 40.6ms\n",
            "Speed: 2.4ms preprocess, 40.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 13 Cars, 86.4ms\n",
            "Speed: 2.4ms preprocess, 86.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 83.2ms\n",
            "Speed: 2.5ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 84.6ms\n",
            "Speed: 2.5ms preprocess, 84.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 15 Cars, 41.0ms\n",
            "Speed: 2.4ms preprocess, 41.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 88.9ms\n",
            "Speed: 2.5ms preprocess, 88.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 13 Cars, 81.7ms\n",
            "Speed: 2.5ms preprocess, 81.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 13 Cars, 84.7ms\n",
            "Speed: 2.5ms preprocess, 84.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 13 Cars, 82.7ms\n",
            "Speed: 2.6ms preprocess, 82.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 13 Cars, 87.3ms\n",
            "Speed: 2.6ms preprocess, 87.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Bus, 12 Cars, 85.5ms\n",
            "Speed: 2.5ms preprocess, 85.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.4ms\n",
            "Speed: 2.4ms preprocess, 83.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.2ms\n",
            "Speed: 2.3ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 84.3ms\n",
            "Speed: 2.5ms preprocess, 84.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.2ms\n",
            "Speed: 2.5ms preprocess, 83.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 81.5ms\n",
            "Speed: 2.5ms preprocess, 81.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 83.5ms\n",
            "Speed: 2.4ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 41.1ms\n",
            "Speed: 2.4ms preprocess, 41.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 12 Cars, 85.4ms\n",
            "Speed: 2.5ms preprocess, 85.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 85.5ms\n",
            "Speed: 2.4ms preprocess, 85.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 Van, 11 Cars, 83.8ms\n",
            "Speed: 2.4ms preprocess, 83.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Middle frame saved as 'middle_frame.jpg'.\n",
            "Peak frame saved as 'peak_frame.jpg'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a9cb3cadd594aa1997683768f0f8b0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot model error: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
            "Ensure you are using a GPU runtime (e.g., T4 or V100) and have installed bitsandbytes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwcAAAGJCAYAAADffU/OAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPn5JREFUeJzt3X18zfXj//Hn2cwZdiGZzVzNRch1rTAyYdGSqBCfwlD5+bgoo08pbUMsXfChC0qxSPIh1CefluVilOvrXKTRUCLEzIxh5/37g53vObaxnc3Z5jzut9v7dnNe7/f79X69z96dzvO8Xq/322QYhiEAAAAALs+tqBsAAAAAoHggHAAAAACQRDgAAAAAcA3hAAAAAIAkwgEAAACAawgHAAAAACQRDgAAAABcQzgAAAAAIIlwAAAAAOAawgGAIjd37lzVr19fHh4eKl++vLX87bffVq1ateTu7q5mzZpJkoKCghQREVEk7SwqMTExMplMRd0MAIALIBwAyJHJZMrTsnr16gId55dfflFERIRq166tmTNn6uOPP5YkLV++XP/617/UunVrzZ49WxMnTiyEs7IXFBRkPQ83NzeVL19ejRs31vPPP6+NGzcW+vEK08SJE7V06dJCrXP16tUymUxatGhRjusjIiLk5eVVqMe83rp16xQTE6OUlJRbehwAQM5KFXUDABRPc+fOtXs9Z84cJSQkZCu/++67C3Sc1atXy2KxaOrUqapTp461fOXKlXJzc9Onn36q0qVLW8v3798vN7fC+12jWbNmGjlypCTp3Llz2rdvnxYuXKiZM2dqxIgRmjx5cqEdy1FjxozRK6+8Ylc2ceJEde/eXd26dSuaRt0i69at09ixYxUREWHXiwQAcA7CAYAcPfPMM3avN2zYoISEhGzl10tPT1fZsmXzfJwTJ05IUrYvgidOnFCZMmXsgoEkmc3mPNedF1WqVMl2TpMmTdI//vEPTZkyRXfddZcGDx5cqMfMr1KlSqlUKT6uAQC3HsOKADjswQcfVKNGjbR161aFhoaqbNmyevXVVyVJX3/9tTp37qzAwECZzWbVrl1b48ePV2ZmpnX/oKAgRUdHS5L8/PxkMpms4+tnz56t8+fPW4f9xMXFWfe5fs5BSkqKRowYoaCgIJnNZlWtWlV9+/bVqVOnHDqvMmXKaO7cuapQoYImTJggwzCs6ywWi/7973+rYcOG8vT0lL+/vwYNGqQzZ87Y1REUFKRHH31UP/74o5o3by5PT0/VqlVLc+bMsdvu8uXLGjt2rO666y55enrqzjvv1AMPPKCEhATrNtfPOTCZTDp//rw+++wz6/sTERGhVatWyWQyacmSJdnO6YsvvpDJZNL69esdek9u5LvvvlObNm1Urlw5eXt7q3PnztqzZ4/dNrt27VJERIRq1aolT09PBQQEaMCAAfr777/tzvOll16SJNWsWdN6bocOHbKe99ChQ7Vw4UI1aNBAZcqUUUhIiH7++WdJ0kcffaQ6derI09NTDz74oHW/LGvXrlWPHj1UvXp1mc1mVatWTSNGjNCFCxfstssaPvXbb7+pU6dOKleunAIDAzVu3Di7awEAbkf8FAWgQP7++2+Fh4erV69eeuaZZ+Tv7y9JiouLk5eXlyIjI+Xl5aWVK1cqKipKqampevvttyVJ//73vzVnzhwtWbJE06dPl5eXl5o0aaI6dero448/1qZNm/TJJ59Iklq1apXj8dPS0tSmTRvt27dPAwYM0L333qtTp07pm2++0R9//KGKFSs6dF5eXl56/PHH9emnn2rv3r1q2LChJGnQoEGKi4tT//79NXz4cCUnJ+v999/X9u3b9dNPP8nDw8Nax4EDB9S9e3cNHDhQ/fr106xZsxQREaHg4GBrfTExMYqNjdWzzz6r5s2bKzU1VVu2bNG2bdv00EMP5di2uXPnWrd//vnnJUm1a9dWy5YtVa1aNc2bN0+PP/643T7z5s1T7dq1FRISctNzP3fuXI7BKiMjI8e29OvXT506ddKkSZOUnp6u6dOn64EHHtD27dsVFBQkSUpISNBvv/2m/v37KyAgQHv27NHHH3+sPXv2aMOGDTKZTHriiSf066+/av78+ZoyZYr1b+fn52c93tq1a/XNN99oyJAhkqTY2Fg9+uij+te//qUPP/xQ//znP3XmzBm99dZbGjBggFauXGndd+HChUpPT9fgwYN15513atOmTXrvvff0xx9/aOHChXbnlZmZqYcfflgtW7bUW2+9pfj4eEVHR+vKlSsaN27cTd9DACixDADIgyFDhhjXf2S0bdvWkGTMmDEj2/bp6enZygYNGmSULVvWuHjxorUsOjrakGScPHnSbtt+/foZ5cqVy1ZHjRo1jH79+llfR0VFGZKMxYsXZ9vWYrHc8Jxq1KhhdO7cOdf1U6ZMMSQZX3/9tWEYhrF27VpDkjFv3jy77eLj47OV16hRw5BkrFmzxlp24sQJw2w2GyNHjrSWNW3a9IZtMIz/e49slStXzu59yDJ69GjDbDYbKSkpdsctVaqUER0dfcPjrFq1ypB0w8X2b3Lu3DmjfPnyxnPPPWdXz/Hjxw1fX1+78pyuh/nz52d7j95++21DkpGcnJxte0mG2Wy2W/fRRx8ZkoyAgAAjNTXV7n24vp6c2hAbG2uYTCbj8OHD1rJ+/foZkoxhw4ZZyywWi9G5c2ejdOnS2a5VALidMKwIQIGYzWb1798/W3mZMmWs/876JbpNmzZKT0/XL7/8UmjH/+qrr9S0adNsv5RLKvDtP7PuzHPu3DlJV3959vX11UMPPaRTp05Zl+DgYHl5eWnVqlV2+zdo0EBt2rSxvvbz81O9evX022+/WcvKly+vPXv2KCkpqUBtzdK3b19lZGTY3XFowYIFunLlyk3ni2SJiopSQkJCtqVjx4522yUkJCglJUW9e/e2ez/c3d3VokULu/fD9nq4ePGiTp06pZYtW0qStm3blufz69Chg7U3QpJatGghSXryySfl7e2drdz2vbZtw/nz53Xq1Cm1atVKhmFo+/bt2Y41dOhQ67+zhjRdunRJP/zwQ57bCwAlDcOKABRIlSpVsk0alqQ9e/ZozJgxWrlypVJTU+3WnT17ttCOf/DgQT355JOFVp+ttLQ0SbJ+6UxKStLZs2dVqVKlHLfPmlydpXr16tm2ueOOO+zmJ4wbN05du3ZV3bp11ahRIz388MPq06ePmjRp4lCb69evr/vvv1/z5s3TwIEDJV0dUtSyZUu7u0HdSOPGjRUWFpat/PPPP7d7nRVo2rdvn2M9Pj4+1n+fPn1aY8eO1ZdffpntfcrP9XD9e+rr6ytJqlatWo7ltu/1kSNHFBUVpW+++SbbHJHr2+Dm5qZatWrZldWtW1eSss1lAIDbCeEAQIHY/hqbJSUlRW3btpWPj4/GjRun2rVry9PTU9u2bdPLL78si8VSBC3Nv927d0uS9Uu1xWJRpUqVNG/evBy3tx0bL0nu7u45bmfYTGoNDQ3VwYMH9fXXX2v58uX65JNPNGXKFM2YMUPPPvusQ+3u27evXnjhBf3xxx/KyMjQhg0b9P777ztU141k/R3nzp2rgICAbOtt77DUs2dPrVu3Ti+99JKaNWsmLy8vWSwWPfzww/m6HnJ7T2/2XmdmZuqhhx7S6dOn9fLLL6t+/foqV66cjh49qoiIiBJzTQLArUY4AFDoVq9erb///luLFy9WaGiotTw5ObnQj1W7dm3rl/jClJaWpiVLlqhatWrWZznUrl1bP/zwg1q3bp1jKHJUhQoV1L9/f/Xv319paWkKDQ1VTEzMDcPBjYZM9erVS5GRkZo/f74uXLggDw8PPfXUU4XW3iy1a9eWJFWqVCnHnoYsZ86c0YoVKzR27FhFRUVZy3MaSnWrngT9888/69dff9Vnn32mvn37Wstt7wply2Kx6LfffrP2FkjSr7/+Kkl2w5oA4HbDnAMAhS7rV1zbX8gvXbqkDz/8sNCP9eSTT2rnzp053r7TcPC2kxcuXFCfPn10+vRpvfbaa9YvrD179lRmZqbGjx+fbZ8rV6449FRf21t5SlfnOdSpUyfHOwPZKleuXK7Hq1ixosLDw/X5559r3rx5evjhhx2+a9ONdOrUST4+Ppo4caIuX76cbf3Jkycl5Xw9SFfvVnW9cuXKSVKhPyE5pzYYhqGpU6fmuo9tb4thGHr//ffl4eGhDh06FGrbAKA4oecAQKFr1aqV7rjjDvXr10/Dhw+XyWTS3Llzb8k94l966SUtWrRIPXr00IABAxQcHKzTp0/rm2++0YwZM9S0adMb7n/06FHrWPq0tDTt3btXCxcu1PHjxzVy5EgNGjTIum3btm01aNAgxcbGaseOHerYsaM8PDyUlJSkhQsXaurUqerevXu+2t+gQQM9+OCDCg4OVoUKFbRlyxYtWrTIbjJsToKDg/XDDz9o8uTJCgwMVM2aNa2TcKWrQ4uy2pJTmCkMPj4+mj59uvr06aN7771XvXr1kp+fn44cOaJly5apdevWev/99+Xj46PQ0FC99dZbunz5sqpUqaLly5fn2JMUHBwsSXrttdfUq1cveXh4qEuXLtbQ4Kj69eurdu3aGjVqlI4ePSofHx999dVX2eYeZPH09FR8fLz69eunFi1a6LvvvtOyZcv06quvZhs+BgC3E8IBgEJ355136ttvv9XIkSM1ZswY3XHHHXrmmWfUoUMHderUqVCP5eXlpbVr1yo6OlpLlizRZ599pkqVKqlDhw6qWrXqTfffsWOH+vTpI5PJJG9vb1WrVk1dunSxPkfgejNmzFBwcLA++ugjvfrqqypVqpSCgoL0zDPPqHXr1vlu//Dhw/XNN99o+fLlysjIUI0aNfTGG29YHwaWm8mTJ+v555/XmDFjdOHCBeuX2CxdunTRHXfcIYvFosceeyzf7cqrf/zjHwoMDNSbb76pt99+WxkZGapSpYratGljdxerL774QsOGDdMHH3wgwzDUsWNHfffddwoMDLSr7/7779f48eM1Y8YMxcfHy2KxKDk5ucDhwMPDQ//97381fPhwxcbGytPTU48//riGDh2aY4B0d3dXfHy8Bg8erJdeekne3t6Kjo62GxYFALcjk3ErfsoDABSpK1euKDAwUF26dNGnn35a1M0pUSIiIrRo0SLr3aoAwJUw5wAAbkNLly7VyZMn7SbfAgBwMwwrAoDbyMaNG7Vr1y6NHz9e99xzj9q2bVvUTQIAlCD0HADAbWT69OkaPHiwKlWqpDlz5hR1cwAAJQxzDgAAAIAiEBMTo7Fjx9qV1atXT7/88ksRtYhhRQAAAECRadiwoX744Qfra9unyxcFwgEAAABQREqVKqWAgICiboYVcw4AAACAQpKRkaHU1FS75UZPvU9KSlJgYKBq1aqlp59+WkeOHHFia7O7LeccGIseyfc+lqVHJUlu3aoU7ODNQ+1fb1rjcFVZbcryy7JUSVL9zj52r2+lRSml87V99/KXbrj++rZn1X+z/Rxxfd25HSurTVlyel+ztrFeH1l/55v8fbP+hnn9W+Xn/c7tPbv+fK6XdQ7XX/O5vc6Pm+2b2/tws79BTu/L9X/X64152jPHuhz97+dm72tOcjv29Z8z+b1OblTf3uf25budOdWZG0c+dxrMvDvH8tw+d03drz7R2TiSYL/Dtf/err++3Cb2tq/31fn/ty6XazLrPKxtu+6/6dz+JlnbZ2tDPv7fUWj/v3GG6/+flpvrPgvz83lSkM+cnOT1/5U3utavr8OZbnZd5HYtZ7n+vPP6mWd7rs74fiFJDc9kf0p6cTHWVM/hfY3o3tnmEURHRysmJibbtt99953S0tJUr149HTt2TGPHjtXRo0e1e/dueXt7O9yGgmBYEQAAAGCjIENr/jV6tCIjI+3KzGZzjtuGh4db/92kSRO1aNFCNWrU0H/+8x8NHDiwAK1wHOEAAAAAsFGQcGA2m3MNAzdTvnx51a1bVwcOHChACwqGOQcAAACADbcCLAWRlpamgwcPqnLlygWsyXGEAwAAAKAIjBo1SomJiTp06JDWrVunxx9/XO7u7urdu/fNd75FGFYEAAAA2HDWr+d//PGHevfurb///lt+fn564IEHtGHDBvn5+TmpBdkRDgAAAAAbzgoHX375pZOOlHeEAwAAAMCGqagbUIQIBwAAAIANV56USzgAAAAAbLhyOHDlcwcAAABgg54DAAAAwIYr/3pOOAAAAABsEA4AAAAASCIcAAAAALiGcAAAAABAkmuHA1c+dwAAAAA26DkAAAAAbLjyr+eEAwAAAMAG4QAAAACAJMIBAAAAgGsIBwAAAAAkSaaibkARcuVgBAAAAMAGPQcAAACADVf+9ZxwAAAAANggHAAAAACQRDgAAAAAcA3hAAAAAIAk1w4HrnzuAAAAAGzQcwAAAADYcOVfzwkHAAAAgA3CAQAAAABJhAMAAAAA1xAOAAAAAEhy7XDgyucOAAAAwAY9BwAAAIANU1E3oAgRDgAAAAAbrjy0hnAAAAAA2CAcAAAAAJBEOAAAAABwjcmFJx24cjACAAAAYIOeAwAAAMCGm8ko6iYUGcIBAAAAYMOVhxURDgAAAAAbLpwNCAcAAACALRPDigAAAABIrj2siLsVAQAAAJBEzwEAAABgx5V7DggHAAAAgA1uZQoAAABAEncrAgAAAHANw4oAAAAASHLtcMDdigAAAABIoucAAAAAsMND0AAAAABIktxceFgR4QAAAACw4cpzDggHAAAAgA2TXHdYEROSAQAAABsmk+NLQbz55psymUx68cUXC+U8HEE4AAAAAIrY5s2b9dFHH6lJkyZF2g7CAQAAAGDD2T0HaWlpevrppzVz5kzdcccdhXsy+UQ4AAAAAGy4mQyHl4yMDKWmptotGRkZNzzekCFD1LlzZ4WFhTnpDHNHOAAAAABsFKTnIDY2Vr6+vnZLbGxsrsf68ssvtW3bthtu40zcrQgAAACwUZB5xaNHj1ZkZKRdmdlsznHb33//XS+88IISEhLk6elZgKMWHsIBAAAAYKMgT0g2m825hoHrbd26VSdOnNC9995rLcvMzNSaNWv0/vvvKyMjQ+7u7g63xRGEAwAAAKAIdOjQQT///LNdWf/+/VW/fn29/PLLTg8GEuEAAAAAsOOsJyR7e3urUaNGdmXlypXTnXfema3cWQgHAAAAgA03J4WD4ohwAAAAANgoyJyDglq9enWRHVsiHAAAAAB2XLjjgOccAAAAALiKngMAAADAhrMmJBdHhAMAAADARlHOOShqhAMAAADABncrAgAAACCJYUUAAAAArnHlcMDdigAAAABIoucAAAAAsGMSE5IBAAAAyLWHFREOAAAAABsmF75dEeEAAAAAsGFy4Vm5hAMAAADAhisPK3LhXAQAAADAFj0HAAAAgC3mHAAAAACQmHMAAAAA4BqTC086IBwAAAAANug5AAAAAHCVC/ccuHAuAgAAAGCLngMAAADABsOKAAAAAEiSTNzKFAAAAIDk0lMOCAcAAACALYYVAQAAALjKhYcVuXAuAgAAAGCLngMAAADABnMOAAAAAEjibkUAAAAArmFCMgAAAABJksmFxxURDgAAAABbLtxz4MKnDgAAAMAWPQcAAACADRceVUQ4AAAAAGxxtyIAAAAAkrhbEQAAAIAsLjyuiHAAAAAA2HDlngMXPnUAAAAAtug5AAAAAGwwIRkAAACAJJeecsCwIgAAAMCWyc3k8OJM7du3V0pKSrby1NRUtW/f3qE66TkAAAAAbJWQnoPVq1fr0qVL2covXryotWvXOlQn4QAAAACwUdzvVrRr1y7rv/fu3avjx49bX2dmZio+Pl5VqlRxqG7CAQAAAFCCNGvWTCaTSSaTKcfhQ2XKlNF7773nUN2EAwAAAMBGcb9bUXJysgzDUK1atbRp0yb5+flZ15UuXVqVKlWSu7u7Q3UTDgAAAAAbxf1uRTVq1JAkWSyWQq+bcAAAAADYKO49B7aSkpK0atUqnThxIltYiIqKynd9hAMAAADAVjGfkJxl5syZGjx4sCpWrKiAgACZbLo8TCYT4QAAAAAosBLSc/DGG29owoQJevnllwutzhKSiwAAAADYOnPmjHr06FGodRIOAAAAAFtuBVicqEePHlq+fHmh1smwIgAAAMCWk4YVTZ8+XdOnT9ehQ4ckSQ0bNlRUVJTCw8PztH+dOnX0+uuva8OGDWrcuLE8PDzs1g8fPjzfbSIcAAAAALac1ANQtWpVvfnmm7rrrrtkGIY+++wzde3aVdu3b1fDhg1vuv/HH38sLy8vJSYmKjEx0W6dyWQiHAAAAAAF5qSegy5duti9njBhgqZPn64NGzbkKRwkJycXepsIBwAAAICtAoSDjIwMZWRk2JWZzWaZzeYb7peZmamFCxfq/PnzCgkJcfj4BUU4AAAAAApJbGysxo4da1cWHR2tmJiYHLf/+eefFRISoosXL8rLy0tLlixRgwYN8nSsAQMG3HD9rFmz8lSPLcIBAAAAYKsAcw5Gjx6tyMhIu7Ib9RrUq1dPO3bs0NmzZ7Vo0SL169dPiYmJeQoIZ86csXt9+fJl7d69WykpKWrfvr1D7SccAAAAALYKMKwoL0OIbJUuXVp16tSRJAUHB2vz5s2aOnWqPvroo5vuu2TJkmxlFotFgwcPVu3atfPeaBs85wAAAACwVYTPObBYLNnmLOSHm5ubIiMjNWXKFIf2p+cAAAAAsOWkuxWNHj1a4eHhql69us6dO6cvvvhCq1ev1vfff1+geg8ePKgrV644tC/hAAAAALDlnGygEydOqG/fvjp27Jh8fX3VpEkTff/993rooYfytP/1cxsMw9CxY8e0bNky9evXz6E2EQ4AAACAIvDpp58WaP/t27fbvXZzc5Ofn5/efffdm97JKDeEAwAAAMCWk4YVFdSqVasKvU6HwsFvv/2mWrVqFXZbAAAAgKJXQsJBlpMnT2r//v2Srt4a1c/Pz+G6HJpTXadOHbVr106ff/65Ll686PDBAQAAgGKnCO9WlB/nz5/XgAEDVLlyZYWGhio0NFSBgYEaOHCg0tPTHarToVPYtm2bmjRposjISAUEBGjQoEHatGmTQw0AAAAAihU3k+OLE0VGRioxMVH//e9/lZKSopSUFH399ddKTEzUyJEjHarToXDQrFkzTZ06VX/++admzZqlY8eO6YEHHlCjRo00efJknTx50qHGAAAAAEXN5Ob44kxfffWVPv30U4WHh8vHx0c+Pj565JFHNHPmTC1atMihOgt0CqVKldITTzyhhQsXatKkSTpw4IBGjRqlatWqWW/LBAAAAKDwpaeny9/fP1t5pUqVnDusKMuWLVv0z3/+U5UrV9bkyZM1atQoHTx4UAkJCfrzzz/VtWvXglQPAAAAOF8JGVYUEhKi6OhouznAFy5c0NixYxUSEuJQnQ7drWjy5MmaPXu29u/fr0ceeURz5szRI488Ije3q1mjZs2aiouLU1BQkEONAgAAAIqMk4cHOWrq1Knq1KmTqlatqqZNm0qSdu7cKU9PT4efsuxQOJg+fboGDBigiIgIVa5cOcdtKlWqVOAHOwAAAABOV0JuZdqoUSMlJSVp3rx5+uWXXyRJvXv31tNPP60yZco4VKdD4SAhIUHVq1e39hRkMQxDv//+u6pXr67SpUs7/NhmAAAAoMiUkHAgSWXLltVzzz1XaPU51GlSu3ZtnTp1Klv56dOnVbNmzQI3CgAAACgyxfw5B1u3blW7du2Umpqabd3Zs2fVrl077dy506G6HToFwzByLE9LS5Onp6dDDQEAAABwc++++67at28vHx+fbOt8fX310EMP6e2333ao7nwNK4qMjJQkmUwmRUVFqWzZstZ1mZmZ2rhxo5o1a+ZQQwAAAIBioZgPK9q4caNeeeWVXNd36dJFn3zyiUN15yscbN++XdLVnoOff/5ZpUuXtq4rXbq0mjZtqlGjRjnUEAAAAKBYKOZ3Kzp69Ki8vb1zXe/l5eXw88byFQ5WrVolSerfv7+mTp2aY1cGAAAAUKIV854DPz8/7d+/P9e5vr/88osqVqzoUN0O5aLZs2cTDAAAAHB7KuYTksPCwjRhwoQc1xmGoQkTJigsLMyhuvPcc/DEE08oLi5OPj4+euKJJ2647eLFix1qDAAAAIAbGzNmjIKDg9WiRQuNHDlS9erVk3S1x+Ddd9/Vr7/+qri4OIfqznM48PX1lclksv4bAAAAuC0V82FFtWvX1g8//KCIiAj16tXL+h3dMAw1aNBACQkJqlOnjkN15zkczJ49O8d/AwAAALeVYj4hWZLuu+8+7d69Wzt27FBSUpIMw1DdunULfOdQh56QfOHCBRmGYb2V6eHDh7VkyRI1aNBAHTt2LFCDAAAAgCJVzHsObDVr1qxQHyXgUC7q2rWr5syZI0lKSUlR8+bN9e6776pr166aPn16oTUOAAAAcDo3k+NLCedQONi2bZvatGkjSVq0aJECAgJ0+PBhzZkzR9OmTSvUBgIAAABOVczvVnQrOTSsKD093frgheXLl+uJJ56Qm5ubWrZsqcOHD+e5nlOnTmnWrFlav369jh8/LkkKCAhQq1atFBERIT8/P0eaBwAAAMABDuWbOnXqaOnSpfr999/1/fffW+cZnDhxIs/PP9i8ebPq1q2radOmydfXV6GhoQoNDZWvr6+mTZum+vXra8uWLTetJyMjQ6mpqXZLxuVMR04LAAAAYFhRfkVFRWnUqFEKCgpSixYtFBISIulqL8I999yTpzqGDRumHj166Pfff1dcXJwmTZqkSZMmKS4uTkeOHFH37t01bNiwm9YTGxsrX19fuyV2yW+OnBYAAABQooYVrV27Vs8884xCQkJ09OhRSdLcuXP1448/OlSfQ6fQvXt3HTlyRFu2bFF8fLy1vEOHDpoyZUqe6ti5c6dGjBhhvS+rLZPJpBEjRmjHjh03rWf06NE6e/as3TL68Vp5PhcAAADAjsnk+OJEX331lTp16qQyZcpo+/btysjIkCSdPXtWEydOdKhOh/NNQECA7rnnHrm5/V8VzZs3V/369fO8/6ZNm3Jdv2nTJvn7+9+0HrPZLB8fH7vF7OGepzYAAAAA2ZgKsDjRG2+8oRkzZmjmzJny8PCwlrdu3Vrbtm1zqE6HJiSfP39eb775plasWKETJ07IYrHYrf/tt5sP6xk1apSef/55bd26VR06dLAGgb/++ksrVqzQzJkz9c477zjSPAAAAMBxTu4BcNT+/fsVGhqardzX11cpKSkO1elQOHj22WeVmJioPn36qHLlyjkODbqZIUOGqGLFipoyZYo+/PBDZWZenUTs7u6u4OBgxcXFqWfPno40DwAAALjtBQQE6MCBAwoKCrIr//HHH1WrlmPD7B0KB999952WLVum1q1bO3TQLE899ZSeeuopXb58WadOnZIkVaxY0a5bBAAAAHCqktFxoOeee04vvPCCZs2aJZPJpD///FPr16/XqFGj9PrrrztUp0Ph4I477lCFChUcOmBOPDw8VLly5UKrDwAAAHBYCRlW9Morr8hisahDhw5KT09XaGiozGazRo0alae7fubEoQnJ48ePV1RUlNLT0x06KAAAAFBslZBbmZpMJr322ms6ffq0du/erQ0bNujkyZMaP368w3U61HPw7rvv6uDBg/L391dQUFC2YUCOzo4GAAAAilwJ6TnIUrp0aTVo0KBQ6nIoHHTr1q1QDg4AAAAUO8U4GzzxxBN53nbx4sX5rt+hcBAdHe3IbgAAAAAKwNfX95bW71A4kKSUlBQtWrRIBw8e1EsvvaQKFSpo27Zt8vf3V5UqVQqzjQAAAIDzFONhRbNnz76l9TsUDnbt2qWwsDD5+vrq0KFDeu6551ShQgUtXrxYR44c0Zw5cwq7nQAAAIBzFN9sYCc5OVlXrlzRXXfdZVeelJQkDw+PbM8/yAuH5lRHRkYqIiJCSUlJ8vT0tJY/8sgjWrNmjSNVAgAAAMWDyeT44kQRERFat25dtvKNGzcqIiLCoTodCgebN2/WoEGDspVXqVJFx48fd6ghAAAAQLFQQm5lun379hwfStyyZUvt2LHDoTodGlZkNpuVmpqarfzXX3+Vn5+fQw0BAAAAioViPOfAlslk0rlz57KVnz17VpmZmQ7V6VC+eeyxxzRu3DhdvnzZ2rAjR47o5Zdf1pNPPulQQwAAAADkXWhoqGJjY+2CQGZmpmJjY/XAAw84VKfDD0Hr3r27/Pz8dOHCBbVt21bHjx9XSEiIJkyY4FBDAAAAgGKhZHQcaNKkSQoNDVW9evXUpk0bSdLatWuVmpqqlStXOlSnQ+HA19dXCQkJ+umnn7Rz506lpaXp3nvvVVhYmEONAAAAAIqNEjKsqEGDBtq1a5fef/997dy5U2XKlFHfvn01dOhQVahQwaE68x0OLBaL4uLitHjxYh06dEgmk0k1a9ZUQECADMOQqYS8mQAAAEBOStLX2cDAQE2cOLHQ6stXODAMQ4899pj+97//qWnTpmrcuLEMw9C+ffsUERGhxYsXa+nSpYXWOAAAAMDpinE62LVrlxo1aiQ3Nzft2rXrhts2adIk3/XnKxzExcVpzZo1WrFihdq1a2e3buXKlerWrZvmzJmjvn375rshAAAAQLFQfLOBmjVrpuPHj6tSpUpq1qyZTCaTDMPItp3JZHLojkX5Cgfz58/Xq6++mi0YSFL79u31yiuvaN68eYQDAAAA4BZITk62PjogOTm50OvPVzjYtWuX3nrrrVzXh4eHa9q0aQVuFAAAAFBk3Ipv10GNGjVy/HdhyVc4OH36tPz9/XNd7+/vrzNnzhS4UQAAAECRKb7ZIJukpCStWrVKJ06ckMVisVsXFRWV7/ryFQ4yMzNVqlTuu7i7u+vKlSv5bgQAAABQbBTjCcm2Zs6cqcGDB6tixYoKCAiwu2uoyWS69eHAMAxFRETIbDbnuD4jIyPfDQAAAACKlZKRDfTGG29owoQJevnllwutznyFg379+t10GyYjAwAAoERzUs9BbGysFi9erF9++UVlypRRq1atNGnSJNWrVy9P+585c0Y9evQo1DblKxzMnj27UA8OAAAAuKrExEQNGTJE999/v65cuaJXX31VHTt21N69e1WuXLmb7t+jRw8tX75c/+///b9Ca1O+n5AMAAAA3NacNKwoPj7e7nVcXJwqVaqkrVu3KjQ0NMd9bO8MWqdOHb3++uvasGGDGjduLA8PD7tthw8fnu82EQ4AAAAAWwW4lWlGRka2ebhmsznXObu2zp49K0mqUKFCrttMmTLF7rWXl5cSExOVmJhoV24ymQgHAAAAQIEVoOcgNjZWY8eOtSuLjo5WTEzMDfezWCx68cUX1bp1azVq1CjX7W7Fg89sEQ4AAAAAWwWYkDx69GhFRkbaleWl12DIkCHavXu3fvzxx3wf89KlS0pOTlbt2rVv+NiBvHAr0N4AAADA7cbk+GI2m+Xj42O33CwcDB06VN9++61WrVqlqlWr5rmZ6enpGjhwoMqWLauGDRvqyJEjkqRhw4bpzTffzOdJX0U4AAAAAIqAYRgaOnSolixZopUrV6pmzZr52n/06NHauXOnVq9eLU9PT2t5WFiYFixY4FCbGFYEAAAA2HLScw6GDBmiL774Ql9//bW8vb11/PhxSZKvr6/KlClz0/2XLl2qBQsWqGXLlnZPR27YsKEOHjzoUJvoOQAAAABsFWBYUX5Mnz5dZ8+e1YMPPqjKlStbl7z+6n/y5ElVqlQpW/n58+ftwkJ+0HMAAAAA2CrArUzzwzCMAu1/3333admyZRo2bJgkWQPBJ598opCQEIfqJBwAAAAAtpw0rKigJk6cqPDwcO3du1dXrlzR1KlTtXfvXq1bty7bcw/yimFFAAAAgC2TyfHFCXbv3i1JeuCBB7Rjxw5duXJFjRs31vLly1WpUiWtX79ewcHBDtVNzwEAAABQgjRp0kT333+/nn32WfXq1UszZ84stLrpOQAAAABsFfOeg8TERDVs2FAjR45U5cqVFRERobVr1xZK3YQDAAAAwJbJzfHFCdq0aaNZs2bp2LFjeu+995ScnKy2bduqbt26mjRpkvWWqI4gHAAAAAC23EyOL05Urlw59e/fX4mJifr111/Vo0cPffDBB6pevboee+wxh+okHAAAAAC2ivmwopzUqVNHr776qsaMGSNvb28tW7bMoXqYkAwAAADYctLwoMKyZs0azZo1S1999ZXc3NzUs2dPDRw40KG6CAcAAABACfPnn38qLi5OcXFxOnDggFq1aqVp06apZ8+eKleunMP1Eg4AAAAAW8X8IWjh4eH64YcfVLFiRfXt21cDBgxQvXr1CqVuwgEAAABgy8kTi/PLw8NDixYt0qOPPip3d/dCrZtwAAAAANgq5nMOvvnmm1tWN+EAAAAAsFXMhxXdSoQDAAAAwJYLh4Pi3WcCAAAAwGnoOQAAAABsFfM5B7cS4QAAAACwVczvVnQrEQ4AAAAAWy4854BwAAAAANhiWBEAAAAASS7dc+C6sQgAAACAHXoOAAAAAFtMSAYAAAAgiTkHAAAAAK5x4TkHhAMAAADAFuEAAAAAgCSXDgeuO6AKAAAAgB16DgAAAABbbq77+znhAAAAALDlwsOKCAcAAACALcIBAAAAAEku/ZwD1z1zAAAAAHboOQAAAABsuTGsCAAAAIDEnAMAAAAA17jwnAPCAQAAAGCLngMAAAAAklw6HLhunwkAAAAAO/QcAAAAALbcXPf3c8IBAAAAYMd1hxURDgAAAABbLjzngHAAAAAA2OJWpgAAAACuct2eA9eNRQAAAADs0HMAAAAA2GLOAQAAAABJzDkAAAAAkIWeAwAAAAASw4oAAAAAZHHdYUWue+YAAAAA7BAOAAAAAFsmk+NLPq1Zs0ZdunRRYGCgTCaTli5dWvjnkw+EAwAAAMCWE8PB+fPn1bRpU33wwQe34ETyjzkHAAAAgB3nTUgODw9XeHi40453M4QDAAAAwFYBnnOQkZGhjIwMuzKz2Syz2VzQVjkFw4oAAAAAWwUYVhQbGytfX1+7JTY2tqjPKM/oOQAAAAAKyejRoxUZGWlXVlJ6DSTCAQAAAHAdx+cclKQhRDkhHAAAAAC2CjDnoKQjHAAAAAA2TA7cktRRaWlpOnDggPV1cnKyduzYoQoVKqh69epOa0cWwgEAAABgx3nhYMuWLWrXrp31ddZ8hX79+ikuLs5p7chCOAAAAABsOXFY0YMPPijDMJx2vJtx3QFVAAAAAOzQcwAAAADYcd6wouKGcAAAAADYcuKE5OKGcAAAAADY4lamAAAAAK6i5wAAAACA5NLDily3zwQAAACAHXoOAAAAAFvMOQAAAABwlesOKyIcAAAAALZceM4B4QAAAACww7AiAAAAAJJL9xy4biwCAAAAYIeeAwAAAMCWC/ccEA4AAAAAO647uIZwAAAAANii5wAAAADAVYQDAAAAAJJLPyHZdc8cAAAAgB16DgAAAABbzDkAAAAAcBXhAAAAAIDk0nMOCAcAAACAHXoOAAAAAEguPefAdftMAAAAANih5wAAAACw47q/nxMOAAAAAFsuPKyIcAAAAADY4m5FAAAAAK6i5wAAAACA5NLDily3zwQAAACAHXoOAAAAADuu+/s54QAAAACw5cLDiggHAAAAgB16DgAAAABI9BwAAAAAuMaFw4Hr9pkAAAAAsEPPAQAAAGDHdX8/JxwAAAAAtlx4WBHhAAAAALBDOAAAAAAgSSaGFQEAAACQ5Mo9B64biwAAAADYoecAAAAAsMWwIgAAAABXue6wIsIBAAAAYItbmQIAAACQ5NLDilz3zAEAAADYoecAAAAAsMOwIgAAAAAScw4AAAAAZHHdkfeue+YAAABATkwmxxcHfPDBBwoKCpKnp6datGihTZs2FfIJ5R3hAAAAALDjVoAlfxYsWKDIyEhFR0dr27Ztatq0qTp16qQTJ04UypnkF+EAAAAAKCKTJ0/Wc889p/79+6tBgwaaMWOGypYtq1mzZhVJewgHAAAAgK0CDCvKyMhQamqq3ZKRkZHjYS5duqStW7cqLCzMWubm5qawsDCtX7/eWWdrx2QYhlEkR75FMjIyFBsbq9GjR8tsNhd1c3Ab41qDM3CdwVm41uAMrnCdxcTEaOzYsXZl0dHRiomJybbtn3/+qSpVqmjdunUKCQmxlv/rX/9SYmKiNm7ceKubm81tFw5SU1Pl6+urs2fPysfHp6ibg9sY1xqcgesMzsK1BmdwhessIyMjW0+B2WzOMQwVx3DArUwBAACAQpJbEMhJxYoV5e7urr/++suu/K+//lJAQMCtaN5NMecAAAAAKAKlS5dWcHCwVqxYYS2zWCxasWKFXU+CM9FzAAAAABSRyMhI9evXT/fdd5+aN2+uf//73zp//rz69+9fJO257cKB2WxWdHT0bTvJBcUH1xqcgesMzsK1BmfgOsvuqaee0smTJxUVFaXjx4+rWbNmio+Pl7+/f5G057abkAwAAADAMcw5AAAAACCJcAAAAADgGsIBAAAAAEmEAwAAAADXlMhw8MEHHygoKEienp5q0aKFNm3adMPtFy5cqPr168vT01ONGzfW//73Pye1FCVVbGys7r//fnl7e6tSpUrq1q2b9u/ff8N94uLiZDKZ7BZPT08ntRglUUxMTLZrpn79+jfch88zOCIoKCjbtWYymTRkyJAct+fzDHmxZs0adenSRYGBgTKZTFq6dKndesMwFBUVpcqVK6tMmTIKCwtTUlLSTevN7/c8FK4SFw4WLFigyMhIRUdHa9u2bWratKk6deqkEydO5Lj9unXr1Lt3bw0cOFDbt29Xt27d1K1bN+3evdvJLUdJkpiYqCFDhmjDhg1KSEjQ5cuX1bFjR50/f/6G+/n4+OjYsWPW5fDhw05qMUqqhg0b2l0zP/74Y67b8nkGR23evNnuOktISJAk9ejRI9d9+DzDzZw/f15NmzbVBx98kOP6t956S9OmTdOMGTO0ceNGlStXTp06ddLFixdzrTO/3/NwCxglTPPmzY0hQ4ZYX2dmZhqBgYFGbGxsjtv37NnT6Ny5s11ZixYtjEGDBt3SduL2cuLECUOSkZiYmOs2s2fPNnx9fZ3XKJR40dHRRtOmTfO8PZ9nKCwvvPCCUbt2bcNiseS4ns8z5JckY8mSJdbXFovFCAgIMN5++21rWUpKimE2m4358+fnWk9+v+eh8JWonoNLly5p69atCgsLs5a5ubkpLCxM69evz3Gf9evX220vSZ06dcp1eyAnZ8+elSRVqFDhhtulpaWpRo0aqlatmrp27ao9e/Y4o3kowZKSkhQYGKhatWrp6aef1pEjR3Ldls8zFIZLly7p888/14ABA2QymXLdjs8zFERycrKOHz9u95nl6+urFi1a5PqZ5cj3PBS+EhUOTp06pczMzGxPjPP399fx48dz3Of48eP52h64nsVi0YsvvqjWrVurUaNGuW5Xr149zZo1S19//bU+//xzWSwWtWrVSn/88YcTW4uSpEWLFoqLi1N8fLymT5+u5ORktWnTRufOnctxez7PUBiWLl2qlJQURURE5LoNn2coqKzPpfx8ZjnyPQ+Fr1RRNwAo7oYMGaLdu3ffcCy4JIWEhCgkJMT6ulWrVrr77rv10Ucfafz48be6mSiBwsPDrf9u0qSJWrRooRo1aug///mPBg4cWIQtw+3s008/VXh4uAIDA3Pdhs8zwHWVqJ6DihUryt3dXX/99Zdd+V9//aWAgIAc9wkICMjX9oCtoUOH6ttvv9WqVatUtWrVfO3r4eGhe+65RwcOHLhFrcPtpnz58qpbt26u1wyfZyiow4cP64cfftCzzz6br/34PEN+ZX0u5eczy5HveSh8JSoclC5dWsHBwVqxYoW1zGKxaMWKFXa/cNgKCQmx216SEhISct0ekK7efm3o0KFasmSJVq5cqZo1a+a7jszMTP3888+qXLnyLWghbkdpaWk6ePBgrtcMn2coqNmzZ6tSpUrq3Llzvvbj8wz5VbNmTQUEBNh9ZqWmpmrjxo25fmY58j0Pt0BRz4jOry+//NIwm81GXFycsXfvXuP55583ypcvbxw/ftwwDMPo06eP8corr1i3/+mnn4xSpUoZ77zzjrFv3z4jOjra8PDwMH7++eeiOgWUAIMHDzZ8fX2N1atXG8eOHbMu6enp1m2uv9bGjh1rfP/998bBgweNrVu3Gr169TI8PT2NPXv2FMUpoAQYOXKksXr1aiM5Odn46aefjLCwMKNixYrGiRMnDMPg8wyFKzMz06hevbrx8ssvZ1vH5xkcce7cOWP79u3G9u3bDUnG5MmTje3btxuHDx82DMMw3nzzTaN8+fLG119/bezatcvo2rWrUbNmTePChQvWOtq3b2+899571tc3+56HW6/EhQPDMIz33nvPqF69ulG6dGmjefPmxoYNG6zr2rZta/Tr189u+//85z9G3bp1jdKlSxsNGzY0li1b5uQWo6SRlOMye/Zs6zbXX2svvvii9br09/c3HnnkEWPbtm3ObzxKjKeeesqoXLmyUbp0aaNKlSrGU089ZRw4cMC6ns8zFKbvv//ekGTs378/2zo+z+CIVatW5fj/yqxryWKxGK+//rrh7+9vmM1mo0OHDtmuvxo1ahjR0dF2ZTf6nodbz2QYhlEkXRYAAAAAipUSNecAAAAAwK1DOAAAAAAgiXAAAAAA4BrCAQAAAABJhAMAAAAA1xAOAAAAAEgiHAAAAAC4hnAAAAAAQBLhAICLioiIULdu3Yrs+H369NHEiROL7PiFIS4uTuXLl8/TtvHx8WrWrJksFsutbRQAoEAIBwBuOyaT6YZLTEyMpk6dqri4uCJp386dO/W///1Pw4cPL5LjF4WHH35YHh4emjdvXlE3BQBwA6WKugEAUNiOHTtm/feCBQsUFRWl/fv3W8u8vLzk5eVVFE2TJL333nvq0aNHkbahKERERGjatGnq06dPUTcFAJALeg4A3HYCAgKsi6+vr0wmk12Zl5dXtmFFDz74oIYNG6YXX3xRd9xxh/z9/TVz5kydP39e/fv3l7e3t+rUqaPvvvvO7li7d+9WeHi4vLy85O/vrz59+ujUqVO5ti0zM1OLFi1Sly5d7Mo//PBD3XXXXfL09JS/v7+6d+9uXWexWBQbG6uaNWuqTJkyatq0qRYtWmS3/549e/Too4/Kx8dH3t7eatOmjQ4ePGjdf9y4capatarMZrOaNWum+Ph4676HDh2SyWTS4sWL1a5dO5UtW1ZNmzbV+vXr7Y4RFxen6tWrq2zZsnr88cf1999/263fuXOn2rVrJ29vb/n4+Cg4OFhbtmyxru/SpYu2bNlibRcAoPghHADANZ999pkqVqyoTZs2adiwYRo8eLB69OihVq1aadu2berYsaP69Omj9PR0SVJKSorat2+ve+65R1u2bFF8fLz++usv9ezZM9dj7Nq1S2fPntV9991nLduyZYuGDx+ucePGaf/+/YqPj1doaKh1fWxsrObMmaMZM2Zoz549GjFihJ555hklJiZKko4eParQ0FCZzWatXLlSW7du1YABA3TlyhVJ0tSpU/Xuu+/qnXfe0a5du9SpUyc99thjSkpKsmvba6+9plGjRmnHjh2qW7euevfuba1j48aNGjhwoIYOHaodO3aoXbt2euONN+z2f/rpp1W1alVt3rxZW7du1SuvvCIPDw/r+urVq8vf319r16515M8DAHAGAwBuY7NnzzZ8fX2zlffr18/o2rWr9XXbtm2NBx54wPr6ypUrRrly5Yw+ffpYy44dO2ZIMtavX28YhmGMHz/e6Nixo129v//+uyHJ2L9/f47tWbJkieHu7m5YLBZr2VdffWX4+PgYqamp2ba/ePGiUbZsWWPdunV25QMHDjR69+5tGIZhjB492qhZs6Zx6dKlHI8ZGBhoTJgwwa7s/vvvN/75z38ahmEYycnJhiTjk08+sa7fs2ePIcnYt2+fYRiG0bt3b+ORRx6xq+Opp56ye2+9vb2NuLi4HNuQ5Z577jFiYmJuuA0AoOjQcwAA1zRp0sT6b3d3d915551q3Lixtczf31+SdOLECUlXh9GsWrXKOofBy8tL9evXl6Rch85cuHBBZrNZJpPJWvbQQw+pRo0aqlWrlvr06aN58+ZZeycOHDig9PR0PfTQQ3bHmTNnjvUYO3bsUJs2bex+pc+SmpqqP//8U61bt7Yrb926tfbt25fr+VeuXNnuXPft26cWLVrYbR8SEmL3OjIyUs8++6zCwsL05ptv5vgelClTxnpuAIDihwnJAHDN9V+uTSaTXVnWF/qs23GmpaWpS5cumjRpUra6sr5cX69ixYpKT0/XpUuXVLp0aUmSt7e3tm3bptWrV2v58uWKiopSTEyMNm/erLS0NEnSsmXLVKVKFbu6zGazpKtfuAvDjc41L2JiYvSPf/xDy5Yt03fffafo6Gh9+eWXevzxx63bnD59Wn5+foXSXgBA4aPnAAAcdO+992rPnj0KCgpSnTp17JZy5crluE+zZs0kSXv37rUrL1WqlMLCwvTWW29p165dOnTokFauXKkGDRrIbDbryJEj2Y5RrVo1SVd/8V+7dq0uX76c7Xg+Pj4KDAzUTz/9ZFf+008/qUGDBnk+17vvvlsbN260K9uwYUO27erWrasRI0Zo+fLleuKJJzR79mzruosXL+rgwYO655578nxcAIBzEQ4AwEFDhgzR6dOn1bt3b23evFkHDx7U999/r/79+yszMzPHffz8/HTvvffqxx9/tJZ9++23mjZtmnbs2KHDhw9rzpw5slgsqlevnry9vTVq1CiNGDFCn332mQ4ePKht27bpvffe02effSZJGjp0qFJTU9WrVy9t2bJFSUlJmjt3rvX2rS+99JImTZqkBQsWaP/+/XrllVe0Y8cOvfDCC3k+1+HDhys+Pl7vvPOOkpKS9P7779vd8ejChQsaOnSoVq9ercOHD+unn37S5s2bdffdd1u32bBhg8xmc7bhSACA4oNwAAAOyvpFPjMzUx07dlTjxo314osvqnz58nJzy/3j9dlnn7V7GFj58uW1ePFitW/fXnfffbdmzJih+fPnq2HDhpKk8ePH6/XXX1dsbKzuvvtuPfzww1q2bJlq1qwpSbrzzju1cuVKpaWlqW3btgoODtbMmTOtw4SGDx+uyMhIjRw5Uo0bN1Z8fLy++eYb3XXXXXk+15YtW2rmzJmaOnWqmjZtquXLl2vMmDHW9e7u7vr777/Vt29f1a1bVz179lR4eLjGjh1r3Wb+/Pl6+umnVbZs2TwfFwDgXCbDMIyibgQAuJILFy6oXr16WrBggcv8in7q1CnVq1dPW7ZssYYaAEDxQ88BADhZmTJlNGfOnBs+LO12c+jQIX344YcEAwAo5ug5AAAAACCJngMAAAAA1xAOAAAAAEgiHAAAAAC4hnAAAAAAQBLhAAAAAMA1hAMAAAAAkggHAAAAAK4hHAAAAACQRDgAAAAAcM3/B1QOu7zKZuskAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heatmap saved as 'heatmap.png'.\n",
            "Traffic data exported to 'traffic_data.csv'.\n"
          ]
        }
      ]
    }
  ]
}