{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ce272bd753214c4581a663d1352de470":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fec4f3a6eec24ee28c5f14b4b1a4af51","IPY_MODEL_db18797e4b6648c888328a7a090d5ead","IPY_MODEL_ffb5dc2afc5d4ea797503b55c1f60d72"],"layout":"IPY_MODEL_ae9f0e3238f4441dac8fb7409fe7e9ca"}},"fec4f3a6eec24ee28c5f14b4b1a4af51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_098716c4cb9c417fbb4300f953b0611d","placeholder":"​","style":"IPY_MODEL_ee4e7b3f53a947c8a83e0c1f5120b88a","value":"config.json: 100%"}},"db18797e4b6648c888328a7a090d5ead":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c72237289e749eba4a7efe8094ee87e","max":662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3bdbf78c05b429baa9865049d3fa24d","value":662}},"ffb5dc2afc5d4ea797503b55c1f60d72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82789f95bfe3423cbb3a25a1ef205669","placeholder":"​","style":"IPY_MODEL_a822574493f84e0aabc213ec46afd117","value":" 662/662 [00:00&lt;00:00, 80.7kB/s]"}},"ae9f0e3238f4441dac8fb7409fe7e9ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"098716c4cb9c417fbb4300f953b0611d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee4e7b3f53a947c8a83e0c1f5120b88a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c72237289e749eba4a7efe8094ee87e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3bdbf78c05b429baa9865049d3fa24d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82789f95bfe3423cbb3a25a1ef205669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a822574493f84e0aabc213ec46afd117":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86dd7f0cba3b48699084fdf2a96247c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e05cc23f121641fdad20db5e6de206b2","IPY_MODEL_41714b086cdb4f8bb952d88cb7e2dd40","IPY_MODEL_b1d524c540874729af68aa497e7357d4"],"layout":"IPY_MODEL_cf2c8651dd694a59a1807eac671c3206"}},"e05cc23f121641fdad20db5e6de206b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c63101885884b3791c41bb36e7802fe","placeholder":"​","style":"IPY_MODEL_996bda6fdef94f57b80913e213f1b73a","value":"model.safetensors: 100%"}},"41714b086cdb4f8bb952d88cb7e2dd40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ccdfe10c1ca40cc91227821740aee15","max":3132668804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50d3bf0bf12c4237bf55e327c330dadd","value":3132668804}},"b1d524c540874729af68aa497e7357d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd796a699654bdfb1de9a3370410f2f","placeholder":"​","style":"IPY_MODEL_b1d6d2b3220b48d1ae08f1de92cb4694","value":" 3.13G/3.13G [00:25&lt;00:00, 127MB/s]"}},"cf2c8651dd694a59a1807eac671c3206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c63101885884b3791c41bb36e7802fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"996bda6fdef94f57b80913e213f1b73a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ccdfe10c1ca40cc91227821740aee15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50d3bf0bf12c4237bf55e327c330dadd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecd796a699654bdfb1de9a3370410f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d6d2b3220b48d1ae08f1de92cb4694":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21499c6654e84747836989f8a5cb3341":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75c39fe1fc95439db2c05780d17a670d","IPY_MODEL_b8303f3f282346c59ac54fedc1f0a869","IPY_MODEL_cf0af215c75042ffac849cffc787f94f"],"layout":"IPY_MODEL_e6f057248c714a9b8b3f666bc86d38ec"}},"75c39fe1fc95439db2c05780d17a670d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c57704e3fe64a339407c15eddc5d679","placeholder":"​","style":"IPY_MODEL_0457d42601f1487db124a6749420afdc","value":"generation_config.json: 100%"}},"b8303f3f282346c59ac54fedc1f0a869":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_834bf2ced6ab42e39b2d6a762d60da14","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd1f0d79716e4838920062323f13a55a","value":147}},"cf0af215c75042ffac849cffc787f94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_699878b8d2df4b03b7b2395c61b58027","placeholder":"​","style":"IPY_MODEL_b732d26a7e554eb9b18c319b34b8607b","value":" 147/147 [00:00&lt;00:00, 22.1kB/s]"}},"e6f057248c714a9b8b3f666bc86d38ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c57704e3fe64a339407c15eddc5d679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0457d42601f1487db124a6749420afdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"834bf2ced6ab42e39b2d6a762d60da14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd1f0d79716e4838920062323f13a55a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"699878b8d2df4b03b7b2395c61b58027":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b732d26a7e554eb9b18c319b34b8607b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e94162eac57544aa9f8c10c79771b05f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a9d9eb6e754379b87811834e760d5d","IPY_MODEL_2384453afe254b518e30ec3c2fc94952","IPY_MODEL_569ab38150214f4aad9ef019d4f98b52"],"layout":"IPY_MODEL_1ea083a00bb34126848df779768c8972"}},"96a9d9eb6e754379b87811834e760d5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8de0d17a9e2e416f86b2552860fadf81","placeholder":"​","style":"IPY_MODEL_20230f87c44f43dea784a6a51d3ed7c4","value":"tokenizer_config.json: 100%"}},"2384453afe254b518e30ec3c2fc94952":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_655d1fcfdda1461a80252468a57bfc50","max":2539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e274f5418954492a98154057e059592","value":2539}},"569ab38150214f4aad9ef019d4f98b52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2039bc802db749a49e50d3d6b084a2bf","placeholder":"​","style":"IPY_MODEL_f65d6fb15bc34937b64ca3ef20388571","value":" 2.54k/2.54k [00:00&lt;00:00, 399kB/s]"}},"1ea083a00bb34126848df779768c8972":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8de0d17a9e2e416f86b2552860fadf81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20230f87c44f43dea784a6a51d3ed7c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"655d1fcfdda1461a80252468a57bfc50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e274f5418954492a98154057e059592":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2039bc802db749a49e50d3d6b084a2bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f65d6fb15bc34937b64ca3ef20388571":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8286ff3f92a942068ffc6be599ae5064":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23e71c6333624c44b31a9141585ef837","IPY_MODEL_6ef7e66074ff4b959d8168c4595d36ca","IPY_MODEL_ee7ad369e2a549b8aceb2520a33222b6"],"layout":"IPY_MODEL_82cff09161984fb8b30189706a1f2b98"}},"23e71c6333624c44b31a9141585ef837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a00e4c2a89aa4419a11e1178c19eb45b","placeholder":"​","style":"IPY_MODEL_8b38eed874dc497f8c337d1d2cb7b799","value":"spiece.model: 100%"}},"6ef7e66074ff4b959d8168c4595d36ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f64c23c26a8f4e48b07c79b79667b7fc","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6107a2ee9b84ed78ba733f506d6f39b","value":791656}},"ee7ad369e2a549b8aceb2520a33222b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74b6aedf17fd4759a6b4487a5bf0b40f","placeholder":"​","style":"IPY_MODEL_4fa6657de7894c898eb322f0f19b950b","value":" 792k/792k [00:00&lt;00:00, 49.0MB/s]"}},"82cff09161984fb8b30189706a1f2b98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a00e4c2a89aa4419a11e1178c19eb45b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b38eed874dc497f8c337d1d2cb7b799":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f64c23c26a8f4e48b07c79b79667b7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6107a2ee9b84ed78ba733f506d6f39b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74b6aedf17fd4759a6b4487a5bf0b40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa6657de7894c898eb322f0f19b950b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b315f15b7a894d83bb8e31f52931276b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_395239a18aad4603baf4faac7b521161","IPY_MODEL_3b27c3ed61a44a31982dcd720ff42453","IPY_MODEL_367649468c3c40bcb88eb821a229ee93"],"layout":"IPY_MODEL_80ae2fe68489447bbb365affc976a15e"}},"395239a18aad4603baf4faac7b521161":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc96ba90e41d4dd18863eeefd840502b","placeholder":"​","style":"IPY_MODEL_66ab0df0c6db4ea18eeaaefbf6229987","value":"tokenizer.json: 100%"}},"3b27c3ed61a44a31982dcd720ff42453":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_544c49ae0d8c4835a8cae7a75e8ed6e3","max":2424064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bcc2edcbd0944e1a0b0853bad38524d","value":2424064}},"367649468c3c40bcb88eb821a229ee93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ce865046724eb286594c902afe5861","placeholder":"​","style":"IPY_MODEL_9cc9846554d046a78b80dfd8edffa91d","value":" 2.42M/2.42M [00:00&lt;00:00, 31.2MB/s]"}},"80ae2fe68489447bbb365affc976a15e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc96ba90e41d4dd18863eeefd840502b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ab0df0c6db4ea18eeaaefbf6229987":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"544c49ae0d8c4835a8cae7a75e8ed6e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcc2edcbd0944e1a0b0853bad38524d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41ce865046724eb286594c902afe5861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cc9846554d046a78b80dfd8edffa91d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07ed84259ec64b2bb95b76a8010bc6bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2099dae21b44c09a78b947f618666ef","IPY_MODEL_5789d402bea1468f94fbd1069fe6d4cc","IPY_MODEL_e3a41a8786e24aad8890e96e1a8bfe6f"],"layout":"IPY_MODEL_07c81bd2c5534e4a9b8487170993c95d"}},"e2099dae21b44c09a78b947f618666ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4734d02ab98f4e33a86d4962e3bf167a","placeholder":"​","style":"IPY_MODEL_f211367c35ad4a998bbc2199a9bc7c11","value":"special_tokens_map.json: 100%"}},"5789d402bea1468f94fbd1069fe6d4cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bddde1a956f7446692d70dae4344d79e","max":2201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb41bc8dc71d4a4890249d7933d34098","value":2201}},"e3a41a8786e24aad8890e96e1a8bfe6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcd8d3a102f844d49ccbe4a8071adf51","placeholder":"​","style":"IPY_MODEL_11ddbc6a2fe945718a84ef589eff3a40","value":" 2.20k/2.20k [00:00&lt;00:00, 334kB/s]"}},"07c81bd2c5534e4a9b8487170993c95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4734d02ab98f4e33a86d4962e3bf167a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f211367c35ad4a998bbc2199a9bc7c11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bddde1a956f7446692d70dae4344d79e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb41bc8dc71d4a4890249d7933d34098":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcd8d3a102f844d49ccbe4a8071adf51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11ddbc6a2fe945718a84ef589eff3a40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11489246,"sourceType":"datasetVersion","datasetId":7200678}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opencv-python numpy ultralytics transformers pillow matplotlib seaborn pandas torch reportlab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDQ9BsGKsEH7","outputId":"bfb4a623-d090-4eef-9386-dd1c38729b5b","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:42:35.709036Z","iopub.execute_input":"2025-04-20T18:42:35.709266Z","iopub.status.idle":"2025-04-20T18:43:51.308399Z","shell.execute_reply.started":"2025-04-20T18:42:35.709237Z","shell.execute_reply":"2025-04-20T18:43:51.307667Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting ultralytics\n  Downloading ultralytics-8.3.111-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting reportlab\n  Downloading reportlab-4.4.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading ultralytics-8.3.111-py3-none-any.whl (978 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.8/978.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading reportlab-4.4.0-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: reportlab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 reportlab-4.4.0 ultralytics-8.3.111 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom transformers import pipeline, logging\nimport re\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport torch\nimport gc\nfrom reportlab.lib.pagesizes import letter, A4\nfrom reportlab.lib import colors\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image as ReportLabImage, PageBreak, Flowable, ListFlowable, ListItem\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nfrom reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER, TA_RIGHT\nimport json\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom datetime import datetime\nimport warnings\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import MaxNLocator\nimport io\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# Disable progress bars\nimport datasets\ndatasets.disable_progress_bar()\n\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords', quiet=True)\n\n# Custom flowable for horizontal line\nclass HorizontalLine(Flowable):\n    def __init__(self, width, thickness=1, color=colors.black):\n        Flowable.__init__(self)\n        self.width = width\n        self.thickness = thickness\n        self.color = color\n\n    def draw(self):\n        self.canv.setStrokeColor(self.color)\n        self.canv.setLineWidth(self.thickness)\n        self.canv.line(0, 0, self.width, 0)\n\nclass TrafficAnalyzer:\n    def __init__(self, model_path, video_path):\n        # Clear GPU memory\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n        \n        # Verify input files\n        self.model_path = model_path\n        self.video_path = video_path\n        if not os.path.exists(model_path) or not os.path.exists(video_path):\n            print(\"Error: Model or video file not found.\")\n            exit()\n            \n        # Initialize variables\n        self.selected_frame = None\n        self.peak_frame = None\n        self.vehicle_counts = {}\n        self.track_id_to_class = {}\n        self.frame_vehicle_counts = []\n        self.max_vehicles = 0\n        self.max_frame_index = 0\n        self.emergency_alerts = []\n        self.track_positions = defaultdict(list)\n        self.average_speeds = {}\n        self.congestion_indices = []\n        \n        # Data processing variables\n        self.processed = False\n        self.context = \"\"\n        self.average_counts = {}\n        self.total_frames = 0\n        self.fps = 0\n        self.max_time_sec = 0\n        self.middle_index = 0\n        \n        # Track history for unique vehicle counting\n        self.track_history = defaultdict(lambda: {\n            'first_seen': float('inf'),\n            'last_seen': -1,\n            'vehicle_type': None,\n            'positions': []\n        })\n        \n        # Load YOLO model\n        try:\n            self.model = YOLO(model_path)\n            print(\"YOLO model loaded successfully.\")\n        except Exception as e:\n            print(f\"Error loading YOLO model: {e}\")\n            exit()\n            \n        # Initialize NLP components\n        self.load_nlp_models()\n    \n    def load_nlp_models(self):\n        try:\n            self.text_generator = pipeline(\n                \"text2text-generation\", \n                model=\"google/flan-t5-base\", \n                device=0 if torch.cuda.is_available() else -1,\n                model_kwargs={\"cache_dir\": \"./model_cache\"},\n                truncation=True\n            )\n            print(\"Flan-T5-Base loaded for text generation.\")\n        except Exception as e:\n            print(f\"Warning: Could not load Flan-T5-Base: {e}\")\n            self.text_generator = None\n            \n        try:\n            # Configure summarizer with appropriate parameters to avoid warnings\n            self.summarizer = pipeline(\n                \"summarization\", \n                model=\"facebook/bart-large-cnn\", \n                device=0 if torch.cuda.is_available() else -1,\n                model_kwargs={\"cache_dir\": \"./model_cache\"},\n                truncation=True,\n                framework=\"pt\",\n                # Disable progress bar\n                disable_tqdm=True\n            )\n            print(\"BART-Large-CNN loaded for summarization.\")\n        except Exception as e:\n            print(f\"Warning: Could not load BART-Large-CNN: {e}\")\n            self.summarizer = None\n    \n    def process_video(self):\n        # Video setup\n        cap = cv2.VideoCapture(self.video_path)\n        if not cap.isOpened():\n            print(f\"Error: Could not open video at {self.video_path}\")\n            return False\n            \n        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        if self.total_frames == 0:\n            print(\"Error: Video has no frames.\")\n            return False\n            \n        self.middle_index = self.total_frames // 2\n        self.fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_index = 0\n        \n        # Process each frame\n        print(\"Processing video...\")\n        while cap.isOpened():\n            success, frame = cap.read()\n            if not success:\n                break\n                \n            frame = cv2.resize(frame, (700, 500))\n            results = self.model.track(frame, persist=True)\n            \n            if results[0].boxes is None or len(results[0].boxes) == 0:\n                self.frame_vehicle_counts.append({})\n                frame_index += 1\n                continue\n                \n            boxes = results[0].boxes.xyxy.cpu().numpy()\n            confidences = results[0].boxes.conf.cpu().numpy()\n            classes = results[0].boxes.cls.cpu().numpy().astype(int)\n            track_ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else []\n            \n            current_frame_counts = defaultdict(int)\n            current_time = frame_index / self.fps\n            \n            for conf, cls, track_id, box in zip(confidences, classes, track_ids, boxes):\n                if conf < 0.5:\n                    continue\n                label = results[0].names[cls]\n                \n                # Only count vehicles (not traffic lights, people, etc.)\n                if label in ['car', 'truck', 'bus', 'motorcycle', 'bicycle', 'ambulance', 'police car']:\n                    # Update track history for unique vehicle counting\n                    if track_id not in self.track_history:\n                        self.track_history[track_id]['vehicle_type'] = label\n                        self.vehicle_counts[label] = self.vehicle_counts.get(label, 0) + 1\n                    \n                    self.track_history[track_id]['first_seen'] = min(self.track_history[track_id]['first_seen'], current_time)\n                    self.track_history[track_id]['last_seen'] = max(self.track_history[track_id]['last_seen'], current_time)\n                    \n                    # Update track ID to class mapping\n                    self.track_id_to_class[track_id] = label\n                    \n                    # Count for current frame\n                    current_frame_counts[label] += 1\n                    \n                    # Store position for speed calculation\n                    center_x = (box[0] + box[2]) / 2\n                    center_y = (box[1] + box[3]) / 2\n                    self.track_positions[track_id].append((frame_index, center_x, center_y))\n            \n            self.frame_vehicle_counts.append(dict(current_frame_counts))\n            \n            # Check for emergency vehicles\n            if current_frame_counts.get(\"ambulance\", 0) > 0:\n                alert = f\"Ambulance activity at {frame_index/self.fps:.2f}s: {current_frame_counts['ambulance']} ambulance(s)\"\n                self.emergency_alerts.append(alert)\n            if current_frame_counts.get(\"police car\", 0) > 0:\n                alert = f\"Police car activity at {frame_index/self.fps:.2f}s: {current_frame_counts['police car']} police car(s)\"\n                self.emergency_alerts.append(alert)\n            \n            # Calculate congestion\n            total_in_frame = sum(current_frame_counts.values())\n            congestion_index = total_in_frame / 5.0  # Normalize to reasonable scale\n            self.congestion_indices.append(congestion_index)\n            \n            # Track peak traffic\n            if total_in_frame > self.max_vehicles:\n                self.max_vehicles = total_in_frame\n                self.max_frame_index = frame_index\n                self.peak_frame = results[0].plot()\n            \n            # Save middle frame\n            if frame_index == self.middle_index:\n                self.selected_frame = results[0].plot()\n            \n            frame_index += 1\n        \n        cap.release()\n        cv2.destroyAllWindows()\n        print(f\"Video processing complete: {self.total_frames} frames processed.\")\n        \n        # Calculate average speeds\n        print(\"Calculating average speeds...\")\n        self.calculate_average_speeds()\n        \n        # Save annotated frames\n        self.save_frames()\n        \n        # Calculate statistics and create context\n        self.calculate_statistics()\n        \n        # Save data to JSON for accurate retrieval later\n        with open('traffic_data.json', 'w') as f:\n            json.dump({\n                'vehicle_counts': self.vehicle_counts,\n                'average_counts': {k: float(v) for k, v in self.average_counts.items()},\n                'average_speeds': {k: float(v) for k, v in self.average_speeds.items()},\n                'max_vehicles': self.max_vehicles,\n                'max_time_sec': self.max_time_sec,\n                'total_frames': self.total_frames,\n                'fps': float(self.fps),\n                'emergency_alerts': self.emergency_alerts,\n                'middle_index': self.middle_index,\n                'middle_frame_counts': self.frame_vehicle_counts[self.middle_index] if self.middle_index < len(self.frame_vehicle_counts) else {}\n            }, f)\n        \n        # Export to CSV\n        self.export_to_csv()\n        \n        # Generate visualizations\n        self.generate_visualizations()\n        \n        # Generate report\n        self.generate_enhanced_report()\n        \n        self.processed = True\n        return True\n        \n    def calculate_average_speeds(self):\n        for track_id, positions in self.track_positions.items():\n            label = self.track_id_to_class[track_id]\n            if len(positions) < 2:\n                continue\n            total_speed = 0\n            count = 0\n            for i in range(1, len(positions)):\n                frame_diff = positions[i][0] - positions[i-1][0]\n                if frame_diff == 0:\n                    continue\n                dx = positions[i][1] - positions[i-1][1]\n                dy = positions[i][2] - positions[i-1][2]\n                distance = np.sqrt(dx**2 + dy**2)\n                time = frame_diff / self.fps\n                speed = distance / time\n                total_speed += speed\n                count += 1\n            if count > 0:\n                if label not in self.average_speeds:\n                    self.average_speeds[label] = 0\n                    self.average_speeds[f\"{label}_count\"] = 0\n                self.average_speeds[label] += total_speed / count\n                self.average_speeds[f\"{label}_count\"] += 1\n        \n        for label in self.vehicle_counts.keys():\n            count_key = f\"{label}_count\"\n            if count_key in self.average_speeds:\n                self.average_speeds[label] = self.average_speeds[label] / self.average_speeds[count_key]\n                del self.average_speeds[count_key]\n    \n    def save_frames(self):\n        try:\n            if self.selected_frame is not None:\n                cv2.imwrite(\"middle_frame.jpg\", self.selected_frame)\n                print(\"Middle frame saved as 'middle_frame.jpg'.\")\n            if self.peak_frame is not None:\n                cv2.imwrite(\"peak_frame.jpg\", self.peak_frame)\n                print(\"Peak frame saved as 'peak_frame.jpg'.\")\n        except Exception as e:\n            print(f\"Error saving frames: {e}\")\n    \n    def calculate_statistics(self):\n        # Calculate averages and max time\n        for vtype in self.vehicle_counts.keys():\n            total = sum(frame_counts.get(vtype, 0) for frame_counts in self.frame_vehicle_counts)\n            self.average_counts[vtype] = total / len(self.frame_vehicle_counts) if self.frame_vehicle_counts else 0\n        \n        self.max_time_sec = self.max_frame_index / self.fps if self.fps > 0 else 0\n        average_congestion = np.mean(self.congestion_indices) if self.congestion_indices else 0\n        \n        # First section, middle section, last section vehicle counts\n        first_section = sum(sum(fc.values()) for fc in self.frame_vehicle_counts[:len(self.frame_vehicle_counts)//4])\n        middle_section = sum(sum(fc.values()) for fc in self.frame_vehicle_counts[len(self.frame_vehicle_counts)//4:3*len(self.frame_vehicle_counts)//4])\n        last_section = sum(sum(fc.values()) for fc in self.frame_vehicle_counts[3*len(self.frame_vehicle_counts)//4:])\n        \n        # Create context string\n        self.context = (\n            f\"- Video duration: {(self.total_frames / self.fps):.2f} seconds\\n\"\n            f\"- Unique vehicles: {', '.join([f'{v}: {c}' for v, c in self.vehicle_counts.items()])} (total: {sum(self.vehicle_counts.values())})\\n\"\n            f\"- Average vehicles per frame: {', '.join([f'{v}: {c:.2f}' for v, c in self.average_counts.items()])}\\n\"\n            f\"- Average speeds (pixels/s): {', '.join([f'{v}: {s:.2f}' for v, s in self.average_speeds.items()])}\\n\"\n            f\"- Peak traffic: {self.max_vehicles} vehicles at {self.max_time_sec:.2f} seconds\\n\"\n            f\"- Middle frame (at {(self.middle_index / self.fps):.2f} seconds): {', '.join([f'{v}: {c}' for v, c in self.frame_vehicle_counts[self.middle_index].items()]) if self.middle_index < len(self.frame_vehicle_counts) else 'no vehicles'}\\n\"\n            f\"- Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high)\\n\"\n            f\"- Emergency alerts: {'; '.join(self.emergency_alerts) if self.emergency_alerts else 'None'}\\n\"\n            f\"- Temporal trends: First 25%: {first_section} vehicles, \"\n            f\"Middle 50%: {middle_section} vehicles, \"\n            f\"Last 25%: {last_section} vehicles\"\n        )\n        print(\"\\n--- Context Generated ---\\n\")\n        print(self.context)\n    \n    def export_to_csv(self):\n        print(\"Exporting data to CSV...\")\n        times = [i / self.fps for i in range(len(self.frame_vehicle_counts))]\n        total_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in self.frame_vehicle_counts]\n        \n        csv_data = {\n            \"Frame\": list(range(len(self.frame_vehicle_counts))),\n            \"Time (s)\": times,\n            \"Total Vehicles\": total_vehicles_per_frame,\n            \"Congestion Index\": self.congestion_indices\n        }\n        \n        for vtype in self.vehicle_counts.keys():\n            csv_data[vtype] = [frame_counts.get(vtype, 0) for frame_counts in self.frame_vehicle_counts]\n        \n        df = pd.DataFrame(csv_data)\n        try:\n            df.to_csv(\"traffic_data.csv\", index=False)\n            print(\"Traffic data exported to 'traffic_data.csv'.\")\n        except Exception as e:\n            print(f\"Error exporting CSV: {e}\")\n    \n    def generate_visualizations(self):\n        print(\"Generating visualizations...\")\n        \n        # 1. Traffic Density Heatmap\n        self.generate_heatmap()\n        \n        # 2. Vehicle Count Timeline\n        self.generate_timeline_graph()\n        \n        # 3. Vehicle Type Distribution Pie Chart\n        self.generate_vehicle_distribution_chart()\n        \n        # 4. Congestion Index Over Time\n        self.generate_congestion_graph()\n        \n        # 5. Vehicle Speed Comparison\n        self.generate_speed_comparison()\n\n    def generate_heatmap(self):\n        print(\"Generating traffic density heatmap...\")\n        times = [i / self.fps for i in range(len(self.frame_vehicle_counts))]\n        total_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in self.frame_vehicle_counts]\n        \n        plt.figure(figsize=(10, 4))\n        sns.heatmap([total_vehicles_per_frame], cmap=\"YlOrRd\", xticklabels=50, cbar_kws={'label': 'Vehicle Count'})\n        plt.xlabel(\"Time (seconds)\")\n        plt.ylabel(\"Density\")\n        plt.title(\"Traffic Density Heatmap\")\n        plt.xticks(ticks=np.linspace(0, len(times)-1, 5), labels=[f\"{t:.1f}\" for t in np.linspace(0, max(times), 5)])\n        try:\n            plt.savefig(\"heatmap.png\", dpi=300, bbox_inches='tight')\n            plt.close()\n            print(\"Heatmap saved as 'heatmap.png'.\")\n        except Exception as e:\n            print(f\"Error saving heatmap: {e}\")\n    \n    def generate_timeline_graph(self):\n        print(\"Generating vehicle count timeline...\")\n        times = [i / self.fps for i in range(len(self.frame_vehicle_counts))]\n        total_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in self.frame_vehicle_counts]\n        \n        # Create a figure with a specific size\n        plt.figure(figsize=(12, 6))\n        \n        # Plot the data with a thicker line and markers\n        plt.plot(times, total_vehicles_per_frame, linewidth=2, marker='o', markersize=3, alpha=0.7)\n        \n        # Add a trend line (moving average)\n        window_size = max(1, len(total_vehicles_per_frame) // 20)  # 5% of total frames\n        if window_size > 1:\n            moving_avg = np.convolve(total_vehicles_per_frame, np.ones(window_size)/window_size, mode='valid')\n            plt.plot(times[window_size-1:], moving_avg, 'r-', linewidth=3, label=f'Trend (Moving Avg, window={window_size})')\n        \n        # Highlight peak traffic point\n        peak_idx = total_vehicles_per_frame.index(max(total_vehicles_per_frame))\n        plt.plot(times[peak_idx], total_vehicles_per_frame[peak_idx], 'ro', markersize=10, label='Peak Traffic')\n        \n        # Add labels and title\n        plt.xlabel('Time (seconds)')\n        plt.ylabel('Number of Vehicles')\n        plt.title('Vehicle Count Over Time')\n        plt.grid(True, alpha=0.3)\n        plt.legend()\n        \n        # Add annotations for key points\n        plt.annotate(f'Peak: {total_vehicles_per_frame[peak_idx]} vehicles', \n                    xy=(times[peak_idx], total_vehicles_per_frame[peak_idx]),\n                    xytext=(times[peak_idx]+0.5, total_vehicles_per_frame[peak_idx]+1),\n                    arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8))\n        \n        # Save the figure\n        try:\n            plt.savefig(\"vehicle_timeline.png\", dpi=300, bbox_inches='tight')\n            plt.close()\n            print(\"Vehicle timeline saved as 'vehicle_timeline.png'.\")\n        except Exception as e:\n            print(f\"Error saving vehicle timeline: {e}\")\n    \n    def generate_vehicle_distribution_chart(self):\n        print(\"Generating vehicle type distribution chart...\")\n        \n        # Create a pie chart of vehicle types\n        plt.figure(figsize=(10, 8))\n        \n        # Extract data\n        labels = list(self.vehicle_counts.keys())\n        sizes = list(self.vehicle_counts.values())\n        \n        # Add percentage to labels\n        total = sum(sizes)\n        labels = [f'{l} ({s}, {s/total*100:.1f}%)' for l, s in zip(labels, sizes)]\n        \n        # Create explode list to emphasize the largest segment\n        explode = [0.1 if s == max(sizes) else 0 for s in sizes]\n        \n        # Create pie chart with shadow and explosion\n        plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n                shadow=True, startangle=90, textprops={'fontsize': 12})\n        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n        plt.title('Distribution of Vehicle Types', fontsize=16)\n        \n        # Save the figure\n        try:\n            plt.savefig(\"vehicle_distribution.png\", dpi=300, bbox_inches='tight')\n            plt.close()\n            print(\"Vehicle distribution chart saved as 'vehicle_distribution.png'.\")\n        except Exception as e:\n            print(f\"Error saving vehicle distribution chart: {e}\")\n    \n    def generate_congestion_graph(self):\n        print(\"Generating congestion index graph...\")\n        times = [i / self.fps for i in range(len(self.congestion_indices))]\n        \n        plt.figure(figsize=(12, 6))\n        plt.plot(times, self.congestion_indices, linewidth=2, color='purple')\n        \n        # Add horizontal lines for congestion levels\n        plt.axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Low Congestion')\n        plt.axhline(y=1.0, color='orange', linestyle='--', alpha=0.7, label='Moderate Congestion')\n        plt.axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='High Congestion')\n        \n        # Add labels and title\n        plt.xlabel('Time (seconds)')\n        plt.ylabel('Congestion Index')\n        plt.title('Traffic Congestion Over Time')\n        plt.grid(True, alpha=0.3)\n        plt.legend()\n        \n        # Save the figure\n        try:\n            plt.savefig(\"congestion_index.png\", dpi=300, bbox_inches='tight')\n            plt.close()\n            print(\"Congestion index graph saved as 'congestion_index.png'.\")\n        except Exception as e:\n            print(f\"Error saving congestion index graph: {e}\")\n    \n    def generate_speed_comparison(self):\n        print(\"Generating vehicle speed comparison...\")\n        \n        if not self.average_speeds:\n            print(\"No speed data available.\")\n            return\n        \n        # Create bar chart of average speeds\n        plt.figure(figsize=(10, 6))\n        \n        # Extract data\n        vehicle_types = list(self.average_speeds.keys())\n        speeds = list(self.average_speeds.values())\n        \n        # Create bar chart\n        bars = plt.bar(vehicle_types, speeds, color='skyblue')\n        \n        # Add value labels on top of bars\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                    f'{height:.1f}',\n                    ha='center', va='bottom', rotation=0)\n        \n        # Add labels and title\n        plt.xlabel('Vehicle Type')\n        plt.ylabel('Average Speed (pixels/second)')\n        plt.title('Average Speed by Vehicle Type')\n        plt.grid(True, alpha=0.3, axis='y')\n        plt.xticks(rotation=45)\n        \n        # Save the figure\n        try:\n            plt.savefig(\"speed_comparison.png\", dpi=300, bbox_inches='tight')\n            plt.close()\n            print(\"Speed comparison chart saved as 'speed_comparison.png'.\")\n        except Exception as e:\n            print(f\"Error saving speed comparison chart: {e}\")\n    \n    def generate_creative_text(self, prompt):\n        if self.text_generator:\n            try:\n                result = self.text_generator(\n                    prompt, \n                    max_new_tokens=300,\n                    do_sample=True,\n                    temperature=0.7,\n                    no_repeat_ngram_size=3\n                )\n                return result[0]['generated_text'].strip()\n            except Exception as e:\n                print(f\"Error generating text: {e}\")\n                return \"Could not generate creative text.\"\n        else:\n            return \"Creative text generation not available (NLP models not loaded).\"\n    \n    def enhance_text(self, text):\n        if self.summarizer:\n            try:\n                # Calculate appropriate max_length based on input length\n                input_length = len(text.split())\n                # For summarization, output should be shorter than input\n                max_length = min(input_length - 1, 300)  # Cap at 300 tokens\n                min_length = min(max(30, input_length // 4), 150)  # Between 30 and 150\n                \n                enhanced = self.summarizer(\n                    text, \n                    max_length=max_length, \n                    min_length=min_length, \n                    do_sample=True,\n                    truncation=True\n                )[0]['summary_text']\n                return enhanced\n            except Exception as e:\n                print(f\"Error enhancing text: {e}\")\n                return text\n        else:\n            return text\n    \n    def generate_descriptive_answer(self, question):\n        \"\"\"\n        Generate a concise, factual description of the traffic without using summarization pipeline\n        \"\"\"\n        # Get frame counts at middle of video for a representative snapshot\n        middle_time = self.middle_index / self.fps if self.fps > 0 else 0\n        middle_frame_counts = self.frame_vehicle_counts[self.middle_index] if self.middle_index < len(self.frame_vehicle_counts) else {}\n        \n        # Get total unique vehicles\n        total_unique = sum(self.vehicle_counts.values())\n        \n        # Construct a simple description\n        description = f\"The video shows traffic with {total_unique} unique vehicles over {(self.total_frames / self.fps):.2f} seconds. \"\n        \n        # Add current frame info\n        if middle_frame_counts:\n            description += f\"At {middle_time:.2f} seconds, there are \"\n            description += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in middle_frame_counts.items()])\n            description += \". \"\n        \n        # Add peak traffic info\n        description += f\"Peak traffic occurs at {self.max_time_sec:.2f} seconds with {self.max_vehicles} vehicles. \"\n        \n        # Add vehicle type distribution\n        if self.vehicle_counts:\n            description += f\"Vehicle distribution: \"\n            description += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in self.vehicle_counts.items()])\n            description += \".\"\n        \n        return description\n    \n    def generate_enhanced_report(self):\n        print(\"Generating enhanced PDF report...\")\n        \n        # Generate text content for different sections\n        flow_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate a detailed traffic flow analysis starting with 'The traffic flow analysis reveals...'. \"\n            \"Include patterns of congestion, peak times, and vehicle distribution.\"\n        )\n        flow_text = self.generate_creative_text(flow_prompt)\n        \n        behavior_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate a detailed description of vehicle behavior starting with 'Vehicle behavior analysis shows...'. \"\n            \"Include speed patterns, types of vehicles, and any unusual events.\"\n        )\n        behavior_text = self.generate_creative_text(behavior_prompt)\n        \n        recommendations_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate 3-4 traffic management recommendations based on this data, \"\n            \"starting with 'Based on the analysis, we recommend...'\"\n        )\n        recommendations_text = self.generate_creative_text(recommendations_prompt)\n        \n        insights_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate 5-6 key insights from the traffic data, focusing on patterns, anomalies, and notable observations. \"\n            \"Format as bullet points.\"\n        )\n        insights_text = self.generate_creative_text(insights_prompt)\n        \n        # Create PDF\n        try:\n            # Set up the document\n            doc = SimpleDocTemplate(\"traffic_analysis_report.pdf\", pagesize=A4)\n            styles = getSampleStyleSheet()\n            \n            # Create custom styles\n            title_style = ParagraphStyle(\n                'Title',\n                parent=styles['Title'],\n                fontSize=24,\n                spaceAfter=12,\n                textColor=colors.darkblue\n            )\n            \n            heading1_style = ParagraphStyle(\n                'Heading1',\n                parent=styles['Heading1'],\n                fontSize=18,\n                spaceAfter=10,\n                textColor=colors.darkblue\n            )\n            \n            heading2_style = ParagraphStyle(\n                'Heading2',\n                parent=styles['Heading2'],\n                fontSize=14,\n                spaceAfter=8,\n                textColor=colors.darkblue\n            )\n            \n            normal_style = ParagraphStyle(\n                'Normal',\n                parent=styles['Normal'],\n                fontSize=10,\n                leading=14,\n                spaceAfter=8,\n                alignment=TA_JUSTIFY\n            )\n            \n            caption_style = ParagraphStyle(\n                'Caption',\n                parent=styles['Normal'],\n                fontSize=9,\n                leading=12,\n                alignment=TA_CENTER,\n                textColor=colors.darkslategray\n            )\n            \n            # Create document elements\n            elements = []\n            \n            # Cover page\n            elements.append(Paragraph(\"Traffic Analysis Report\", title_style))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Add current date\n            current_date = datetime.now().strftime(\"%B %d, %Y\")\n            elements.append(Paragraph(f\"Generated on: {current_date}\", normal_style))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Add video information\n            video_name = os.path.basename(self.video_path)\n            elements.append(Paragraph(f\"Analysis of: {video_name}\", normal_style))\n            elements.append(Paragraph(f\"Duration: {self.total_frames / self.fps:.2f} seconds\", normal_style))\n            elements.append(Paragraph(f\"Total unique vehicles detected: {sum(self.vehicle_counts.values())}\", normal_style))\n            \n            # Add peak frame image\n            if os.path.exists(\"peak_frame.jpg\"):\n                elements.append(Spacer(1, 0.3 * inch))\n                elements.append(Paragraph(\"Peak Traffic Frame:\", heading2_style))\n                img = ReportLabImage(\"peak_frame.jpg\", width=6*inch, height=4*inch)\n                elements.append(img)\n                elements.append(Paragraph(f\"Peak traffic occurred at {self.max_time_sec:.2f} seconds with {self.max_vehicles} vehicles\", caption_style))\n            \n            # Add page break\n            elements.append(PageBreak())\n            \n            # Table of Contents\n            elements.append(Paragraph(\"Table of Contents\", heading1_style))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            toc_data = [\n                [\"1. Executive Summary\", \"3\"],\n                [\"2. Traffic Flow Analysis\", \"4\"],\n                [\"3. Vehicle Behavior\", \"5\"],\n                [\"4. Key Insights\", \"6\"],\n                [\"5. Visualizations\", \"7\"],\n                [\"6. Statistical Summary\", \"9\"],\n                [\"7. Recommendations\", \"10\"]\n            ]\n            \n            toc_table = Table(toc_data, colWidths=[5*inch, 0.5*inch])\n            toc_table.setStyle(TableStyle([\n                ('FONT', (0, 0), (-1, -1), 'Helvetica'),\n                ('FONTSIZE', (0, 0), (-1, -1), 11),\n                ('BOTTOMPADDING', (0, 0), (-1, -1), 10),\n                ('RIGHTPADDING', (0, 0), (0, -1), 20),\n                ('ALIGN', (1, 0), (1, -1), 'RIGHT'),\n            ]))\n            elements.append(toc_table)\n            elements.append(PageBreak())\n            \n            # Executive Summary\n            elements.append(Paragraph(\"1. Executive Summary\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            summary_text = self.generate_descriptive_answer(\"Describe the traffic in the video\")\n            elements.append(Paragraph(summary_text, normal_style))\n            \n            # Add middle frame image\n            if os.path.exists(\"middle_frame.jpg\"):\n                elements.append(Spacer(1, 0.3 * inch))\n                elements.append(Paragraph(\"Representative Frame (Middle of Video):\", heading2_style))\n                img = ReportLabImage(\"middle_frame.jpg\", width=6*inch, height=4*inch)\n                elements.append(img)\n                middle_time = self.middle_index / self.fps\n                elements.append(Paragraph(f\"Frame captured at {middle_time:.2f} seconds\", caption_style))\n            \n            elements.append(PageBreak())\n            \n            # Traffic Flow Analysis\n            elements.append(Paragraph(\"2. Traffic Flow Analysis\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            elements.append(Paragraph(flow_text, normal_style))\n            \n            # Add timeline graph\n            if os.path.exists(\"vehicle_timeline.png\"):\n                elements.append(Spacer(1, 0.3 * inch))\n                elements.append(Paragraph(\"Vehicle Count Over Time:\", heading2_style))\n                img = ReportLabImage(\"vehicle_timeline.png\", width=6*inch, height=3*inch)\n                elements.append(img)\n                elements.append(Paragraph(\"This graph shows how the number of vehicles changes throughout the video duration\", caption_style))\n            \n            elements.append(PageBreak())\n            \n            # Vehicle Behavior\n            elements.append(Paragraph(\"3. Vehicle Behavior\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            elements.append(Paragraph(behavior_text, normal_style))\n            \n            # Add vehicle distribution chart\n            if os.path.exists(\"vehicle_distribution.png\"):\n                elements.append(Spacer(1, 0.3 * inch))\n                elements.append(Paragraph(\"Vehicle Type Distribution:\", heading2_style))\n                img = ReportLabImage(\"vehicle_distribution.png\", width=5*inch, height=4*inch)\n                elements.append(img)\n                elements.append(Paragraph(\"Distribution of different vehicle types detected in the video\", caption_style))\n            \n            elements.append(PageBreak())\n            \n            # Key Insights\n            elements.append(Paragraph(\"4. Key Insights\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Process insights text into bullet points\n            insights_lines = insights_text.split('\\n')\n            for line in insights_lines:\n                line = line.strip()\n                if line:\n                    if line.startswith('-'):\n                        line = line[1:].strip()\n                    elements.append(Paragraph(f\"• {line}\", normal_style))\n                    elements.append(Spacer(1, 0.1 * inch))\n            \n            # Add congestion graph\n            if os.path.exists(\"congestion_index.png\"):\n                elements.append(Spacer(1, 0.3 * inch))\n                elements.append(Paragraph(\"Traffic Congestion Over Time:\", heading2_style))\n                img = ReportLabImage(\"congestion_index.png\", width=6*inch, height=3*inch)\n                elements.append(img)\n                elements.append(Paragraph(\"This graph shows how traffic congestion varies throughout the video\", caption_style))\n            \n            elements.append(PageBreak())\n            \n            # Visualizations\n            elements.append(Paragraph(\"5. Visualizations\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Add heatmap\n            if os.path.exists(\"heatmap.png\"):\n                elements.append(Paragraph(\"Traffic Density Heatmap:\", heading2_style))\n                img = ReportLabImage(\"heatmap.png\", width=6*inch, height=2.5*inch)\n                elements.append(img)\n                elements.append(Paragraph(\"Heatmap showing the intensity of traffic over time\", caption_style))\n                elements.append(Spacer(1, 0.3 * inch))\n            \n            # Add speed comparison\n            if os.path.exists(\"speed_comparison.png\"):\n                elements.append(Paragraph(\"Average Speed by Vehicle Type:\", heading2_style))\n                img = ReportLabImage(\"speed_comparison.png\", width=6*inch, height=3*inch)\n                elements.append(img)\n                elements.append(Paragraph(\"Comparison of average speeds for different vehicle types\", caption_style))\n            \n            elements.append(PageBreak())\n            \n            # Statistical Summary\n            elements.append(Paragraph(\"6. Statistical Summary\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Vehicle counts table\n            elements.append(Paragraph(\"Vehicle Counts:\", heading2_style))\n            \n            vehicle_data = [['Vehicle Type', 'Unique Count', 'Avg. per Frame', 'Avg. Speed (px/s)']]\n            for vtype in self.vehicle_counts.keys():\n                vehicle_data.append([\n                    vtype, \n                    str(self.vehicle_counts.get(vtype, 0)), \n                    f\"{self.average_counts.get(vtype, 0):.2f}\", \n                    f\"{self.average_speeds.get(vtype, 0):.2f}\"\n                ])\n            \n            # Add total row\n            vehicle_data.append([\n                'Total', \n                str(sum(self.vehicle_counts.values())), \n                f\"{sum(self.average_counts.values()):.2f}\", \n                'N/A'\n            ])\n            \n            vehicle_table = Table(vehicle_data, colWidths=[1.5*inch, 1.2*inch, 1.5*inch, 1.5*inch])\n            vehicle_table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                ('FONTSIZE', (0, 0), (-1, 0), 11),\n                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                ('BACKGROUND', (0, -1), (-1, -1), colors.lightgrey),\n                ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black),\n                ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n            ]))\n            elements.append(vehicle_table)\n            elements.append(Spacer(1, 0.3 * inch))\n            \n            # Key metrics table\n            elements.append(Paragraph(\"Key Metrics:\", heading2_style))\n            \n            metrics_data = [\n                ['Metric', 'Value'],\n                ['Video Duration', f\"{self.total_frames / self.fps:.2f} seconds\"],\n                ['Peak Traffic Time', f\"{self.max_time_sec:.2f} seconds\"],\n                ['Peak Vehicle Count', f\"{self.max_vehicles} vehicles\"],\n                ['Average Congestion Index', f\"{np.mean(self.congestion_indices):.2f}/5.0\"],\n                ['Emergency Alerts', f\"{len(self.emergency_alerts)}\"]\n            ]\n            \n            metrics_table = Table(metrics_data, colWidths=[2.5*inch, 3.5*inch])\n            metrics_table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                ('FONTSIZE', (0, 0), (-1, 0), 11),\n                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                ('BACKGROUND', (0, 1), (0, -1), colors.lightgrey),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black),\n                ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n            ]))\n            elements.append(metrics_table)\n            \n            elements.append(PageBreak())\n            \n            # Recommendations\n            elements.append(Paragraph(\"7. Recommendations\", heading1_style))\n            elements.append(HorizontalLine(450))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            elements.append(Paragraph(recommendations_text, normal_style))\n            \n            # Build the document\n            doc.build(elements)\n            print(\"Enhanced PDF report generated as 'traffic_analysis_report.pdf'.\")\n        except Exception as e:\n            print(f\"Error generating enhanced PDF report: {e}\")\n    \n    def get_unique_vehicles_in_timerange(self, start_time, end_time):\n        \"\"\"\n        Count unique vehicles that appeared between start_time and end_time.\n        \"\"\"\n        unique_vehicles = defaultdict(int)\n        \n        for track_id, data in self.track_history.items():\n            # Check if the vehicle was present during the time range\n            if data['first_seen'] <= end_time and data['last_seen'] >= start_time:\n                vehicle_type = data['vehicle_type']\n                if vehicle_type:\n                    unique_vehicles[vehicle_type] += 1\n        \n        return dict(unique_vehicles)\n    \n    def answer_question(self, question):\n        \"\"\"\n        Rules-based factual question answering from stored data, with creative descriptions\n        where appropriate.\n        \"\"\"\n        if not self.processed:\n            return \"Please process the video first before asking questions.\"\n            \n        if not question or not isinstance(question, str):\n            return \"Please ask a valid question.\"\n            \n        # Load saved data to ensure accuracy\n        try:\n            with open('traffic_data.json', 'r') as f:\n                data = json.load(f)\n        except:\n            # Fall back to in-memory data if file not found\n            data = {\n                'vehicle_counts': self.vehicle_counts,\n                'average_counts': self.average_counts,\n                'average_speeds': self.average_speeds,\n                'max_vehicles': self.max_vehicles,\n                'max_time_sec': self.max_time_sec,\n                'total_frames': self.total_frames,\n                'fps': self.fps,\n                'emergency_alerts': self.emergency_alerts,\n                'middle_index': self.middle_index,\n                'middle_frame_counts': self.frame_vehicle_counts[self.middle_index] if self.middle_index < len(self.frame_vehicle_counts) else {}\n            }\n            \n        # Normalize question\n        question = question.lower().strip()\n        tokens = word_tokenize(question)\n        stop_words = set(stopwords.words('english'))\n        filtered_tokens = [w for w in tokens if w.isalnum() and w not in stop_words]\n        \n        # Special handling for \"describe the traffic\" to avoid summarization issues\n        if (\"describe\" in question and \"traffic\" in question) or (\"summarize\" in question and \"traffic\" in question):\n            return self.generate_descriptive_answer(question)\n        \n        # Time range handling - IMPROVED for unique vehicle counting\n        time_range_match = re.search(r\"from (\\d+(?:\\.\\d+)?)\\s*(?:to|s(?:econd)?s?\\s*to)\\s*(\\d+(?:\\.\\d+)?)\\s*s(?:econd)?s?\", question)\n        time_match = re.search(r\"at (\\d+(?:\\.\\d+)?) seconds?\", question)\n        \n        if time_range_match:\n            start_time = float(time_range_match.group(1))\n            end_time = float(time_range_match.group(2))\n            \n            if start_time >= end_time or end_time > self.total_frames / self.fps:\n                return f\"Invalid time range: {start_time}s to {end_time}s. Video duration is {self.total_frames/self.fps:.2f}s.\"\n                \n            # Get unique vehicles in the time range\n            unique_vehicles = self.get_unique_vehicles_in_timerange(start_time, end_time)\n            total_unique = sum(unique_vehicles.values())\n            \n            # Generate response\n            response = f\"From {start_time}s to {end_time}s, {total_unique} unique vehicles passed: \"\n            response += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in unique_vehicles.items()])\n            return response\n            \n        elif time_match:\n            time_sec = float(time_match.group(1))\n            \n            if time_sec > self.total_frames / self.fps:\n                return f\"Time {time_sec}s exceeds video duration ({self.total_frames/self.fps:.2f}s).\"\n                \n            frame_index = min(int(time_sec * self.fps), self.total_frames - 1)\n            frame_counts = self.frame_vehicle_counts[frame_index]\n            \n            if not frame_counts:\n                return f\"At {time_sec}s: No vehicles detected.\"\n                \n            response = f\"At {time_sec}s: \"\n            response += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n            return response\n        \n        # Total vehicle questions\n        if any(w in filtered_tokens for w in ['total', 'all']) and any(w in filtered_tokens for w in ['vehicle', 'vehicles', 'cars', 'detected']):\n            return f\"Total unique vehicles detected: {sum(data['vehicle_counts'].values())}.\"\n        \n        # Vehicle type questions\n        for vehicle_type in data['vehicle_counts'].keys():\n            if vehicle_type.lower() in question:\n                count = data['vehicle_counts'].get(vehicle_type, 0)\n                avg = data['average_counts'].get(vehicle_type, 0)\n                speed = data['average_speeds'].get(vehicle_type, 0)\n                response = f\"{count} {vehicle_type}(s) detected in total. \"\n                response += f\"Average of {avg:.2f} per frame. \"\n                \n                if speed > 0:\n                    response += f\"Average speed: {speed:.2f} pixels/second.\"\n                return response\n                \n        # Peak traffic questions\n        if any(w in filtered_tokens for w in ['peak', 'busiest', 'maximum']):\n            if any(w in filtered_tokens for w in ['time', 'when']):\n                return f\"Peak traffic occurred at {data['max_time_sec']:.2f} seconds with {data['max_vehicles']} vehicles in frame.\"\n            else:\n                return f\"Peak traffic was {data['max_vehicles']} vehicles at {data['max_time_sec']:.2f} seconds.\"\n                \n        # Middle of video questions\n        if any(w in filtered_tokens for w in ['middle', 'mid']):\n            middle_time = data['middle_index'] / data['fps']\n            middle_counts = data.get('middle_frame_counts', {})\n            \n            if not middle_counts:\n                return f\"At the middle of the video ({middle_time:.2f}s): No vehicles detected.\"\n                \n            response = f\"At the middle of the video ({middle_time:.2f}s): \"\n            response += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in middle_counts.items()])\n            return response\n            \n        # Duration question\n        if any(w in filtered_tokens for w in ['duration', 'long', 'length']):\n            return f\"Video duration: {self.total_frames/self.fps:.2f} seconds.\"\n            \n        # Types of vehicles\n        if any(w in filtered_tokens for w in ['types', 'kind', 'kinds']) and any(w in filtered_tokens for w in ['vehicle', 'vehicles']):\n            return f\"Types of vehicles detected: {', '.join(data['vehicle_counts'].keys())}.\"\n            \n        # Emergency vehicles\n        if any(w in filtered_tokens for w in ['emergency', 'ambulance', 'police']):\n            if data['emergency_alerts']:\n                return f\"Emergency vehicle activity: {'; '.join(data['emergency_alerts'])}\"\n            else:\n                return \"No emergency vehicles were detected in the video.\"\n                \n        # Fall back to context info\n        return f\"Here's what I know about the video:\\n{self.context}\"\n        \n    def run_chat_loop(self):\n        \"\"\"Run an interactive chat loop to answer questions about the video.\"\"\"\n        print(\"\\nVideo processing complete. Ask any question about the video (type 'exit' to quit).\")\n        \n        # Generate suggestions based on detected vehicles\n        suggestions = [\n            \"How many vehicles were detected in total?\",\n            \"What types of vehicles were detected?\",\n            \"What was the peak traffic time?\",\n            \"How many unique vehicles passed from 2 to 5 seconds?\",\n            \"Describe the traffic in the video.\",\n            \"What was happening in the middle of the video?\"\n        ]\n        \n        # Add vehicle-specific suggestions\n        for vehicle_type in self.vehicle_counts.keys():\n            suggestions.append(f\"How many {vehicle_type}s were detected?\")\n            \n        # Add emergency vehicle suggestions if applicable\n        if self.emergency_alerts:\n            suggestions.append(\"Were there any emergency vehicles?\")\n            \n        print(\"Suggested questions:\")\n        for i, suggestion in enumerate(suggestions[:8], 1):  # Limit to 8 suggestions\n            print(f\"{i}. {suggestion}\")\n            \n        while True:\n            try:\n                user_query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n                if user_query.lower().strip() == \"exit\":\n                    print(\"Goodbye.\")\n                    break\n                    \n                response = self.answer_question(user_query)\n                print(\"\\nAnswer:\")\n                print(response)\n                \n            except KeyboardInterrupt:\n                print(\"\\nChat interrupted. Type 'exit' to quit or continue.\")\n            except Exception as e:\n                print(f\"Error in chat loop: {e}\")\n\ndef main():\n    model_path = \"/kaggle/input/testing/yolov8x.pt\"  # Update with your model path\n    video_path = \"/kaggle/input/testing/test_30_Multi - Made with Clipchamp.mp4\"  # Update with your video path\n    \n    analyzer = TrafficAnalyzer(model_path, video_path)\n    if analyzer.process_video():\n        analyzer.run_chat_loop()\n    else:\n        print(\"Failed to process video.\")\n    \n    # Cleanup\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"id":"kCmBvaKHaOjq","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ce272bd753214c4581a663d1352de470","fec4f3a6eec24ee28c5f14b4b1a4af51","db18797e4b6648c888328a7a090d5ead","ffb5dc2afc5d4ea797503b55c1f60d72","ae9f0e3238f4441dac8fb7409fe7e9ca","098716c4cb9c417fbb4300f953b0611d","ee4e7b3f53a947c8a83e0c1f5120b88a","3c72237289e749eba4a7efe8094ee87e","b3bdbf78c05b429baa9865049d3fa24d","82789f95bfe3423cbb3a25a1ef205669","a822574493f84e0aabc213ec46afd117","86dd7f0cba3b48699084fdf2a96247c9","e05cc23f121641fdad20db5e6de206b2","41714b086cdb4f8bb952d88cb7e2dd40","b1d524c540874729af68aa497e7357d4","cf2c8651dd694a59a1807eac671c3206","1c63101885884b3791c41bb36e7802fe","996bda6fdef94f57b80913e213f1b73a","4ccdfe10c1ca40cc91227821740aee15","50d3bf0bf12c4237bf55e327c330dadd","ecd796a699654bdfb1de9a3370410f2f","b1d6d2b3220b48d1ae08f1de92cb4694","21499c6654e84747836989f8a5cb3341","75c39fe1fc95439db2c05780d17a670d","b8303f3f282346c59ac54fedc1f0a869","cf0af215c75042ffac849cffc787f94f","e6f057248c714a9b8b3f666bc86d38ec","1c57704e3fe64a339407c15eddc5d679","0457d42601f1487db124a6749420afdc","834bf2ced6ab42e39b2d6a762d60da14","cd1f0d79716e4838920062323f13a55a","699878b8d2df4b03b7b2395c61b58027","b732d26a7e554eb9b18c319b34b8607b","e94162eac57544aa9f8c10c79771b05f","96a9d9eb6e754379b87811834e760d5d","2384453afe254b518e30ec3c2fc94952","569ab38150214f4aad9ef019d4f98b52","1ea083a00bb34126848df779768c8972","8de0d17a9e2e416f86b2552860fadf81","20230f87c44f43dea784a6a51d3ed7c4","655d1fcfdda1461a80252468a57bfc50","6e274f5418954492a98154057e059592","2039bc802db749a49e50d3d6b084a2bf","f65d6fb15bc34937b64ca3ef20388571","8286ff3f92a942068ffc6be599ae5064","23e71c6333624c44b31a9141585ef837","6ef7e66074ff4b959d8168c4595d36ca","ee7ad369e2a549b8aceb2520a33222b6","82cff09161984fb8b30189706a1f2b98","a00e4c2a89aa4419a11e1178c19eb45b","8b38eed874dc497f8c337d1d2cb7b799","f64c23c26a8f4e48b07c79b79667b7fc","a6107a2ee9b84ed78ba733f506d6f39b","74b6aedf17fd4759a6b4487a5bf0b40f","4fa6657de7894c898eb322f0f19b950b","b315f15b7a894d83bb8e31f52931276b","395239a18aad4603baf4faac7b521161","3b27c3ed61a44a31982dcd720ff42453","367649468c3c40bcb88eb821a229ee93","80ae2fe68489447bbb365affc976a15e","bc96ba90e41d4dd18863eeefd840502b","66ab0df0c6db4ea18eeaaefbf6229987","544c49ae0d8c4835a8cae7a75e8ed6e3","3bcc2edcbd0944e1a0b0853bad38524d","41ce865046724eb286594c902afe5861","9cc9846554d046a78b80dfd8edffa91d","07ed84259ec64b2bb95b76a8010bc6bd","e2099dae21b44c09a78b947f618666ef","5789d402bea1468f94fbd1069fe6d4cc","e3a41a8786e24aad8890e96e1a8bfe6f","07c81bd2c5534e4a9b8487170993c95d","4734d02ab98f4e33a86d4962e3bf167a","f211367c35ad4a998bbc2199a9bc7c11","bddde1a956f7446692d70dae4344d79e","fb41bc8dc71d4a4890249d7933d34098","fcd8d3a102f844d49ccbe4a8071adf51","11ddbc6a2fe945718a84ef589eff3a40"]},"outputId":"3cd9c60e-6e33-4d8e-9286-e3beb09d54ee","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:55:28.860891Z","iopub.execute_input":"2025-04-20T18:55:28.861668Z"}},"outputs":[{"name":"stdout","text":"YOLO model loaded successfully.\nFlan-T5-Base loaded for text generation.\nBART-Large-CNN loaded for summarization.\nProcessing video...\n\n0: 480x640 9 cars, 74.7ms\nSpeed: 2.5ms preprocess, 74.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 68.7ms\nSpeed: 2.1ms preprocess, 68.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.1ms\nSpeed: 2.2ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.6ms\nSpeed: 2.7ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.1ms\nSpeed: 2.2ms preprocess, 47.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.0ms\nSpeed: 2.3ms preprocess, 44.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.7ms\nSpeed: 2.1ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.3ms\nSpeed: 2.3ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.4ms\nSpeed: 2.1ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.9ms\nSpeed: 2.1ms preprocess, 46.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.5ms\nSpeed: 2.1ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.4ms\nSpeed: 2.1ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.1ms\nSpeed: 2.1ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.8ms\nSpeed: 2.7ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.1ms\nSpeed: 2.1ms preprocess, 46.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.2ms\nSpeed: 2.3ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.9ms\nSpeed: 2.2ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.8ms\nSpeed: 2.3ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.0ms\nSpeed: 2.2ms preprocess, 46.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 48.1ms\nSpeed: 2.3ms preprocess, 48.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.8ms\nSpeed: 2.3ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.6ms\nSpeed: 2.3ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.9ms\nSpeed: 2.1ms preprocess, 46.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.5ms\nSpeed: 2.1ms preprocess, 44.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.3ms\nSpeed: 2.3ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 48.1ms\nSpeed: 2.1ms preprocess, 48.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.8ms\nSpeed: 2.7ms preprocess, 44.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.5ms\nSpeed: 2.3ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.1ms\nSpeed: 2.3ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 48.7ms\nSpeed: 2.3ms preprocess, 48.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.4ms\nSpeed: 2.1ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.8ms\nSpeed: 2.0ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 47.8ms\nSpeed: 2.2ms preprocess, 47.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.8ms\nSpeed: 2.3ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.7ms\nSpeed: 2.1ms preprocess, 44.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.7ms\nSpeed: 2.1ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 48.7ms\nSpeed: 2.3ms preprocess, 48.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.9ms\nSpeed: 2.3ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.2ms\nSpeed: 2.7ms preprocess, 44.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.1ms\nSpeed: 2.4ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.8ms\nSpeed: 2.3ms preprocess, 46.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.1ms\nSpeed: 2.8ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.4ms\nSpeed: 2.6ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.8ms\nSpeed: 2.3ms preprocess, 44.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.4ms\nSpeed: 2.1ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 47.5ms\nSpeed: 2.1ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.9ms\nSpeed: 2.1ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.5ms\nSpeed: 2.3ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.2ms\nSpeed: 2.2ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.5ms\nSpeed: 2.7ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.0ms\nSpeed: 2.1ms preprocess, 46.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.1ms\nSpeed: 2.3ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.9ms\nSpeed: 2.1ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.1ms\nSpeed: 2.7ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.8ms\nSpeed: 2.2ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.2ms\nSpeed: 2.2ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 48.3ms\nSpeed: 2.1ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.1ms\nSpeed: 2.2ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.9ms\nSpeed: 2.3ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.3ms\nSpeed: 2.1ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 47.5ms\nSpeed: 2.1ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.6ms\nSpeed: 2.1ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.5ms\nSpeed: 2.3ms preprocess, 45.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.3ms\nSpeed: 2.3ms preprocess, 45.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.4ms\nSpeed: 2.1ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.8ms\nSpeed: 2.3ms preprocess, 47.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.0ms\nSpeed: 2.3ms preprocess, 47.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.9ms\nSpeed: 2.2ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.8ms\nSpeed: 2.6ms preprocess, 45.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 48.8ms\nSpeed: 2.3ms preprocess, 48.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.2ms\nSpeed: 2.1ms preprocess, 47.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.1ms\nSpeed: 2.4ms preprocess, 46.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.4ms\nSpeed: 2.1ms preprocess, 45.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.2ms\nSpeed: 2.3ms preprocess, 45.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.1ms\nSpeed: 2.1ms preprocess, 45.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.4ms\nSpeed: 2.1ms preprocess, 44.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.8ms\nSpeed: 2.1ms preprocess, 44.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 40.8ms\nSpeed: 4.8ms preprocess, 40.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 49.2ms\nSpeed: 2.2ms preprocess, 49.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.6ms\nSpeed: 2.1ms preprocess, 45.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.0ms\nSpeed: 2.6ms preprocess, 45.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.1ms\nSpeed: 2.7ms preprocess, 45.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 40.7ms\nSpeed: 2.8ms preprocess, 40.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.3ms\nSpeed: 2.7ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.3ms\nSpeed: 2.7ms preprocess, 43.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.3ms\nSpeed: 2.3ms preprocess, 45.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 43.3ms\nSpeed: 2.2ms preprocess, 43.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 49.4ms\nSpeed: 2.1ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.9ms\nSpeed: 2.1ms preprocess, 45.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 46.1ms\nSpeed: 2.2ms preprocess, 46.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 45.6ms\nSpeed: 2.6ms preprocess, 45.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.1ms\nSpeed: 2.8ms preprocess, 45.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.2ms\nSpeed: 2.7ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 45.7ms\nSpeed: 2.8ms preprocess, 45.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 48.0ms\nSpeed: 2.4ms preprocess, 48.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.2ms\nSpeed: 2.4ms preprocess, 46.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.5ms\nSpeed: 2.7ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.7ms\nSpeed: 2.1ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.7ms\nSpeed: 2.1ms preprocess, 46.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.5ms\nSpeed: 2.1ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 48.7ms\nSpeed: 2.0ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 46.5ms\nSpeed: 2.7ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 45.7ms\nSpeed: 2.3ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 45.8ms\nSpeed: 2.1ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 48.8ms\nSpeed: 2.2ms preprocess, 48.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 47.5ms\nSpeed: 2.1ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 48.2ms\nSpeed: 2.1ms preprocess, 48.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 45.8ms\nSpeed: 2.6ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 46.1ms\nSpeed: 2.0ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 48.1ms\nSpeed: 2.1ms preprocess, 48.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 46.6ms\nSpeed: 2.6ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 46.4ms\nSpeed: 2.6ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 48.2ms\nSpeed: 2.4ms preprocess, 48.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 46.6ms\nSpeed: 2.7ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 46.3ms\nSpeed: 2.3ms preprocess, 46.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 45.9ms\nSpeed: 2.2ms preprocess, 45.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 48.5ms\nSpeed: 2.3ms preprocess, 48.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 47.8ms\nSpeed: 2.3ms preprocess, 47.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 47.8ms\nSpeed: 2.3ms preprocess, 47.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 45.8ms\nSpeed: 2.6ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 47.5ms\nSpeed: 2.2ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 49.1ms\nSpeed: 2.3ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.7ms\nSpeed: 2.3ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 47.4ms\nSpeed: 2.2ms preprocess, 47.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 45.1ms\nSpeed: 2.3ms preprocess, 45.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.0ms\nSpeed: 2.7ms preprocess, 48.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 48.8ms\nSpeed: 2.1ms preprocess, 48.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 48.4ms\nSpeed: 2.1ms preprocess, 48.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 46.7ms\nSpeed: 2.2ms preprocess, 46.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 46.4ms\nSpeed: 2.7ms preprocess, 46.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.0ms\nSpeed: 2.2ms preprocess, 48.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 49.0ms\nSpeed: 2.5ms preprocess, 49.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 47.5ms\nSpeed: 2.3ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.8ms\nSpeed: 2.3ms preprocess, 46.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.7ms\nSpeed: 2.3ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.4ms\nSpeed: 2.3ms preprocess, 46.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.3ms\nSpeed: 2.2ms preprocess, 48.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 47.6ms\nSpeed: 2.3ms preprocess, 47.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 49.7ms\nSpeed: 2.0ms preprocess, 49.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 49.2ms\nSpeed: 2.2ms preprocess, 49.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 45.5ms\nSpeed: 2.7ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.0ms\nSpeed: 2.1ms preprocess, 46.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.4ms\nSpeed: 2.1ms preprocess, 46.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.6ms\nSpeed: 2.3ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 47.9ms\nSpeed: 2.1ms preprocess, 47.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 48.3ms\nSpeed: 2.6ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 51.3ms\nSpeed: 2.3ms preprocess, 51.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.0ms\nSpeed: 2.3ms preprocess, 48.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.8ms\nSpeed: 2.6ms preprocess, 46.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 46.1ms\nSpeed: 2.7ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.6ms\nSpeed: 2.1ms preprocess, 48.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.7ms\nSpeed: 2.2ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 50.3ms\nSpeed: 2.3ms preprocess, 50.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.3ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 48.0ms\nSpeed: 2.1ms preprocess, 48.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 46.5ms\nSpeed: 2.3ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 48.3ms\nSpeed: 2.3ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 47.4ms\nSpeed: 2.4ms preprocess, 47.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 49.0ms\nSpeed: 2.6ms preprocess, 49.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 49.7ms\nSpeed: 2.3ms preprocess, 49.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 48.6ms\nSpeed: 2.3ms preprocess, 48.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.3ms\nSpeed: 2.3ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.7ms\nSpeed: 2.7ms preprocess, 46.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 51.3ms\nSpeed: 2.2ms preprocess, 51.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 50.2ms\nSpeed: 2.1ms preprocess, 50.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 46.9ms\nSpeed: 2.6ms preprocess, 46.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.8ms\nSpeed: 2.3ms preprocess, 46.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 49.6ms\nSpeed: 2.2ms preprocess, 49.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 49.0ms\nSpeed: 2.4ms preprocess, 49.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.7ms\nSpeed: 2.3ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.0ms\nSpeed: 2.1ms preprocess, 48.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.5ms\nSpeed: 2.2ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 47.7ms\nSpeed: 2.1ms preprocess, 47.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 49.3ms\nSpeed: 2.4ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 50.2ms\nSpeed: 2.3ms preprocess, 50.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 50.2ms\nSpeed: 2.2ms preprocess, 50.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 47.6ms\nSpeed: 2.1ms preprocess, 47.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 49.3ms\nSpeed: 2.3ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 49.4ms\nSpeed: 2.3ms preprocess, 49.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 50.7ms\nSpeed: 2.3ms preprocess, 50.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 50.3ms\nSpeed: 2.1ms preprocess, 50.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.9ms\nSpeed: 2.2ms preprocess, 48.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.1ms preprocess, 47.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 46.7ms\nSpeed: 2.1ms preprocess, 46.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 49.7ms\nSpeed: 2.6ms preprocess, 49.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 49.3ms\nSpeed: 2.8ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 49.3ms\nSpeed: 2.6ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 50.7ms\nSpeed: 2.2ms preprocess, 50.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 48.6ms\nSpeed: 2.3ms preprocess, 48.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 46.5ms\nSpeed: 2.1ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 49.1ms\nSpeed: 2.6ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 51.0ms\nSpeed: 2.3ms preprocess, 51.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 50.3ms\nSpeed: 2.3ms preprocess, 50.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 48.8ms\nSpeed: 2.6ms preprocess, 48.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 umbrella, 49.3ms\nSpeed: 2.3ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 48.3ms\nSpeed: 2.2ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.2ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 47.5ms\nSpeed: 2.6ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 50.3ms\nSpeed: 2.2ms preprocess, 50.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 49.6ms\nSpeed: 2.1ms preprocess, 49.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.0ms\nSpeed: 2.2ms preprocess, 48.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.2ms\nSpeed: 2.7ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 46.5ms\nSpeed: 2.2ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 48.0ms\nSpeed: 2.6ms preprocess, 48.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 14 cars, 1 umbrella, 50.8ms\nSpeed: 2.1ms preprocess, 50.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 14 cars, 1 umbrella, 50.0ms\nSpeed: 2.5ms preprocess, 50.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 49.1ms\nSpeed: 2.1ms preprocess, 49.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 49.1ms\nSpeed: 2.1ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 47.5ms\nSpeed: 2.6ms preprocess, 47.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 47.8ms\nSpeed: 2.3ms preprocess, 47.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 47.1ms\nSpeed: 2.7ms preprocess, 47.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.0ms\nSpeed: 2.3ms preprocess, 47.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 umbrella, 47.7ms\nSpeed: 2.6ms preprocess, 47.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 48.9ms\nSpeed: 2.2ms preprocess, 48.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 50.0ms\nSpeed: 2.3ms preprocess, 50.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 umbrella, 50.4ms\nSpeed: 2.3ms preprocess, 50.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.8ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 46.5ms\nSpeed: 2.5ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 47.6ms\nSpeed: 2.1ms preprocess, 47.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 46.6ms\nSpeed: 2.4ms preprocess, 46.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 45.8ms\nSpeed: 2.7ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 49.1ms\nSpeed: 2.2ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.4ms\nSpeed: 2.4ms preprocess, 48.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 49.6ms\nSpeed: 2.3ms preprocess, 49.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.0ms\nSpeed: 2.6ms preprocess, 48.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 49.2ms\nSpeed: 2.3ms preprocess, 49.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 bus, 1 umbrella, 46.6ms\nSpeed: 2.1ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 46.2ms\nSpeed: 2.7ms preprocess, 46.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.0ms\nSpeed: 2.3ms preprocess, 48.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.6ms\nSpeed: 2.1ms preprocess, 48.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 49.0ms\nSpeed: 2.3ms preprocess, 49.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 47.1ms\nSpeed: 2.3ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.7ms\nSpeed: 2.1ms preprocess, 47.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.6ms\nSpeed: 2.3ms preprocess, 46.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.9ms\nSpeed: 2.2ms preprocess, 47.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 48.5ms\nSpeed: 2.3ms preprocess, 48.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 49.9ms\nSpeed: 2.2ms preprocess, 49.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 50.0ms\nSpeed: 2.2ms preprocess, 50.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 49.9ms\nSpeed: 2.3ms preprocess, 49.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.5ms\nSpeed: 2.3ms preprocess, 47.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.7ms\nSpeed: 2.1ms preprocess, 47.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 49.8ms\nSpeed: 2.3ms preprocess, 49.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 49.2ms\nSpeed: 2.2ms preprocess, 49.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 49.2ms\nSpeed: 2.1ms preprocess, 49.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.8ms\nSpeed: 2.1ms preprocess, 47.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.0ms\nSpeed: 2.2ms preprocess, 47.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.1ms\nSpeed: 2.7ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.3ms\nSpeed: 2.4ms preprocess, 46.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.0ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.8ms\nSpeed: 2.1ms preprocess, 47.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 49.7ms\nSpeed: 2.1ms preprocess, 49.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 49.4ms\nSpeed: 2.1ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.0ms\nSpeed: 2.1ms preprocess, 47.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.2ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.6ms\nSpeed: 2.1ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.2ms\nSpeed: 2.2ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.6ms\nSpeed: 2.3ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.6ms\nSpeed: 2.7ms preprocess, 47.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.6ms\nSpeed: 2.6ms preprocess, 48.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.3ms\nSpeed: 2.4ms preprocess, 48.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.2ms\nSpeed: 2.3ms preprocess, 48.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 44.5ms\nSpeed: 2.1ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 48.4ms\nSpeed: 2.2ms preprocess, 48.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 49.1ms\nSpeed: 2.2ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 50.6ms\nSpeed: 2.3ms preprocess, 50.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 48.1ms\nSpeed: 2.1ms preprocess, 48.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.5ms\nSpeed: 2.1ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.7ms\nSpeed: 2.7ms preprocess, 46.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.1ms\nSpeed: 2.3ms preprocess, 46.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.1ms\nSpeed: 2.7ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.5ms\nSpeed: 2.1ms preprocess, 48.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.3ms\nSpeed: 2.1ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 47.4ms\nSpeed: 2.1ms preprocess, 47.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.6ms\nSpeed: 2.2ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.1ms\nSpeed: 2.1ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.0ms\nSpeed: 2.6ms preprocess, 46.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 48.2ms\nSpeed: 2.2ms preprocess, 48.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 47.3ms\nSpeed: 2.5ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 46.5ms\nSpeed: 2.8ms preprocess, 46.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 44.7ms\nSpeed: 2.8ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.9ms\nSpeed: 2.2ms preprocess, 46.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 45.3ms\nSpeed: 2.3ms preprocess, 45.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.1ms\nSpeed: 2.6ms preprocess, 47.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 47.6ms\nSpeed: 2.2ms preprocess, 47.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 48.4ms\nSpeed: 1.9ms preprocess, 48.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 45.7ms\nSpeed: 2.3ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 46.3ms\nSpeed: 2.7ms preprocess, 46.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 44.8ms\nSpeed: 2.2ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.3ms\nSpeed: 2.3ms preprocess, 48.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 45.9ms\nSpeed: 2.6ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 48.9ms\nSpeed: 2.2ms preprocess, 48.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 47.7ms\nSpeed: 2.1ms preprocess, 47.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 45.0ms\nSpeed: 2.4ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 47.6ms\nSpeed: 2.2ms preprocess, 47.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 47.6ms\nSpeed: 2.6ms preprocess, 47.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 45.9ms\nSpeed: 2.3ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 45.0ms\nSpeed: 2.3ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 47.5ms\nSpeed: 2.1ms preprocess, 47.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 45.7ms\nSpeed: 2.7ms preprocess, 45.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 47.2ms\nSpeed: 2.3ms preprocess, 47.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 44.8ms\nSpeed: 2.3ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 44.7ms\nSpeed: 2.3ms preprocess, 44.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 45.8ms\nSpeed: 2.7ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 48.1ms\nSpeed: 2.4ms preprocess, 48.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 46.8ms\nSpeed: 2.1ms preprocess, 46.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 44.2ms\nSpeed: 2.4ms preprocess, 44.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 44.6ms\nSpeed: 2.6ms preprocess, 44.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 46.8ms\nSpeed: 2.5ms preprocess, 46.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.4ms\nSpeed: 2.7ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.0ms\nSpeed: 2.3ms preprocess, 47.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.1ms\nSpeed: 2.2ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.6ms\nSpeed: 2.3ms preprocess, 47.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.6ms\nSpeed: 2.7ms preprocess, 46.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 43.7ms\nSpeed: 2.4ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 44.9ms\nSpeed: 2.3ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.3ms\nSpeed: 2.3ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.5ms\nSpeed: 2.3ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.9ms\nSpeed: 2.3ms preprocess, 44.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 48.0ms\nSpeed: 2.1ms preprocess, 48.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.2ms\nSpeed: 2.1ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 47.3ms\nSpeed: 2.0ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.8ms\nSpeed: 2.3ms preprocess, 45.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 45.5ms\nSpeed: 2.2ms preprocess, 45.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 49.0ms\nSpeed: 2.1ms preprocess, 49.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.1ms\nSpeed: 2.7ms preprocess, 44.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.3ms\nSpeed: 2.2ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 47.2ms\nSpeed: 2.2ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 48.7ms\nSpeed: 2.3ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.5ms\nSpeed: 2.6ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.6ms\nSpeed: 2.3ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 47.4ms\nSpeed: 2.1ms preprocess, 47.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 47.3ms\nSpeed: 2.1ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.7ms\nSpeed: 2.1ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 46.0ms\nSpeed: 2.4ms preprocess, 46.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.4ms\nSpeed: 2.3ms preprocess, 47.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.7ms\nSpeed: 2.3ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.5ms\nSpeed: 2.2ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.9ms\nSpeed: 2.0ms preprocess, 46.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.6ms\nSpeed: 2.6ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.2ms\nSpeed: 2.2ms preprocess, 44.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 41.5ms\nSpeed: 2.2ms preprocess, 41.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.2ms\nSpeed: 2.1ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 47.7ms\nSpeed: 2.3ms preprocess, 47.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.8ms\nSpeed: 2.2ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.7ms\nSpeed: 2.2ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.8ms\nSpeed: 2.9ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 49.1ms\nSpeed: 2.2ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.5ms\nSpeed: 2.6ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.3ms\nSpeed: 2.0ms preprocess, 45.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.1ms\nSpeed: 2.7ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 46.3ms\nSpeed: 2.5ms preprocess, 46.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.8ms\nSpeed: 2.3ms preprocess, 43.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.5ms\nSpeed: 2.3ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.8ms\nSpeed: 2.1ms preprocess, 46.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.8ms\nSpeed: 2.3ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.1ms\nSpeed: 2.3ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.5ms\nSpeed: 2.3ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.6ms\nSpeed: 2.5ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.7ms\nSpeed: 2.4ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.4ms\nSpeed: 2.2ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.9ms\nSpeed: 2.1ms preprocess, 46.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.6ms\nSpeed: 2.6ms preprocess, 44.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.6ms\nSpeed: 2.3ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.6ms\nSpeed: 2.1ms preprocess, 46.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.6ms\nSpeed: 2.8ms preprocess, 43.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.8ms\nSpeed: 2.3ms preprocess, 43.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.6ms\nSpeed: 2.3ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.4ms\nSpeed: 2.6ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.4ms\nSpeed: 2.6ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.2ms\nSpeed: 2.7ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.1ms\nSpeed: 2.1ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.2ms\nSpeed: 2.1ms preprocess, 44.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 46.1ms\nSpeed: 2.1ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 46.4ms\nSpeed: 2.1ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.4ms\nSpeed: 2.4ms preprocess, 44.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.5ms\nSpeed: 2.4ms preprocess, 43.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.8ms\nSpeed: 2.7ms preprocess, 45.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.2ms\nSpeed: 2.3ms preprocess, 46.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.0ms\nSpeed: 2.2ms preprocess, 44.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.2ms\nSpeed: 2.3ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.7ms\nSpeed: 2.8ms preprocess, 43.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 47.2ms\nSpeed: 2.3ms preprocess, 47.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 43.9ms\nSpeed: 2.2ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.9ms\nSpeed: 2.3ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.5ms\nSpeed: 2.8ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.7ms\nSpeed: 2.3ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.5ms\nSpeed: 2.1ms preprocess, 43.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.2ms\nSpeed: 2.4ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.3ms\nSpeed: 3.1ms preprocess, 42.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.6ms\nSpeed: 2.3ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 45.2ms\nSpeed: 2.1ms preprocess, 45.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 46.1ms\nSpeed: 2.1ms preprocess, 46.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.5ms\nSpeed: 2.1ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.2ms\nSpeed: 2.1ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.5ms\nSpeed: 2.6ms preprocess, 44.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.3ms\nSpeed: 2.1ms preprocess, 43.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 47.1ms\nSpeed: 2.1ms preprocess, 47.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.0ms\nSpeed: 2.6ms preprocess, 44.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 43.4ms\nSpeed: 2.2ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 48.2ms\nSpeed: 2.3ms preprocess, 48.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.2ms\nSpeed: 2.6ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.6ms\nSpeed: 2.2ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 47.2ms\nSpeed: 2.1ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 43.6ms\nSpeed: 2.1ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 44.4ms\nSpeed: 2.1ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 45.8ms\nSpeed: 2.1ms preprocess, 45.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 43.7ms\nSpeed: 2.1ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.0ms\nSpeed: 2.7ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 46.2ms\nSpeed: 2.2ms preprocess, 46.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.2ms\nSpeed: 2.1ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 47.2ms\nSpeed: 2.2ms preprocess, 47.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 44.3ms\nSpeed: 2.2ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 44.0ms\nSpeed: 2.1ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.3ms\nSpeed: 2.7ms preprocess, 44.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.3ms\nSpeed: 2.3ms preprocess, 44.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.9ms\nSpeed: 2.7ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.2ms\nSpeed: 2.4ms preprocess, 44.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 42.9ms\nSpeed: 2.7ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 48.6ms\nSpeed: 2.2ms preprocess, 48.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 43.3ms\nSpeed: 2.7ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 46.1ms\nSpeed: 2.3ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 47.3ms\nSpeed: 2.2ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.6ms\nSpeed: 2.0ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 45.1ms\nSpeed: 2.1ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.3ms\nSpeed: 2.3ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.5ms\nSpeed: 2.1ms preprocess, 45.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.1ms\nSpeed: 2.7ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.8ms\nSpeed: 2.3ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 47.3ms\nSpeed: 2.3ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 40.5ms\nSpeed: 2.9ms preprocess, 40.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.8ms\nSpeed: 3.0ms preprocess, 39.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.6ms\nSpeed: 2.7ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.7ms\nSpeed: 2.2ms preprocess, 44.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.6ms\nSpeed: 2.1ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 44.2ms\nSpeed: 2.3ms preprocess, 44.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.6ms\nSpeed: 2.8ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 47.0ms\nSpeed: 2.3ms preprocess, 47.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 1 truck, 42.5ms\nSpeed: 2.0ms preprocess, 42.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.2ms\nSpeed: 2.3ms preprocess, 45.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 43.7ms\nSpeed: 2.1ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.8ms\nSpeed: 2.2ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 46.4ms\nSpeed: 2.1ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.4ms\nSpeed: 2.2ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 46.1ms\nSpeed: 2.6ms preprocess, 46.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 48.5ms\nSpeed: 2.3ms preprocess, 48.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 44.0ms\nSpeed: 2.5ms preprocess, 44.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.3ms\nSpeed: 2.5ms preprocess, 45.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.1ms\nSpeed: 2.2ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.4ms\nSpeed: 2.3ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 46.4ms\nSpeed: 2.4ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 42.7ms\nSpeed: 2.3ms preprocess, 42.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.9ms\nSpeed: 2.1ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 46.5ms\nSpeed: 2.2ms preprocess, 46.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.9ms\nSpeed: 2.3ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 46.7ms\nSpeed: 2.1ms preprocess, 46.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 44.8ms\nSpeed: 2.1ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.8ms\nSpeed: 2.1ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 44.7ms\nSpeed: 2.1ms preprocess, 44.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 43.8ms\nSpeed: 2.7ms preprocess, 43.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.9ms\nSpeed: 2.3ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.9ms\nSpeed: 2.2ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 43.4ms\nSpeed: 2.7ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 45.9ms\nSpeed: 2.4ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 43.5ms\nSpeed: 2.7ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 45.0ms\nSpeed: 2.0ms preprocess, 45.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 44.0ms\nSpeed: 2.1ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 45.3ms\nSpeed: 2.3ms preprocess, 45.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 44.0ms\nSpeed: 2.2ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 46.6ms\nSpeed: 2.3ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.4ms\nSpeed: 2.3ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.8ms\nSpeed: 2.2ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.8ms\nSpeed: 2.1ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 45.8ms\nSpeed: 2.1ms preprocess, 45.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 40.9ms\nSpeed: 2.5ms preprocess, 40.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 43.7ms\nSpeed: 2.1ms preprocess, 43.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 45.4ms\nSpeed: 2.3ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 43.9ms\nSpeed: 2.1ms preprocess, 43.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 45.8ms\nSpeed: 3.0ms preprocess, 45.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 45.1ms\nSpeed: 2.1ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 44.3ms\nSpeed: 2.1ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 45.5ms\nSpeed: 2.1ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 44.9ms\nSpeed: 2.7ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 46.2ms\nSpeed: 2.3ms preprocess, 46.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 43.5ms\nSpeed: 2.2ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.2ms\nSpeed: 2.1ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.5ms\nSpeed: 2.1ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.8ms\nSpeed: 2.1ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.2ms\nSpeed: 2.1ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.5ms\nSpeed: 2.1ms preprocess, 44.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 41.8ms\nSpeed: 2.1ms preprocess, 41.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 1 truck, 44.7ms\nSpeed: 2.1ms preprocess, 44.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.1ms\nSpeed: 2.1ms preprocess, 44.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.5ms\nSpeed: 2.3ms preprocess, 43.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.5ms\nSpeed: 2.8ms preprocess, 42.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 40.3ms\nSpeed: 3.0ms preprocess, 40.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 40.2ms\nSpeed: 2.9ms preprocess, 40.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.7ms\nSpeed: 2.1ms preprocess, 45.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.8ms\nSpeed: 2.1ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.5ms\nSpeed: 2.6ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.8ms\nSpeed: 2.6ms preprocess, 42.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 43.6ms\nSpeed: 2.6ms preprocess, 43.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 45.1ms\nSpeed: 2.6ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.7ms\nSpeed: 2.6ms preprocess, 43.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.9ms\nSpeed: 3.0ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 43.7ms\nSpeed: 3.1ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.4ms\nSpeed: 3.1ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 45.5ms\nSpeed: 2.7ms preprocess, 45.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.6ms\nSpeed: 2.3ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 43.5ms\nSpeed: 2.7ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.4ms\nSpeed: 2.2ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.7ms\nSpeed: 2.3ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.8ms\nSpeed: 2.1ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.4ms\nSpeed: 2.6ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 46.6ms\nSpeed: 2.2ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 44.4ms\nSpeed: 2.3ms preprocess, 44.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.7ms\nSpeed: 3.0ms preprocess, 42.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.2ms\nSpeed: 2.5ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 45.6ms\nSpeed: 2.1ms preprocess, 45.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.9ms\nSpeed: 2.3ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 45.0ms\nSpeed: 2.2ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.8ms\nSpeed: 2.3ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.9ms\nSpeed: 2.3ms preprocess, 43.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.9ms\nSpeed: 2.2ms preprocess, 42.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.1ms\nSpeed: 2.1ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 45.0ms\nSpeed: 2.2ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.5ms\nSpeed: 2.6ms preprocess, 42.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.1ms\nSpeed: 2.3ms preprocess, 45.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.3ms\nSpeed: 3.9ms preprocess, 43.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.2ms\nSpeed: 2.4ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.2ms\nSpeed: 2.3ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.3ms\nSpeed: 2.4ms preprocess, 43.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.5ms\nSpeed: 2.2ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.2ms\nSpeed: 2.3ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.4ms\nSpeed: 2.2ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.2ms\nSpeed: 2.2ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.4ms\nSpeed: 2.3ms preprocess, 43.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.0ms\nSpeed: 2.4ms preprocess, 43.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.2ms\nSpeed: 2.3ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.4ms\nSpeed: 2.9ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.4ms\nSpeed: 2.0ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.2ms\nSpeed: 2.1ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.3ms\nSpeed: 2.7ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.6ms\nSpeed: 2.7ms preprocess, 43.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 43.4ms\nSpeed: 2.2ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.6ms\nSpeed: 2.7ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.6ms\nSpeed: 2.3ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.4ms\nSpeed: 2.1ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.5ms\nSpeed: 2.1ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.9ms\nSpeed: 2.1ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 42.8ms\nSpeed: 2.6ms preprocess, 42.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 44.4ms\nSpeed: 2.6ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.2ms\nSpeed: 2.1ms preprocess, 44.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.4ms\nSpeed: 2.1ms preprocess, 45.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.8ms\nSpeed: 2.0ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.3ms\nSpeed: 2.3ms preprocess, 45.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.1ms\nSpeed: 2.2ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.8ms\nSpeed: 2.7ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.8ms\nSpeed: 2.1ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.3ms\nSpeed: 2.1ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 46.1ms\nSpeed: 2.1ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 43.2ms\nSpeed: 2.7ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 44.8ms\nSpeed: 2.1ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.4ms\nSpeed: 2.7ms preprocess, 42.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 45.0ms\nSpeed: 2.4ms preprocess, 45.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 42.8ms\nSpeed: 2.6ms preprocess, 42.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.8ms\nSpeed: 2.4ms preprocess, 44.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.2ms\nSpeed: 2.2ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.4ms\nSpeed: 2.2ms preprocess, 43.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 44.6ms\nSpeed: 2.7ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.6ms\nSpeed: 2.3ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.8ms\nSpeed: 2.6ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 43.3ms\nSpeed: 2.2ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 44.0ms\nSpeed: 2.7ms preprocess, 44.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 43.4ms\nSpeed: 2.2ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 42.8ms\nSpeed: 2.6ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.7ms\nSpeed: 2.2ms preprocess, 44.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 42.7ms\nSpeed: 2.1ms preprocess, 42.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 45.1ms\nSpeed: 2.2ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.3ms\nSpeed: 2.1ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.9ms\nSpeed: 2.1ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 42.4ms\nSpeed: 2.3ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.5ms\nSpeed: 2.4ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.8ms\nSpeed: 2.1ms preprocess, 42.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.5ms\nSpeed: 2.7ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 45.2ms\nSpeed: 2.2ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.2ms\nSpeed: 2.3ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 46.5ms\nSpeed: 2.3ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.1ms\nSpeed: 2.7ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 45.4ms\nSpeed: 2.3ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.5ms\nSpeed: 2.3ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.4ms\nSpeed: 2.3ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.3ms\nSpeed: 2.6ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 41.6ms\nSpeed: 2.2ms preprocess, 41.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.2ms\nSpeed: 2.0ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.3ms\nSpeed: 2.2ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.9ms\nSpeed: 2.3ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 41.7ms\nSpeed: 2.3ms preprocess, 41.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 41.2ms\nSpeed: 2.5ms preprocess, 41.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 41.4ms\nSpeed: 2.1ms preprocess, 41.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 45.4ms\nSpeed: 2.0ms preprocess, 45.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.7ms\nSpeed: 3.2ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.5ms\nSpeed: 3.1ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.4ms\nSpeed: 2.4ms preprocess, 42.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 41.9ms\nSpeed: 3.1ms preprocess, 41.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.4ms\nSpeed: 2.2ms preprocess, 44.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.7ms\nSpeed: 2.3ms preprocess, 42.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 42.0ms\nSpeed: 2.7ms preprocess, 42.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 41.9ms\nSpeed: 2.5ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.0ms\nSpeed: 2.3ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.1ms\nSpeed: 2.2ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.2ms\nSpeed: 2.1ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.4ms\nSpeed: 2.1ms preprocess, 43.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.4ms\nSpeed: 2.1ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.5ms\nSpeed: 2.3ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.3ms\nSpeed: 2.3ms preprocess, 42.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.6ms\nSpeed: 2.5ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.0ms\nSpeed: 2.6ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 41.9ms\nSpeed: 2.3ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 46.0ms\nSpeed: 2.3ms preprocess, 46.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 41.8ms\nSpeed: 2.3ms preprocess, 41.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.4ms\nSpeed: 2.7ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.1ms\nSpeed: 2.3ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.6ms\nSpeed: 2.7ms preprocess, 42.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.7ms\nSpeed: 2.3ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 44.1ms\nSpeed: 2.1ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 42.7ms\nSpeed: 2.3ms preprocess, 42.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.2ms\nSpeed: 2.1ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.3ms\nSpeed: 2.3ms preprocess, 43.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 40.7ms\nSpeed: 2.8ms preprocess, 40.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 43.7ms\nSpeed: 2.3ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 42.3ms\nSpeed: 2.2ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.7ms\nSpeed: 2.3ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.6ms\nSpeed: 2.3ms preprocess, 42.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.1ms\nSpeed: 2.2ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.5ms\nSpeed: 2.2ms preprocess, 43.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.2ms\nSpeed: 2.1ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.6ms\nSpeed: 2.3ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.6ms\nSpeed: 2.2ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 44.7ms\nSpeed: 2.3ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 41.8ms\nSpeed: 2.3ms preprocess, 41.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 45.0ms\nSpeed: 2.3ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.3ms\nSpeed: 2.3ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 42.5ms\nSpeed: 2.1ms preprocess, 42.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.1ms\nSpeed: 2.3ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 43.5ms\nSpeed: 2.2ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 42.0ms\nSpeed: 2.2ms preprocess, 42.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.0ms\nSpeed: 2.2ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.2ms\nSpeed: 2.2ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.7ms\nSpeed: 2.3ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 43.1ms\nSpeed: 2.1ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.3ms\nSpeed: 2.7ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 44.8ms\nSpeed: 2.1ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.6ms\nSpeed: 2.2ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 42.6ms\nSpeed: 2.6ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 45.0ms\nSpeed: 2.3ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 42.5ms\nSpeed: 2.3ms preprocess, 42.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 42.6ms\nSpeed: 2.2ms preprocess, 42.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 44.1ms\nSpeed: 2.1ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 41.9ms\nSpeed: 2.3ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 42.9ms\nSpeed: 2.1ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 42.8ms\nSpeed: 2.1ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 43.4ms\nSpeed: 2.3ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 42.9ms\nSpeed: 2.4ms preprocess, 42.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 42.3ms\nSpeed: 2.3ms preprocess, 42.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 41.1ms\nSpeed: 2.3ms preprocess, 41.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 42.8ms\nSpeed: 2.4ms preprocess, 42.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 42.2ms\nSpeed: 2.3ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 1 truck, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 43.3ms\nSpeed: 2.6ms preprocess, 43.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 42.6ms\nSpeed: 2.3ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 43.2ms\nSpeed: 2.3ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 42.0ms\nSpeed: 2.0ms preprocess, 42.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 44.9ms\nSpeed: 2.2ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 42.1ms\nSpeed: 2.2ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 41.7ms\nSpeed: 2.3ms preprocess, 41.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 truck, 43.1ms\nSpeed: 2.4ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 42.5ms\nSpeed: 2.2ms preprocess, 42.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.8ms\nSpeed: 2.1ms preprocess, 43.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 42.1ms\nSpeed: 2.1ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 42.6ms\nSpeed: 2.3ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 43.5ms\nSpeed: 2.6ms preprocess, 43.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 4 trucks, 42.6ms\nSpeed: 2.7ms preprocess, 42.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 4 trucks, 41.6ms\nSpeed: 2.3ms preprocess, 41.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 44.4ms\nSpeed: 2.6ms preprocess, 44.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 42.5ms\nSpeed: 2.2ms preprocess, 42.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 43.8ms\nSpeed: 2.7ms preprocess, 43.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 43.2ms\nSpeed: 2.2ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 1 truck, 44.8ms\nSpeed: 2.1ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 42.0ms\nSpeed: 2.3ms preprocess, 42.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 44.6ms\nSpeed: 2.4ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 42.6ms\nSpeed: 2.2ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 4 trucks, 42.8ms\nSpeed: 2.2ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 3 trucks, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 4 trucks, 45.4ms\nSpeed: 2.7ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 42.9ms\nSpeed: 2.3ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 2 trucks, 44.2ms\nSpeed: 2.3ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 2 trucks, 42.9ms\nSpeed: 2.1ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 2 trucks, 43.8ms\nSpeed: 2.3ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 43.4ms\nSpeed: 2.3ms preprocess, 43.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 42.0ms\nSpeed: 2.2ms preprocess, 42.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 3 trucks, 42.9ms\nSpeed: 2.1ms preprocess, 42.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 43.1ms\nSpeed: 2.3ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 45.1ms\nSpeed: 2.2ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 42.6ms\nSpeed: 2.3ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 43.5ms\nSpeed: 2.1ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 43.7ms\nSpeed: 2.2ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 3 trucks, 42.5ms\nSpeed: 2.1ms preprocess, 42.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 43.6ms\nSpeed: 2.1ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 45.0ms\nSpeed: 2.3ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 42.4ms\nSpeed: 2.2ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 42.9ms\nSpeed: 2.0ms preprocess, 42.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 43.4ms\nSpeed: 2.3ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 41.6ms\nSpeed: 2.1ms preprocess, 41.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 43.8ms\nSpeed: 2.6ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 42.8ms\nSpeed: 2.3ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 42.5ms\nSpeed: 2.2ms preprocess, 42.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 truck, 41.6ms\nSpeed: 2.1ms preprocess, 41.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 44.5ms\nSpeed: 2.3ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 42.4ms\nSpeed: 2.0ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 43.9ms\nSpeed: 2.7ms preprocess, 43.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 1 truck, 41.7ms\nSpeed: 2.1ms preprocess, 41.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 42.7ms\nSpeed: 2.3ms preprocess, 42.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 2 trucks, 44.9ms\nSpeed: 2.1ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 1 truck, 42.9ms\nSpeed: 2.3ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 41.6ms\nSpeed: 2.6ms preprocess, 41.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 43.9ms\nSpeed: 2.3ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 42.4ms\nSpeed: 2.1ms preprocess, 42.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 5 trucks, 43.2ms\nSpeed: 2.2ms preprocess, 43.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 3 trucks, 42.2ms\nSpeed: 2.6ms preprocess, 42.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 41.7ms\nSpeed: 2.3ms preprocess, 41.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 43.8ms\nSpeed: 2.4ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 43.0ms\nSpeed: 2.0ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 41.3ms\nSpeed: 2.9ms preprocess, 41.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 42.4ms\nSpeed: 2.3ms preprocess, 42.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 42.3ms\nSpeed: 2.6ms preprocess, 42.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 42.3ms\nSpeed: 2.3ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 3 trucks, 42.2ms\nSpeed: 2.2ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 1 truck, 44.7ms\nSpeed: 2.3ms preprocess, 44.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 42.6ms\nSpeed: 2.3ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 truck, 43.1ms\nSpeed: 2.1ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 43.6ms\nSpeed: 2.6ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 1 truck, 42.2ms\nSpeed: 2.3ms preprocess, 42.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 44.7ms\nSpeed: 2.6ms preprocess, 44.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.1ms\nSpeed: 2.2ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 43.1ms\nSpeed: 2.3ms preprocess, 43.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 43.7ms\nSpeed: 2.3ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 42.7ms\nSpeed: 2.4ms preprocess, 42.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 42.7ms\nSpeed: 2.6ms preprocess, 42.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 42.4ms\nSpeed: 2.4ms preprocess, 42.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 1 truck, 41.6ms\nSpeed: 2.6ms preprocess, 41.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 44.8ms\nSpeed: 2.3ms preprocess, 44.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 41.8ms\nSpeed: 2.3ms preprocess, 41.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 41.2ms\nSpeed: 2.3ms preprocess, 41.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 43.3ms\nSpeed: 2.2ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 42.2ms\nSpeed: 2.1ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.6ms\nSpeed: 2.9ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 42.8ms\nSpeed: 2.3ms preprocess, 42.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 43.0ms\nSpeed: 2.3ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 42.2ms\nSpeed: 2.3ms preprocess, 42.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 45.3ms\nSpeed: 2.3ms preprocess, 45.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 1 umbrella, 45.3ms\nSpeed: 2.2ms preprocess, 45.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 1 umbrella, 43.0ms\nSpeed: 2.2ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 1 umbrella, 43.5ms\nSpeed: 2.3ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 41.9ms\nSpeed: 2.1ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 44.4ms\nSpeed: 2.1ms preprocess, 44.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.1ms\nSpeed: 2.2ms preprocess, 42.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 43.3ms\nSpeed: 2.6ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.5ms\nSpeed: 2.2ms preprocess, 42.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 1 umbrella, 43.1ms\nSpeed: 2.0ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 43.3ms\nSpeed: 2.3ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 42.2ms\nSpeed: 2.1ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 44.9ms\nSpeed: 2.1ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 43.7ms\nSpeed: 2.1ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.7ms\nSpeed: 2.1ms preprocess, 42.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.6ms\nSpeed: 2.2ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 41.5ms\nSpeed: 2.1ms preprocess, 41.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.4ms\nSpeed: 2.1ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 42.9ms\nSpeed: 2.1ms preprocess, 42.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 43.6ms\nSpeed: 2.6ms preprocess, 43.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 41.9ms\nSpeed: 2.2ms preprocess, 41.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 42.2ms\nSpeed: 2.2ms preprocess, 42.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 45.0ms\nSpeed: 2.3ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 42.6ms\nSpeed: 2.4ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 42.7ms\nSpeed: 2.2ms preprocess, 42.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 42.8ms\nSpeed: 2.1ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 42.4ms\nSpeed: 2.6ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 42.8ms\nSpeed: 2.6ms preprocess, 42.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 42.1ms\nSpeed: 2.3ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 43.0ms\nSpeed: 2.2ms preprocess, 43.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 41.9ms\nSpeed: 2.6ms preprocess, 41.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 42.6ms\nSpeed: 2.2ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 43.2ms\nSpeed: 2.1ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 41.4ms\nSpeed: 2.2ms preprocess, 41.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 41.2ms\nSpeed: 2.7ms preprocess, 41.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 41.9ms\nSpeed: 2.6ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 42.9ms\nSpeed: 2.3ms preprocess, 42.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 41.8ms\nSpeed: 2.1ms preprocess, 41.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 42.2ms\nSpeed: 2.1ms preprocess, 42.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 41.3ms\nSpeed: 2.6ms preprocess, 41.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 43.9ms\nSpeed: 2.5ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 42.3ms\nSpeed: 2.2ms preprocess, 42.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 43.9ms\nSpeed: 2.1ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 41.7ms\nSpeed: 2.2ms preprocess, 41.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 42.3ms\nSpeed: 2.2ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 43.8ms\nSpeed: 2.6ms preprocess, 43.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 3 trucks, 1 umbrella, 41.1ms\nSpeed: 2.3ms preprocess, 41.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 41.5ms\nSpeed: 2.3ms preprocess, 41.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 41.9ms\nSpeed: 2.7ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 1 umbrella, 44.3ms\nSpeed: 2.3ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 42.2ms\nSpeed: 2.2ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 42.8ms\nSpeed: 2.3ms preprocess, 42.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 4 trucks, 1 umbrella, 43.2ms\nSpeed: 2.8ms preprocess, 43.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 buss, 4 trucks, 1 umbrella, 42.1ms\nSpeed: 2.4ms preprocess, 42.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 2 buss, 2 trucks, 42.2ms\nSpeed: 2.1ms preprocess, 42.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 2 trucks, 44.6ms\nSpeed: 2.1ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 3 trucks, 43.0ms\nSpeed: 2.3ms preprocess, 43.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 41.5ms\nSpeed: 2.2ms preprocess, 41.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 44.6ms\nSpeed: 2.4ms preprocess, 44.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 3 trucks, 42.7ms\nSpeed: 2.3ms preprocess, 42.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 4 trucks, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 42.7ms\nSpeed: 2.7ms preprocess, 42.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 41.3ms\nSpeed: 2.7ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 5 trucks, 41.8ms\nSpeed: 2.3ms preprocess, 41.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 43.7ms\nSpeed: 2.1ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 43.1ms\nSpeed: 2.2ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 42.1ms\nSpeed: 2.3ms preprocess, 42.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 44.1ms\nSpeed: 2.3ms preprocess, 44.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 42.3ms\nSpeed: 2.6ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 43.0ms\nSpeed: 2.3ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 43.4ms\nSpeed: 2.7ms preprocess, 43.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 43.2ms\nSpeed: 2.4ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 43.0ms\nSpeed: 2.3ms preprocess, 43.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 43.4ms\nSpeed: 2.3ms preprocess, 43.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.8ms\nSpeed: 2.1ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 44.0ms\nSpeed: 2.1ms preprocess, 44.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.0ms\nSpeed: 2.1ms preprocess, 42.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.6ms\nSpeed: 2.1ms preprocess, 42.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 6 trucks, 44.1ms\nSpeed: 2.6ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 5 trucks, 41.4ms\nSpeed: 2.4ms preprocess, 41.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 42.0ms\nSpeed: 2.4ms preprocess, 42.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 44.9ms\nSpeed: 2.2ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.8ms\nSpeed: 2.6ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 43.1ms\nSpeed: 2.3ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 43.4ms\nSpeed: 2.3ms preprocess, 43.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.4ms\nSpeed: 2.1ms preprocess, 42.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 42.8ms\nSpeed: 2.2ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.4ms\nSpeed: 2.0ms preprocess, 42.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 4 trucks, 42.1ms\nSpeed: 2.6ms preprocess, 42.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 5 trucks, 44.4ms\nSpeed: 2.2ms preprocess, 44.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 42.4ms\nSpeed: 2.3ms preprocess, 42.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 43.5ms\nSpeed: 2.2ms preprocess, 43.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 5 trucks, 44.6ms\nSpeed: 2.5ms preprocess, 44.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 44.0ms\nSpeed: 2.2ms preprocess, 44.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 6 trucks, 43.7ms\nSpeed: 2.3ms preprocess, 43.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 43.9ms\nSpeed: 2.1ms preprocess, 43.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 42.6ms\nSpeed: 2.3ms preprocess, 42.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 4 trucks, 41.5ms\nSpeed: 2.3ms preprocess, 41.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 4 trucks, 43.3ms\nSpeed: 2.2ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 42.7ms\nSpeed: 2.3ms preprocess, 42.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 41.8ms\nSpeed: 2.5ms preprocess, 41.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 3 trucks, 44.6ms\nSpeed: 2.5ms preprocess, 44.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 buss, 3 trucks, 39.7ms\nSpeed: 2.7ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 5 trucks, 44.9ms\nSpeed: 2.3ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 5 trucks, 43.0ms\nSpeed: 2.1ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 5 trucks, 45.4ms\nSpeed: 2.3ms preprocess, 45.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 42.3ms\nSpeed: 2.1ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 2 trucks, 42.1ms\nSpeed: 2.6ms preprocess, 42.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 2 trucks, 45.1ms\nSpeed: 2.2ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 42.4ms\nSpeed: 2.2ms preprocess, 42.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 43.3ms\nSpeed: 2.6ms preprocess, 43.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 43.1ms\nSpeed: 2.1ms preprocess, 43.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 44.0ms\nSpeed: 2.0ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 buss, 2 trucks, 44.9ms\nSpeed: 2.2ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 3 trucks, 43.1ms\nSpeed: 2.0ms preprocess, 43.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\nVideo processing complete: 900 frames processed.\nCalculating average speeds...\nMiddle frame saved as 'middle_frame.jpg'.\nPeak frame saved as 'peak_frame.jpg'.\n\n--- Context Generated ---\n\n- Video duration: 30.00 seconds\n- Unique vehicles: car: 51, truck: 9, bus: 4 (total: 64)\n- Average vehicles per frame: car: 6.29, truck: 0.82, bus: 0.08\n- Average speeds (pixels/s): car: 87.65, truck: 111.03, bus: 42.94\n- Peak traffic: 12 vehicles at 27.73 seconds\n- Middle frame (at 15.00 seconds): car: 4, truck: 1\n- Average congestion index: 1.44 (0=low, 1=moderate, >2=high)\n- Emergency alerts: None\n- Temporal trends: First 25%: 1715 vehicles, Middle 50%: 3007 vehicles, Last 25%: 1750 vehicles\nExporting data to CSV...\nTraffic data exported to 'traffic_data.csv'.\nGenerating visualizations...\nGenerating traffic density heatmap...\nHeatmap saved as 'heatmap.png'.\nGenerating vehicle count timeline...\nVehicle timeline saved as 'vehicle_timeline.png'.\nGenerating vehicle type distribution chart...\nVehicle distribution chart saved as 'vehicle_distribution.png'.\nGenerating congestion index graph...\nCongestion index graph saved as 'congestion_index.png'.\nGenerating vehicle speed comparison...\nSpeed comparison chart saved as 'speed_comparison.png'.\nGenerating enhanced PDF report...\nEnhanced PDF report generated as 'traffic_analysis_report.pdf'.\n\nVideo processing complete. Ask any question about the video (type 'exit' to quit).\nSuggested questions:\n1. How many vehicles were detected in total?\n2. What types of vehicles were detected?\n3. What was the peak traffic time?\n4. How many unique vehicles passed from 2 to 5 seconds?\n5. Describe the traffic in the video.\n6. What was happening in the middle of the video?\n7. How many cars were detected?\n8. How many trucks were detected?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  Give me summary of the report\n"},{"name":"stdout","text":"\nAnswer:\nHere's what I know about the video:\n- Video duration: 30.00 seconds\n- Unique vehicles: car: 51, truck: 9, bus: 4 (total: 64)\n- Average vehicles per frame: car: 6.29, truck: 0.82, bus: 0.08\n- Average speeds (pixels/s): car: 87.65, truck: 111.03, bus: 42.94\n- Peak traffic: 12 vehicles at 27.73 seconds\n- Middle frame (at 15.00 seconds): car: 4, truck: 1\n- Average congestion index: 1.44 (0=low, 1=moderate, >2=high)\n- Emergency alerts: None\n- Temporal trends: First 25%: 1715 vehicles, Middle 50%: 3007 vehicles, Last 25%: 1750 vehicles\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  how many vehicles passed from 2 to 5 seconds\n"},{"name":"stdout","text":"\nAnswer:\nFrom 2.0s to 5.0s, 16 unique vehicles passed: 15 car(s), 1 truck(s)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  how many cars were detected\n"},{"name":"stdout","text":"\nAnswer:\n51 car(s) detected in total. Average of 6.29 per frame. Average speed: 87.65 pixels/second.\n","output_type":"stream"}],"execution_count":null}]}