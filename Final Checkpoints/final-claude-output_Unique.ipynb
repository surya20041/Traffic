{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ce272bd753214c4581a663d1352de470":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fec4f3a6eec24ee28c5f14b4b1a4af51","IPY_MODEL_db18797e4b6648c888328a7a090d5ead","IPY_MODEL_ffb5dc2afc5d4ea797503b55c1f60d72"],"layout":"IPY_MODEL_ae9f0e3238f4441dac8fb7409fe7e9ca"}},"fec4f3a6eec24ee28c5f14b4b1a4af51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_098716c4cb9c417fbb4300f953b0611d","placeholder":"​","style":"IPY_MODEL_ee4e7b3f53a947c8a83e0c1f5120b88a","value":"config.json: 100%"}},"db18797e4b6648c888328a7a090d5ead":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c72237289e749eba4a7efe8094ee87e","max":662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3bdbf78c05b429baa9865049d3fa24d","value":662}},"ffb5dc2afc5d4ea797503b55c1f60d72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82789f95bfe3423cbb3a25a1ef205669","placeholder":"​","style":"IPY_MODEL_a822574493f84e0aabc213ec46afd117","value":" 662/662 [00:00&lt;00:00, 80.7kB/s]"}},"ae9f0e3238f4441dac8fb7409fe7e9ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"098716c4cb9c417fbb4300f953b0611d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee4e7b3f53a947c8a83e0c1f5120b88a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c72237289e749eba4a7efe8094ee87e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3bdbf78c05b429baa9865049d3fa24d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82789f95bfe3423cbb3a25a1ef205669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a822574493f84e0aabc213ec46afd117":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86dd7f0cba3b48699084fdf2a96247c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e05cc23f121641fdad20db5e6de206b2","IPY_MODEL_41714b086cdb4f8bb952d88cb7e2dd40","IPY_MODEL_b1d524c540874729af68aa497e7357d4"],"layout":"IPY_MODEL_cf2c8651dd694a59a1807eac671c3206"}},"e05cc23f121641fdad20db5e6de206b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c63101885884b3791c41bb36e7802fe","placeholder":"​","style":"IPY_MODEL_996bda6fdef94f57b80913e213f1b73a","value":"model.safetensors: 100%"}},"41714b086cdb4f8bb952d88cb7e2dd40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ccdfe10c1ca40cc91227821740aee15","max":3132668804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50d3bf0bf12c4237bf55e327c330dadd","value":3132668804}},"b1d524c540874729af68aa497e7357d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd796a699654bdfb1de9a3370410f2f","placeholder":"​","style":"IPY_MODEL_b1d6d2b3220b48d1ae08f1de92cb4694","value":" 3.13G/3.13G [00:25&lt;00:00, 127MB/s]"}},"cf2c8651dd694a59a1807eac671c3206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c63101885884b3791c41bb36e7802fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"996bda6fdef94f57b80913e213f1b73a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ccdfe10c1ca40cc91227821740aee15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50d3bf0bf12c4237bf55e327c330dadd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecd796a699654bdfb1de9a3370410f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d6d2b3220b48d1ae08f1de92cb4694":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21499c6654e84747836989f8a5cb3341":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75c39fe1fc95439db2c05780d17a670d","IPY_MODEL_b8303f3f282346c59ac54fedc1f0a869","IPY_MODEL_cf0af215c75042ffac849cffc787f94f"],"layout":"IPY_MODEL_e6f057248c714a9b8b3f666bc86d38ec"}},"75c39fe1fc95439db2c05780d17a670d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c57704e3fe64a339407c15eddc5d679","placeholder":"​","style":"IPY_MODEL_0457d42601f1487db124a6749420afdc","value":"generation_config.json: 100%"}},"b8303f3f282346c59ac54fedc1f0a869":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_834bf2ced6ab42e39b2d6a762d60da14","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd1f0d79716e4838920062323f13a55a","value":147}},"cf0af215c75042ffac849cffc787f94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_699878b8d2df4b03b7b2395c61b58027","placeholder":"​","style":"IPY_MODEL_b732d26a7e554eb9b18c319b34b8607b","value":" 147/147 [00:00&lt;00:00, 22.1kB/s]"}},"e6f057248c714a9b8b3f666bc86d38ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c57704e3fe64a339407c15eddc5d679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0457d42601f1487db124a6749420afdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"834bf2ced6ab42e39b2d6a762d60da14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd1f0d79716e4838920062323f13a55a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"699878b8d2df4b03b7b2395c61b58027":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b732d26a7e554eb9b18c319b34b8607b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e94162eac57544aa9f8c10c79771b05f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a9d9eb6e754379b87811834e760d5d","IPY_MODEL_2384453afe254b518e30ec3c2fc94952","IPY_MODEL_569ab38150214f4aad9ef019d4f98b52"],"layout":"IPY_MODEL_1ea083a00bb34126848df779768c8972"}},"96a9d9eb6e754379b87811834e760d5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8de0d17a9e2e416f86b2552860fadf81","placeholder":"​","style":"IPY_MODEL_20230f87c44f43dea784a6a51d3ed7c4","value":"tokenizer_config.json: 100%"}},"2384453afe254b518e30ec3c2fc94952":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_655d1fcfdda1461a80252468a57bfc50","max":2539,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e274f5418954492a98154057e059592","value":2539}},"569ab38150214f4aad9ef019d4f98b52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2039bc802db749a49e50d3d6b084a2bf","placeholder":"​","style":"IPY_MODEL_f65d6fb15bc34937b64ca3ef20388571","value":" 2.54k/2.54k [00:00&lt;00:00, 399kB/s]"}},"1ea083a00bb34126848df779768c8972":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8de0d17a9e2e416f86b2552860fadf81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20230f87c44f43dea784a6a51d3ed7c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"655d1fcfdda1461a80252468a57bfc50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e274f5418954492a98154057e059592":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2039bc802db749a49e50d3d6b084a2bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f65d6fb15bc34937b64ca3ef20388571":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8286ff3f92a942068ffc6be599ae5064":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23e71c6333624c44b31a9141585ef837","IPY_MODEL_6ef7e66074ff4b959d8168c4595d36ca","IPY_MODEL_ee7ad369e2a549b8aceb2520a33222b6"],"layout":"IPY_MODEL_82cff09161984fb8b30189706a1f2b98"}},"23e71c6333624c44b31a9141585ef837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a00e4c2a89aa4419a11e1178c19eb45b","placeholder":"​","style":"IPY_MODEL_8b38eed874dc497f8c337d1d2cb7b799","value":"spiece.model: 100%"}},"6ef7e66074ff4b959d8168c4595d36ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f64c23c26a8f4e48b07c79b79667b7fc","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6107a2ee9b84ed78ba733f506d6f39b","value":791656}},"ee7ad369e2a549b8aceb2520a33222b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74b6aedf17fd4759a6b4487a5bf0b40f","placeholder":"​","style":"IPY_MODEL_4fa6657de7894c898eb322f0f19b950b","value":" 792k/792k [00:00&lt;00:00, 49.0MB/s]"}},"82cff09161984fb8b30189706a1f2b98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a00e4c2a89aa4419a11e1178c19eb45b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b38eed874dc497f8c337d1d2cb7b799":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f64c23c26a8f4e48b07c79b79667b7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6107a2ee9b84ed78ba733f506d6f39b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74b6aedf17fd4759a6b4487a5bf0b40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa6657de7894c898eb322f0f19b950b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b315f15b7a894d83bb8e31f52931276b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_395239a18aad4603baf4faac7b521161","IPY_MODEL_3b27c3ed61a44a31982dcd720ff42453","IPY_MODEL_367649468c3c40bcb88eb821a229ee93"],"layout":"IPY_MODEL_80ae2fe68489447bbb365affc976a15e"}},"395239a18aad4603baf4faac7b521161":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc96ba90e41d4dd18863eeefd840502b","placeholder":"​","style":"IPY_MODEL_66ab0df0c6db4ea18eeaaefbf6229987","value":"tokenizer.json: 100%"}},"3b27c3ed61a44a31982dcd720ff42453":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_544c49ae0d8c4835a8cae7a75e8ed6e3","max":2424064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bcc2edcbd0944e1a0b0853bad38524d","value":2424064}},"367649468c3c40bcb88eb821a229ee93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ce865046724eb286594c902afe5861","placeholder":"​","style":"IPY_MODEL_9cc9846554d046a78b80dfd8edffa91d","value":" 2.42M/2.42M [00:00&lt;00:00, 31.2MB/s]"}},"80ae2fe68489447bbb365affc976a15e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc96ba90e41d4dd18863eeefd840502b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ab0df0c6db4ea18eeaaefbf6229987":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"544c49ae0d8c4835a8cae7a75e8ed6e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcc2edcbd0944e1a0b0853bad38524d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41ce865046724eb286594c902afe5861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cc9846554d046a78b80dfd8edffa91d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07ed84259ec64b2bb95b76a8010bc6bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2099dae21b44c09a78b947f618666ef","IPY_MODEL_5789d402bea1468f94fbd1069fe6d4cc","IPY_MODEL_e3a41a8786e24aad8890e96e1a8bfe6f"],"layout":"IPY_MODEL_07c81bd2c5534e4a9b8487170993c95d"}},"e2099dae21b44c09a78b947f618666ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4734d02ab98f4e33a86d4962e3bf167a","placeholder":"​","style":"IPY_MODEL_f211367c35ad4a998bbc2199a9bc7c11","value":"special_tokens_map.json: 100%"}},"5789d402bea1468f94fbd1069fe6d4cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bddde1a956f7446692d70dae4344d79e","max":2201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb41bc8dc71d4a4890249d7933d34098","value":2201}},"e3a41a8786e24aad8890e96e1a8bfe6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcd8d3a102f844d49ccbe4a8071adf51","placeholder":"​","style":"IPY_MODEL_11ddbc6a2fe945718a84ef589eff3a40","value":" 2.20k/2.20k [00:00&lt;00:00, 334kB/s]"}},"07c81bd2c5534e4a9b8487170993c95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4734d02ab98f4e33a86d4962e3bf167a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f211367c35ad4a998bbc2199a9bc7c11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bddde1a956f7446692d70dae4344d79e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb41bc8dc71d4a4890249d7933d34098":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcd8d3a102f844d49ccbe4a8071adf51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11ddbc6a2fe945718a84ef589eff3a40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11489246,"sourceType":"datasetVersion","datasetId":7200678}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opencv-python numpy ultralytics transformers pillow matplotlib seaborn pandas torch reportlab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDQ9BsGKsEH7","outputId":"bfb4a623-d090-4eef-9386-dd1c38729b5b","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:25:31.000010Z","iopub.execute_input":"2025-04-20T18:25:31.000269Z","iopub.status.idle":"2025-04-20T18:26:40.294821Z","shell.execute_reply.started":"2025-04-20T18:25:31.000248Z","shell.execute_reply":"2025-04-20T18:26:40.294137Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting ultralytics\n  Downloading ultralytics-8.3.111-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nCollecting reportlab\n  Downloading reportlab-4.4.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading ultralytics-8.3.111-py3-none-any.whl (978 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.8/978.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading reportlab-4.4.0-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: reportlab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 reportlab-4.4.0 ultralytics-8.3.111 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom transformers import pipeline\nimport re\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport torch\nimport gc\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib import colors\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image as ReportLabImage\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nimport json\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom datetime import datetime\n\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords')\n\nclass TrafficAnalyzer:\n    def __init__(self, model_path, video_path):\n        # Clear GPU memory\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n        \n        # Verify input files\n        self.model_path = model_path\n        self.video_path = video_path\n        if not os.path.exists(model_path) or not os.path.exists(video_path):\n            print(\"Error: Model or video file not found.\")\n            exit()\n            \n        # Initialize variables\n        self.selected_frame = None\n        self.peak_frame = None\n        self.vehicle_counts = {}\n        self.track_id_to_class = {}\n        self.frame_vehicle_counts = []\n        self.max_vehicles = 0\n        self.max_frame_index = 0\n        self.emergency_alerts = []\n        self.track_positions = defaultdict(list)\n        self.average_speeds = {}\n        self.congestion_indices = []\n        \n        # Data processing variables\n        self.processed = False\n        self.context = \"\"\n        self.average_counts = {}\n        self.total_frames = 0\n        self.fps = 0\n        self.max_time_sec = 0\n        self.middle_index = 0\n        \n        # Track history for unique vehicle counting\n        self.track_history = defaultdict(lambda: {\n            'first_seen': float('inf'),\n            'last_seen': -1,\n            'vehicle_type': None,\n            'positions': []\n        })\n        \n        # Load YOLO model\n        try:\n            self.model = YOLO(model_path)\n            print(\"YOLO model loaded successfully.\")\n        except Exception as e:\n            print(f\"Error loading YOLO model: {e}\")\n            exit()\n            \n        # Initialize NLP components\n        self.load_nlp_models()\n    \n    def load_nlp_models(self):\n        try:\n            self.text_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", \n                                          device=0 if torch.cuda.is_available() else -1)\n            print(\"Flan-T5-Base loaded for text generation.\")\n        except Exception as e:\n            print(f\"Warning: Could not load Flan-T5-Base: {e}\")\n            self.text_generator = None\n            \n        try:\n            self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", \n                                      device=0 if torch.cuda.is_available() else -1)\n            print(\"BART-Large-CNN loaded for summarization.\")\n        except Exception as e:\n            print(f\"Warning: Could not load BART-Large-CNN: {e}\")\n            self.summarizer = None\n    \n    def process_video(self):\n        # Video setup\n        cap = cv2.VideoCapture(self.video_path)\n        if not cap.isOpened():\n            print(f\"Error: Could not open video at {self.video_path}\")\n            return False\n            \n        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        if self.total_frames == 0:\n            print(\"Error: Video has no frames.\")\n            return False\n            \n        self.middle_index = self.total_frames // 2\n        self.fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_index = 0\n        \n        # Process each frame\n        print(\"Processing video...\")\n        while cap.isOpened():\n            success, frame = cap.read()\n            if not success:\n                break\n                \n            frame = cv2.resize(frame, (700, 500))\n            results = self.model.track(frame, persist=True)\n            \n            if results[0].boxes is None or len(results[0].boxes) == 0:\n                self.frame_vehicle_counts.append({})\n                frame_index += 1\n                continue\n                \n            boxes = results[0].boxes.xyxy.cpu().numpy()\n            confidences = results[0].boxes.conf.cpu().numpy()\n            classes = results[0].boxes.cls.cpu().numpy().astype(int)\n            track_ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else []\n            \n            current_frame_counts = defaultdict(int)\n            current_time = frame_index / self.fps\n            \n            for conf, cls, track_id, box in zip(confidences, classes, track_ids, boxes):\n                if conf < 0.5:\n                    continue\n                label = results[0].names[cls]\n                \n                # Only count vehicles (not traffic lights, people, etc.)\n                if label in ['car', 'truck', 'bus', 'motorcycle', 'bicycle', 'ambulance', 'police car']:\n                    # Update track history for unique vehicle counting\n                    if track_id not in self.track_history:\n                        self.track_history[track_id]['vehicle_type'] = label\n                        self.vehicle_counts[label] = self.vehicle_counts.get(label, 0) + 1\n                    \n                    self.track_history[track_id]['first_seen'] = min(self.track_history[track_id]['first_seen'], current_time)\n                    self.track_history[track_id]['last_seen'] = max(self.track_history[track_id]['last_seen'], current_time)\n                    \n                    # Update track ID to class mapping\n                    self.track_id_to_class[track_id] = label\n                    \n                    # Count for current frame\n                    current_frame_counts[label] += 1\n                    \n                    # Store position for speed calculation\n                    center_x = (box[0] + box[2]) / 2\n                    center_y = (box[1] + box[3]) / 2\n                    self.track_positions[track_id].append((frame_index, center_x, center_y))\n            \n            self.frame_vehicle_counts.append(dict(current_frame_counts))\n            \n            # Check for emergency vehicles\n            if current_frame_counts.get(\"ambulance\", 0) > 0:\n                alert = f\"Ambulance activity at {frame_index/self.fps:.2f}s: {current_frame_counts['ambulance']} ambulance(s)\"\n                self.emergency_alerts.append(alert)\n            if current_frame_counts.get(\"police car\", 0) > 0:\n                alert = f\"Police car activity at {frame_index/self.fps:.2f}s: {current_frame_counts['police car']} police car(s)\"\n                self.emergency_alerts.append(alert)\n            \n            # Calculate congestion\n            total_in_frame = sum(current_frame_counts.values())\n            congestion_index = total_in_frame / 5.0  # Normalize to reasonable scale\n            self.congestion_indices.append(congestion_index)\n            \n            # Track peak traffic\n            if total_in_frame > self.max_vehicles:\n                self.max_vehicles = total_in_frame\n                self.max_frame_index = frame_index\n                self.peak_frame = results[0].plot()\n            \n            # Save middle frame\n            if frame_index == self.middle_index:\n                self.selected_frame = results[0].plot()\n            \n            frame_index += 1\n        \n        cap.release()\n        cv2.destroyAllWindows()\n        print(f\"Video processing complete: {self.total_frames} frames processed.\")\n        \n        # Calculate average speeds\n        print(\"Calculating average speeds...\")\n        self.calculate_average_speeds()\n        \n        # Save annotated frames\n        self.save_frames()\n        \n        # Calculate statistics and create context\n        self.calculate_statistics()\n        \n        # Save data to JSON for accurate retrieval later\n        with open('traffic_data.json', 'w') as f:\n            json.dump({\n                'vehicle_counts': self.vehicle_counts,\n                'average_counts': {k: float(v) for k, v in self.average_counts.items()},\n                'average_speeds': {k: float(v) for k, v in self.average_speeds.items()},\n                'max_vehicles': self.max_vehicles,\n                'max_time_sec': self.max_time_sec,\n                'total_frames': self.total_frames,\n                'fps': float(self.fps),\n                'emergency_alerts': self.emergency_alerts,\n                'middle_index': self.middle_index,\n                'middle_frame_counts': self.frame_vehicle_counts[self.middle_index] if self.middle_index < len(self.frame_vehicle_counts) else {}\n            }, f)\n        \n        # Export to CSV\n        self.export_to_csv()\n        \n        # Generate heatmap\n        self.generate_heatmap()\n        \n        # Generate report\n        self.generate_report()\n        \n        self.processed = True\n        return True\n        \n    def calculate_average_speeds(self):\n        for track_id, positions in self.track_positions.items():\n            label = self.track_id_to_class[track_id]\n            if len(positions) < 2:\n                continue\n            total_speed = 0\n            count = 0\n            for i in range(1, len(positions)):\n                frame_diff = positions[i][0] - positions[i-1][0]\n                if frame_diff == 0:\n                    continue\n                dx = positions[i][1] - positions[i-1][1]\n                dy = positions[i][2] - positions[i-1][2]\n                distance = np.sqrt(dx**2 + dy**2)\n                time = frame_diff / self.fps\n                speed = distance / time\n                total_speed += speed\n                count += 1\n            if count > 0:\n                if label not in self.average_speeds:\n                    self.average_speeds[label] = 0\n                    self.average_speeds[f\"{label}_count\"] = 0\n                self.average_speeds[label] += total_speed / count\n                self.average_speeds[f\"{label}_count\"] += 1\n        \n        for label in self.vehicle_counts.keys():\n            count_key = f\"{label}_count\"\n            if count_key in self.average_speeds:\n                self.average_speeds[label] = self.average_speeds[label] / self.average_speeds[count_key]\n                del self.average_speeds[count_key]\n    \n    def save_frames(self):\n        try:\n            if self.selected_frame is not None:\n                cv2.imwrite(\"middle_frame.jpg\", self.selected_frame)\n                print(\"Middle frame saved as 'middle_frame.jpg'.\")\n            if self.peak_frame is not None:\n                cv2.imwrite(\"peak_frame.jpg\", self.peak_frame)\n                print(\"Peak frame saved as 'peak_frame.jpg'.\")\n        except Exception as e:\n            print(f\"Error saving frames: {e}\")\n    \n    def calculate_statistics(self):\n        # Calculate averages and max time\n        for vtype in self.vehicle_counts.keys():\n            total = sum(frame_counts.get(vtype, 0) for frame_counts in self.frame_vehicle_counts)\n            self.average_counts[vtype] = total / len(self.frame_vehicle_counts) if self.frame_vehicle_counts else 0\n        \n        self.max_time_sec = self.max_frame_index / self.fps if self.fps > 0 else 0\n        average_congestion = np.mean(self.congestion_indices) if self.congestion_indices else 0\n        \n        # First section, middle section, last section vehicle counts\n        first_section = sum(sum(fc.values()) for fc in self.frame_vehicle_counts[:len(self.frame_vehicle_counts)//4])\n        middle_section = sum(sum(fc.values()) for fc in self.frame_vehicle_counts[len(self.frame_vehicle_counts)//4:3*len(self.frame_vehicle_counts)//4])\n        last_section = sum(sum(fc.values()) for fc in self.frame_vehicle_counts[3*len(self.frame_vehicle_counts)//4:])\n        \n        # Create context string\n        self.context = (\n            f\"- Video duration: {(self.total_frames / self.fps):.2f} seconds\\n\"\n            f\"- Unique vehicles: {', '.join([f'{v}: {c}' for v, c in self.vehicle_counts.items()])} (total: {sum(self.vehicle_counts.values())})\\n\"\n            f\"- Average vehicles per frame: {', '.join([f'{v}: {c:.2f}' for v, c in self.average_counts.items()])}\\n\"\n            f\"- Average speeds (pixels/s): {', '.join([f'{v}: {s:.2f}' for v, s in self.average_speeds.items()])}\\n\"\n            f\"- Peak traffic: {self.max_vehicles} vehicles at {self.max_time_sec:.2f} seconds\\n\"\n            f\"- Middle frame (at {(self.middle_index / self.fps):.2f} seconds): {', '.join([f'{v}: {c}' for v, c in self.frame_vehicle_counts[self.middle_index].items()]) if self.middle_index < len(self.frame_vehicle_counts) else 'no vehicles'}\\n\"\n            f\"- Average congestion index: {average_congestion:.2f} (0=low, 1=moderate, >2=high)\\n\"\n            f\"- Emergency alerts: {'; '.join(self.emergency_alerts) if self.emergency_alerts else 'None'}\\n\"\n            f\"- Temporal trends: First 25%: {first_section} vehicles, \"\n            f\"Middle 50%: {middle_section} vehicles, \"\n            f\"Last 25%: {last_section} vehicles\"\n        )\n        print(\"\\n--- Context Generated ---\\n\")\n        print(self.context)\n    \n    def export_to_csv(self):\n        print(\"Exporting data to CSV...\")\n        times = [i / self.fps for i in range(len(self.frame_vehicle_counts))]\n        total_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in self.frame_vehicle_counts]\n        \n        csv_data = {\n            \"Frame\": list(range(len(self.frame_vehicle_counts))),\n            \"Time (s)\": times,\n            \"Total Vehicles\": total_vehicles_per_frame,\n            \"Congestion Index\": self.congestion_indices\n        }\n        \n        for vtype in self.vehicle_counts.keys():\n            csv_data[vtype] = [frame_counts.get(vtype, 0) for frame_counts in self.frame_vehicle_counts]\n        \n        df = pd.DataFrame(csv_data)\n        try:\n            df.to_csv(\"traffic_data.csv\", index=False)\n            print(\"Traffic data exported to 'traffic_data.csv'.\")\n        except Exception as e:\n            print(f\"Error exporting CSV: {e}\")\n    \n    def generate_heatmap(self):\n        print(\"Generating traffic density heatmap...\")\n        times = [i / self.fps for i in range(len(self.frame_vehicle_counts))]\n        total_vehicles_per_frame = [sum(frame_counts.values()) for frame_counts in self.frame_vehicle_counts]\n        \n        plt.figure(figsize=(10, 4))\n        sns.heatmap([total_vehicles_per_frame], cmap=\"YlOrRd\", xticklabels=50, cbar_kws={'label': 'Vehicle Count'})\n        plt.xlabel(\"Time (seconds)\")\n        plt.ylabel(\"Density\")\n        plt.title(\"Traffic Density Heatmap\")\n        plt.xticks(ticks=np.linspace(0, len(times)-1, 5), labels=[f\"{t:.1f}\" for t in np.linspace(0, max(times), 5)])\n        try:\n            plt.savefig(\"heatmap.png\")\n            plt.close()\n            print(\"Heatmap saved as 'heatmap.png'.\")\n        except Exception as e:\n            print(f\"Error saving heatmap: {e}\")\n    \n    def generate_creative_text(self, prompt):\n        if self.text_generator:\n            try:\n                result = self.text_generator(prompt, max_new_tokens=300)\n                return result[0]['generated_text'].strip()\n            except Exception as e:\n                print(f\"Error generating text: {e}\")\n                return \"Could not generate creative text.\"\n        else:\n            return \"Creative text generation not available (NLP models not loaded).\"\n    \n    def enhance_text(self, text):\n        if self.summarizer:\n            try:\n                enhanced = self.summarizer(text, max_length=len(text.split()) + 50, min_length=len(text.split()), \n                                          do_sample=True)[0]['summary_text']\n                return enhanced\n            except Exception as e:\n                print(f\"Error enhancing text: {e}\")\n                return text\n        else:\n            return text\n    \n    def generate_report(self):\n        print(\"Generating text report...\")\n        \n        # Generate traffic flow description\n        flow_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate a detailed traffic flow analysis starting with 'The traffic flow analysis reveals...'. \"\n            \"Include patterns of congestion, peak times, and vehicle distribution.\"\n        )\n        flow_text = self.generate_creative_text(flow_prompt)\n        \n        # Generate vehicle behavior description\n        behavior_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate a detailed description of vehicle behavior starting with 'Vehicle behavior analysis shows...'. \"\n            \"Include speed patterns, types of vehicles, and any unusual events.\"\n        )\n        behavior_text = self.generate_creative_text(behavior_prompt)\n        \n        # Generate recommendations\n        recommendations_prompt = (\n            f\"Context:\\n{self.context}\\n\"\n            \"Instruction: Generate 3-4 traffic management recommendations based on this data, \"\n            \"starting with 'Based on the analysis, we recommend...'\"\n        )\n        recommendations_text = self.generate_creative_text(recommendations_prompt)\n        \n        # Combine all sections\n        report_text = (\n            f\"# Traffic Analysis Report\\n\\n\"\n            f\"## Overview\\n\\n\"\n            f\"This report analyzes traffic patterns observed in a video of duration {(self.total_frames / self.fps):.2f} seconds. \"\n            f\"A total of {sum(self.vehicle_counts.values())} unique vehicles were detected, \"\n            f\"with peak traffic occurring at {self.max_time_sec:.2f} seconds.\\n\\n\"\n            f\"## Traffic Flow Analysis\\n\\n{flow_text}\\n\\n\"\n            f\"## Vehicle Behavior\\n\\n{behavior_text}\\n\\n\"\n            f\"## Recommendations\\n\\n{recommendations_text}\\n\\n\"\n            f\"## Statistical Summary\\n\\n\"\n            f\"- Total unique vehicles: {sum(self.vehicle_counts.values())}\\n\"\n            f\"- Vehicle types: {', '.join([f'{v}: {c}' for v, c in self.vehicle_counts.items()])}\\n\"\n            f\"- Peak congestion: {self.max_vehicles} vehicles at {self.max_time_sec:.2f} seconds\\n\"\n            f\"- Average congestion index: {np.mean(self.congestion_indices):.2f}\\n\"\n        )\n        \n        # Enhance the report with more creative language\n        enhanced_report = self.enhance_text(report_text)\n        \n        print(\"\\n--- Generated Report ---\\n\")\n        print(enhanced_report)\n        \n        # Generate PDF report\n        print(\"Generating PDF report...\")\n        try:\n            pdf = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\n            styles = getSampleStyleSheet()\n            normal_style = ParagraphStyle(name='NormalWrap', parent=styles['Normal'], wordWrap='CJK')\n            heading_style = styles['Heading1']\n            subheading_style = styles['Heading2']\n            elements = []\n            \n            elements.append(Paragraph(\"Vehicle Detection Report\", heading_style))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Add current date\n            current_date = datetime.now().strftime(\"%B %d, %Y\")\n            elements.append(Paragraph(f\"Generated on: {current_date}\", normal_style))\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Process markdown-like sections in the report\n            sections = enhanced_report.split('##')\n            for i, section in enumerate(sections):\n                if i == 0:  # First part (title)\n                    continue\n                \n                lines = section.strip().split('\\n')\n                section_title = lines[0].strip()\n                section_content = '\\n'.join(lines[1:]).strip()\n                \n                elements.append(Paragraph(section_title, subheading_style))\n                elements.append(Spacer(1, 0.1 * inch))\n                \n                for paragraph in section_content.split('\\n\\n'):\n                    if paragraph.strip():\n                        elements.append(Paragraph(paragraph.strip(), normal_style))\n                        elements.append(Spacer(1, 0.1 * inch))\n            \n            # Add vehicle statistics table\n            elements.append(Paragraph(\"Vehicle Statistics\", subheading_style))\n            elements.append(Spacer(1, 0.1 * inch))\n            table_data = [['Vehicle Type', 'Unique Count', 'Avg/Frame', 'Middle Frame', 'Avg Speed (px/s)']]\n            \n            for vtype in self.vehicle_counts.keys():\n                table_data.append([\n                    vtype, \n                    str(self.vehicle_counts.get(vtype, 0)), \n                    f\"{self.average_counts.get(vtype, 0):.2f}\", \n                    str(self.frame_vehicle_counts[self.middle_index].get(vtype, 0) if self.middle_index < len(self.frame_vehicle_counts) else 0), \n                    f\"{self.average_speeds.get(vtype, 0):.2f}\"\n                ])\n            \n            table = Table(table_data)\n            table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                ('FONTSIZE', (0, 0), (-1, 0), 10),\n                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black)\n            ]))\n            elements.append(table)\n            elements.append(Spacer(1, 0.2 * inch))\n            \n            # Add visualizations\n            elements.append(Paragraph(\"Visualizations\", subheading_style))\n            elements.append(Spacer(1, 0.1 * inch))\n            image_paths = [(\"heatmap.png\", \"Traffic Density Heatmap\"), (\"middle_frame.jpg\", \"Middle Frame\"), (\"peak_frame.jpg\", \"Peak Traffic Frame\")]\n            \n            for path, title in image_paths:\n                if os.path.exists(path):\n                    img = ReportLabImage(path, width=5*inch, height=3*inch)\n                    elements.append(Paragraph(title, normal_style))\n                    elements.append(img)\n                    elements.append(Spacer(1, 0.1 * inch))\n                else:\n                    elements.append(Paragraph(f\"{title} not available.\", normal_style))\n            \n            pdf.build(elements)\n            print(\"PDF report generated as 'report.pdf'.\")\n        except Exception as e:\n            print(f\"Error generating PDF: {e}\")\n    \n    def get_unique_vehicles_in_timerange(self, start_time, end_time):\n        \"\"\"\n        Count unique vehicles that appeared between start_time and end_time.\n        \"\"\"\n        unique_vehicles = defaultdict(int)\n        \n        for track_id, data in self.track_history.items():\n            # Check if the vehicle was present during the time range\n            if data['first_seen'] <= end_time and data['last_seen'] >= start_time:\n                vehicle_type = data['vehicle_type']\n                if vehicle_type:\n                    unique_vehicles[vehicle_type] += 1\n        \n        return dict(unique_vehicles)\n    \n    def answer_question(self, question):\n        \"\"\"\n        Rules-based factual question answering from stored data, with creative descriptions\n        where appropriate.\n        \"\"\"\n        if not self.processed:\n            return \"Please process the video first before asking questions.\"\n            \n        if not question or not isinstance(question, str):\n            return \"Please ask a valid question.\"\n            \n        # Load saved data to ensure accuracy\n        try:\n            with open('traffic_data.json', 'r') as f:\n                data = json.load(f)\n        except:\n            # Fall back to in-memory data if file not found\n            data = {\n                'vehicle_counts': self.vehicle_counts,\n                'average_counts': self.average_counts,\n                'average_speeds': self.average_speeds,\n                'max_vehicles': self.max_vehicles,\n                'max_time_sec': self.max_time_sec,\n                'total_frames': self.total_frames,\n                'fps': self.fps,\n                'emergency_alerts': self.emergency_alerts,\n                'middle_index': self.middle_index,\n                'middle_frame_counts': self.frame_vehicle_counts[self.middle_index] if self.middle_index < len(self.frame_vehicle_counts) else {}\n            }\n            \n        # Normalize question\n        question = question.lower().strip()\n        tokens = word_tokenize(question)\n        stop_words = set(stopwords.words('english'))\n        filtered_tokens = [w for w in tokens if w.isalnum() and w not in stop_words]\n        \n        # Time range handling - IMPROVED for unique vehicle counting\n        time_range_match = re.search(r\"from (\\d+(?:\\.\\d+)?)\\s*(?:to|s(?:econd)?s?\\s*to)\\s*(\\d+(?:\\.\\d+)?)\\s*s(?:econd)?s?\", question)\n        time_match = re.search(r\"at (\\d+(?:\\.\\d+)?) seconds?\", question)\n        \n        if time_range_match:\n            start_time = float(time_range_match.group(1))\n            end_time = float(time_range_match.group(2))\n            \n            if start_time >= end_time or end_time > self.total_frames / self.fps:\n                return f\"Invalid time range: {start_time}s to {end_time}s. Video duration is {self.total_frames/self.fps:.2f}s.\"\n                \n            # Get unique vehicles in the time range\n            unique_vehicles = self.get_unique_vehicles_in_timerange(start_time, end_time)\n            total_unique = sum(unique_vehicles.values())\n            \n            # Generate response\n            response = f\"From {start_time}s to {end_time}s, {total_unique} unique vehicles passed: \"\n            response += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in unique_vehicles.items()])\n            return response\n            \n        elif time_match:\n            time_sec = float(time_match.group(1))\n            \n            if time_sec > self.total_frames / self.fps:\n                return f\"Time {time_sec}s exceeds video duration ({self.total_frames/self.fps:.2f}s).\"\n                \n            frame_index = min(int(time_sec * self.fps), self.total_frames - 1)\n            frame_counts = self.frame_vehicle_counts[frame_index]\n            \n            if not frame_counts:\n                return f\"At {time_sec}s: No vehicles detected.\"\n                \n            response = f\"At {time_sec}s: \"\n            response += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in frame_counts.items()])\n            return response\n        \n        # Total vehicle questions\n        if any(w in filtered_tokens for w in ['total', 'all']) and any(w in filtered_tokens for w in ['vehicle', 'vehicles', 'cars', 'detected']):\n            return f\"Total unique vehicles detected: {sum(data['vehicle_counts'].values())}.\"\n        \n        # Vehicle type questions\n        for vehicle_type in data['vehicle_counts'].keys():\n            if vehicle_type.lower() in question:\n                count = data['vehicle_counts'].get(vehicle_type, 0)\n                avg = data['average_counts'].get(vehicle_type, 0)\n                speed = data['average_speeds'].get(vehicle_type, 0)\n                response = f\"{count} {vehicle_type}(s) detected in total. \"\n                response += f\"Average of {avg:.2f} per frame. \"\n                \n                if speed > 0:\n                    response += f\"Average speed: {speed:.2f} pixels/second.\"\n                return response\n                \n        # Peak traffic questions\n        if any(w in filtered_tokens for w in ['peak', 'busiest', 'maximum']):\n            if any(w in filtered_tokens for w in ['time', 'when']):\n                return f\"Peak traffic occurred at {data['max_time_sec']:.2f} seconds with {data['max_vehicles']} vehicles in frame.\"\n            else:\n                return f\"Peak traffic was {data['max_vehicles']} vehicles at {data['max_time_sec']:.2f} seconds.\"\n                \n        # Middle of video questions\n        if any(w in filtered_tokens for w in ['middle', 'mid']):\n            middle_time = data['middle_index'] / data['fps']\n            middle_counts = data.get('middle_frame_counts', {})\n            \n            if not middle_counts:\n                return f\"At the middle of the video ({middle_time:.2f}s): No vehicles detected.\"\n                \n            response = f\"At the middle of the video ({middle_time:.2f}s): \"\n            response += \", \".join([f\"{count} {vtype}(s)\" for vtype, count in middle_counts.items()])\n            return response\n            \n        # Duration question\n        if any(w in filtered_tokens for w in ['duration', 'long', 'length']):\n            return f\"Video duration: {self.total_frames/self.fps:.2f} seconds.\"\n            \n        # Types of vehicles\n        if any(w in filtered_tokens for w in ['types', 'kind', 'kinds']) and any(w in filtered_tokens for w in ['vehicle', 'vehicles']):\n            return f\"Types of vehicles detected: {', '.join(data['vehicle_counts'].keys())}.\"\n            \n        # Emergency vehicles\n        if any(w in filtered_tokens for w in ['emergency', 'ambulance', 'police']):\n            if data['emergency_alerts']:\n                return f\"Emergency vehicle activity: {'; '.join(data['emergency_alerts'])}\"\n            else:\n                return \"No emergency vehicles were detected in the video.\"\n                \n        # Descriptive questions\n        if any(w in filtered_tokens for w in ['describe', 'description', 'summarize', 'summary', 'explain']):\n            # Use language model for creative descriptions\n            description_prompt = (\n                f\"Context:\\n{self.context}\\n\"\n                f\"Instruction: Describe the traffic scene based on this data in detail. \"\n                f\"Include information about vehicle types, traffic flow, congestion levels, and any notable events.\"\n            )\n            description = self.generate_creative_text(description_prompt)\n            return self.enhance_text(description)\n            \n        # Fall back to context info\n        return f\"Here's what I know about the video:\\n{self.context}\"\n        \n    def run_chat_loop(self):\n        \"\"\"Run an interactive chat loop to answer questions about the video.\"\"\"\n        print(\"\\nVideo processing complete. Ask any question about the video (type 'exit' to quit).\")\n        \n        # Generate suggestions based on detected vehicles\n        suggestions = [\n            \"How many vehicles were detected in total?\",\n            \"What types of vehicles were detected?\",\n            \"What was the peak traffic time?\",\n            \"How many unique vehicles passed from 2 to 5 seconds?\",\n            \"Describe the traffic in the video.\",\n            \"What was happening in the middle of the video?\"\n        ]\n        \n        # Add vehicle-specific suggestions\n        for vehicle_type in self.vehicle_counts.keys():\n            suggestions.append(f\"How many {vehicle_type}s were detected?\")\n            \n        # Add emergency vehicle suggestions if applicable\n        if self.emergency_alerts:\n            suggestions.append(\"Were there any emergency vehicles?\")\n            \n        print(\"Suggested questions:\")\n        for i, suggestion in enumerate(suggestions[:8], 1):  # Limit to 8 suggestions\n            print(f\"{i}. {suggestion}\")\n            \n        while True:\n            try:\n                user_query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n                if user_query.lower().strip() == \"exit\":\n                    print(\"Goodbye.\")\n                    break\n                    \n                response = self.answer_question(user_query)\n                print(\"\\nAnswer:\")\n                print(response)\n                \n            except KeyboardInterrupt:\n                print(\"\\nChat interrupted. Type 'exit' to quit or continue.\")\n            except Exception as e:\n                print(f\"Error in chat loop: {e}\")\n\ndef main():\n    model_path = \"/kaggle/input/testing/yolov8x.pt\"  # Update with your model path\n    video_path = \"/kaggle/input/testing/test_30_Multi - Made with Clipchamp.mp4\"  # Update with your video path\n    \n    analyzer = TrafficAnalyzer(model_path, video_path)\n    if analyzer.process_video():\n        analyzer.run_chat_loop()\n    else:\n        print(\"Failed to process video.\")\n    \n    # Cleanup\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"id":"kCmBvaKHaOjq","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ce272bd753214c4581a663d1352de470","fec4f3a6eec24ee28c5f14b4b1a4af51","db18797e4b6648c888328a7a090d5ead","ffb5dc2afc5d4ea797503b55c1f60d72","ae9f0e3238f4441dac8fb7409fe7e9ca","098716c4cb9c417fbb4300f953b0611d","ee4e7b3f53a947c8a83e0c1f5120b88a","3c72237289e749eba4a7efe8094ee87e","b3bdbf78c05b429baa9865049d3fa24d","82789f95bfe3423cbb3a25a1ef205669","a822574493f84e0aabc213ec46afd117","86dd7f0cba3b48699084fdf2a96247c9","e05cc23f121641fdad20db5e6de206b2","41714b086cdb4f8bb952d88cb7e2dd40","b1d524c540874729af68aa497e7357d4","cf2c8651dd694a59a1807eac671c3206","1c63101885884b3791c41bb36e7802fe","996bda6fdef94f57b80913e213f1b73a","4ccdfe10c1ca40cc91227821740aee15","50d3bf0bf12c4237bf55e327c330dadd","ecd796a699654bdfb1de9a3370410f2f","b1d6d2b3220b48d1ae08f1de92cb4694","21499c6654e84747836989f8a5cb3341","75c39fe1fc95439db2c05780d17a670d","b8303f3f282346c59ac54fedc1f0a869","cf0af215c75042ffac849cffc787f94f","e6f057248c714a9b8b3f666bc86d38ec","1c57704e3fe64a339407c15eddc5d679","0457d42601f1487db124a6749420afdc","834bf2ced6ab42e39b2d6a762d60da14","cd1f0d79716e4838920062323f13a55a","699878b8d2df4b03b7b2395c61b58027","b732d26a7e554eb9b18c319b34b8607b","e94162eac57544aa9f8c10c79771b05f","96a9d9eb6e754379b87811834e760d5d","2384453afe254b518e30ec3c2fc94952","569ab38150214f4aad9ef019d4f98b52","1ea083a00bb34126848df779768c8972","8de0d17a9e2e416f86b2552860fadf81","20230f87c44f43dea784a6a51d3ed7c4","655d1fcfdda1461a80252468a57bfc50","6e274f5418954492a98154057e059592","2039bc802db749a49e50d3d6b084a2bf","f65d6fb15bc34937b64ca3ef20388571","8286ff3f92a942068ffc6be599ae5064","23e71c6333624c44b31a9141585ef837","6ef7e66074ff4b959d8168c4595d36ca","ee7ad369e2a549b8aceb2520a33222b6","82cff09161984fb8b30189706a1f2b98","a00e4c2a89aa4419a11e1178c19eb45b","8b38eed874dc497f8c337d1d2cb7b799","f64c23c26a8f4e48b07c79b79667b7fc","a6107a2ee9b84ed78ba733f506d6f39b","74b6aedf17fd4759a6b4487a5bf0b40f","4fa6657de7894c898eb322f0f19b950b","b315f15b7a894d83bb8e31f52931276b","395239a18aad4603baf4faac7b521161","3b27c3ed61a44a31982dcd720ff42453","367649468c3c40bcb88eb821a229ee93","80ae2fe68489447bbb365affc976a15e","bc96ba90e41d4dd18863eeefd840502b","66ab0df0c6db4ea18eeaaefbf6229987","544c49ae0d8c4835a8cae7a75e8ed6e3","3bcc2edcbd0944e1a0b0853bad38524d","41ce865046724eb286594c902afe5861","9cc9846554d046a78b80dfd8edffa91d","07ed84259ec64b2bb95b76a8010bc6bd","e2099dae21b44c09a78b947f618666ef","5789d402bea1468f94fbd1069fe6d4cc","e3a41a8786e24aad8890e96e1a8bfe6f","07c81bd2c5534e4a9b8487170993c95d","4734d02ab98f4e33a86d4962e3bf167a","f211367c35ad4a998bbc2199a9bc7c11","bddde1a956f7446692d70dae4344d79e","fb41bc8dc71d4a4890249d7933d34098","fcd8d3a102f844d49ccbe4a8071adf51","11ddbc6a2fe945718a84ef589eff3a40"]},"outputId":"3cd9c60e-6e33-4d8e-9286-e3beb09d54ee","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:27:50.318410Z","iopub.execute_input":"2025-04-20T18:27:50.319103Z"}},"outputs":[{"name":"stdout","text":"YOLO model loaded successfully.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7398d659906142c6911cf72785047c83"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093dbed0fed1499c86aa71c8e724c554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1e26bcfc77411385057eb2043d4764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d103428858049208cd86c5eac2dae96"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b5a8cf0bd34bee9bb18143a0e12b6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043ce714b06843ac9873e484056a527c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a10ed1a38654258b800b673b560f0c2"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Flan-T5-Base loaded for text generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f97244cba654f3dba64b492ed6232e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"973febb1ba114a829753bad16cd98791"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c9b75eaf9814728b7744e9235531dee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d906fc34ed5e424db54aee137455cafb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d11ca5bdabfc41558de14c478b5f0ee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c76bb29daae4ccea35f9c130188f019"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"BART-Large-CNN loaded for summarization.\nProcessing video...\n\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\nCollecting lap>=0.5.12\n  Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from lap>=0.5.12) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->lap>=0.5.12) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nDownloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 27.0 MB/s eta 0:00:00\nInstalling collected packages: lap\nSuccessfully installed lap-0.5.12\n\n\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 3.6s, installed 1 package: ['lap>=0.5.12']\n\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n\n\n0: 480x640 9 cars, 68.0ms\nSpeed: 3.8ms preprocess, 68.0ms inference, 257.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 59.8ms\nSpeed: 2.2ms preprocess, 59.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 59.8ms\nSpeed: 2.1ms preprocess, 59.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 48.4ms\nSpeed: 2.6ms preprocess, 48.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 48.2ms\nSpeed: 2.3ms preprocess, 48.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 48.1ms\nSpeed: 2.7ms preprocess, 48.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.3ms\nSpeed: 2.2ms preprocess, 37.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.0ms\nSpeed: 2.9ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.4ms\nSpeed: 2.7ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 36.5ms\nSpeed: 2.4ms preprocess, 36.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.6ms\nSpeed: 2.4ms preprocess, 36.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.6ms\nSpeed: 2.3ms preprocess, 36.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.2ms\nSpeed: 2.1ms preprocess, 37.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.7ms\nSpeed: 2.0ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.7ms\nSpeed: 2.1ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.0ms\nSpeed: 2.1ms preprocess, 37.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.0ms\nSpeed: 2.4ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 36.9ms\nSpeed: 2.1ms preprocess, 36.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.5ms\nSpeed: 2.6ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 36.4ms\nSpeed: 2.6ms preprocess, 36.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.5ms\nSpeed: 2.6ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.2ms\nSpeed: 2.3ms preprocess, 37.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.0ms\nSpeed: 2.2ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.0ms\nSpeed: 2.1ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.7ms\nSpeed: 2.0ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.8ms\nSpeed: 2.4ms preprocess, 36.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.2ms\nSpeed: 2.6ms preprocess, 37.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.0ms\nSpeed: 2.6ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.7ms\nSpeed: 2.3ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.1ms\nSpeed: 2.1ms preprocess, 37.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.4ms\nSpeed: 2.0ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.5ms\nSpeed: 2.6ms preprocess, 36.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.0ms\nSpeed: 2.3ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.7ms\nSpeed: 2.1ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.8ms\nSpeed: 2.6ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.1ms\nSpeed: 2.4ms preprocess, 36.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 35.6ms\nSpeed: 2.8ms preprocess, 35.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.0ms\nSpeed: 2.1ms preprocess, 36.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.0ms\nSpeed: 2.1ms preprocess, 38.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.3ms\nSpeed: 2.7ms preprocess, 38.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.4ms\nSpeed: 2.3ms preprocess, 38.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.3ms\nSpeed: 2.3ms preprocess, 36.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.4ms\nSpeed: 2.2ms preprocess, 37.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 36.9ms\nSpeed: 2.6ms preprocess, 36.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.5ms\nSpeed: 2.2ms preprocess, 37.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.6ms\nSpeed: 2.3ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.1ms\nSpeed: 2.1ms preprocess, 37.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.3ms\nSpeed: 2.0ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.9ms\nSpeed: 2.4ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.7ms\nSpeed: 2.3ms preprocess, 37.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.2ms\nSpeed: 2.1ms preprocess, 37.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.3ms\nSpeed: 2.6ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 36.9ms\nSpeed: 2.2ms preprocess, 36.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.6ms\nSpeed: 2.6ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.3ms\nSpeed: 2.3ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.6ms\nSpeed: 2.7ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.3ms\nSpeed: 2.4ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.7ms\nSpeed: 2.3ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.1ms\nSpeed: 2.1ms preprocess, 37.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.6ms\nSpeed: 2.3ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.6ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.8ms\nSpeed: 2.2ms preprocess, 36.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.9ms\nSpeed: 2.2ms preprocess, 36.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.5ms\nSpeed: 2.3ms preprocess, 37.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.2ms\nSpeed: 2.5ms preprocess, 36.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.8ms\nSpeed: 2.4ms preprocess, 36.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.2ms\nSpeed: 3.3ms preprocess, 37.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.5ms\nSpeed: 4.2ms preprocess, 36.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 35.7ms\nSpeed: 3.5ms preprocess, 35.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 36.2ms\nSpeed: 2.7ms preprocess, 36.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.5ms\nSpeed: 2.1ms preprocess, 36.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.2ms\nSpeed: 3.8ms preprocess, 37.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.1ms\nSpeed: 3.4ms preprocess, 37.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.1ms\nSpeed: 2.1ms preprocess, 37.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.3ms\nSpeed: 2.8ms preprocess, 36.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.6ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.1ms\nSpeed: 4.2ms preprocess, 37.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.1ms\nSpeed: 3.4ms preprocess, 37.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 37.6ms\nSpeed: 2.6ms preprocess, 37.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.0ms\nSpeed: 2.6ms preprocess, 38.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.5ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 37.4ms\nSpeed: 2.5ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 37.5ms\nSpeed: 2.1ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.2ms\nSpeed: 2.2ms preprocess, 37.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.6ms\nSpeed: 2.0ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.2ms\nSpeed: 2.8ms preprocess, 37.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.1ms\nSpeed: 2.2ms preprocess, 37.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 36.7ms\nSpeed: 2.6ms preprocess, 36.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.4ms\nSpeed: 2.3ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 38.2ms\nSpeed: 2.2ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 38.1ms\nSpeed: 2.2ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 38.1ms\nSpeed: 2.3ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.7ms\nSpeed: 2.6ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.8ms\nSpeed: 2.2ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 36.9ms\nSpeed: 2.6ms preprocess, 36.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 37.4ms\nSpeed: 2.2ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 38.0ms\nSpeed: 2.6ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 38.1ms\nSpeed: 2.2ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 36.9ms\nSpeed: 2.1ms preprocess, 36.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 37.8ms\nSpeed: 2.2ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 37.7ms\nSpeed: 2.6ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 37.0ms\nSpeed: 2.4ms preprocess, 37.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 umbrella, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.9ms\nSpeed: 2.0ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.1ms\nSpeed: 2.1ms preprocess, 37.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.4ms\nSpeed: 2.1ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.7ms\nSpeed: 2.1ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 37.5ms\nSpeed: 2.7ms preprocess, 37.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.4ms\nSpeed: 2.1ms preprocess, 37.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.0ms\nSpeed: 2.0ms preprocess, 37.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.2ms preprocess, 38.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.0ms\nSpeed: 2.0ms preprocess, 37.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.6ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.3ms\nSpeed: 2.3ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.0ms\nSpeed: 2.2ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.7ms\nSpeed: 2.7ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.7ms\nSpeed: 2.4ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.7ms\nSpeed: 2.3ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.3ms\nSpeed: 2.6ms preprocess, 38.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.6ms\nSpeed: 2.3ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.2ms\nSpeed: 2.5ms preprocess, 37.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.3ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.0ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.6ms\nSpeed: 2.3ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.5ms\nSpeed: 2.2ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.9ms\nSpeed: 2.6ms preprocess, 37.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.4ms\nSpeed: 2.3ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.5ms\nSpeed: 2.3ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.6ms\nSpeed: 2.6ms preprocess, 37.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.7ms\nSpeed: 2.6ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.6ms\nSpeed: 2.2ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.0ms\nSpeed: 2.5ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.2ms\nSpeed: 2.9ms preprocess, 38.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 37.4ms\nSpeed: 2.6ms preprocess, 37.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 36.3ms\nSpeed: 2.5ms preprocess, 36.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.6ms\nSpeed: 2.4ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 umbrella, 36.2ms\nSpeed: 2.7ms preprocess, 36.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.6ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 38.0ms\nSpeed: 2.2ms preprocess, 38.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 37.7ms\nSpeed: 2.2ms preprocess, 37.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 37.6ms\nSpeed: 2.4ms preprocess, 37.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.7ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.3ms preprocess, 38.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.1ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.0ms preprocess, 38.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.2ms\nSpeed: 3.1ms preprocess, 37.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.5ms preprocess, 37.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.4ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.0ms preprocess, 37.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.0ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.1ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.1ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.0ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.2ms\nSpeed: 2.1ms preprocess, 37.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.7ms preprocess, 38.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.3ms preprocess, 38.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.6ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.4ms preprocess, 38.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.0ms preprocess, 38.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.4ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.5ms\nSpeed: 2.0ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.0ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.7ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.6ms preprocess, 38.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.2ms preprocess, 38.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.4ms\nSpeed: 2.7ms preprocess, 37.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 36.8ms\nSpeed: 3.1ms preprocess, 36.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.1ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 42.1ms\nSpeed: 2.0ms preprocess, 42.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 1 umbrella, 42.6ms\nSpeed: 2.7ms preprocess, 42.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 umbrella, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 umbrella, 38.2ms\nSpeed: 2.7ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.3ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.6ms preprocess, 37.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 36.5ms\nSpeed: 2.5ms preprocess, 36.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 1 umbrella, 37.2ms\nSpeed: 2.6ms preprocess, 37.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.9ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 40.0ms\nSpeed: 2.6ms preprocess, 40.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 36.2ms\nSpeed: 2.1ms preprocess, 36.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 umbrella, 37.7ms\nSpeed: 2.9ms preprocess, 37.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 14 cars, 1 umbrella, 37.5ms\nSpeed: 2.6ms preprocess, 37.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 14 cars, 1 umbrella, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.1ms\nSpeed: 2.1ms preprocess, 37.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.0ms\nSpeed: 2.2ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.7ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.0ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 umbrella, 37.0ms\nSpeed: 2.5ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.6ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.5ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 umbrella, 36.9ms\nSpeed: 2.6ms preprocess, 36.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.6ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.9ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.2ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.5ms\nSpeed: 2.2ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.3ms preprocess, 38.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.0ms\nSpeed: 2.2ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 bus, 1 umbrella, 38.0ms\nSpeed: 2.3ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.5ms\nSpeed: 2.1ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.7ms\nSpeed: 2.3ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.5ms\nSpeed: 2.2ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.7ms\nSpeed: 2.3ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.6ms\nSpeed: 2.6ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.2ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.6ms\nSpeed: 2.0ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.2ms preprocess, 38.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.1ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.6ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.6ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.2ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 38.4ms\nSpeed: 2.0ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.0ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.0ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.4ms\nSpeed: 2.0ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.1ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.2ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.6ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.2ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.1ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.0ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.5ms\nSpeed: 2.6ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 38.1ms\nSpeed: 2.6ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 13 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.6ms\nSpeed: 2.6ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 39.0ms\nSpeed: 2.4ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.4ms\nSpeed: 2.0ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.7ms\nSpeed: 2.5ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.4ms\nSpeed: 2.6ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.2ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 37.4ms\nSpeed: 2.6ms preprocess, 37.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.2ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.0ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.4ms\nSpeed: 2.0ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.8ms\nSpeed: 2.2ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.6ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.2ms preprocess, 37.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.7ms\nSpeed: 2.9ms preprocess, 37.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 37.9ms\nSpeed: 2.6ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 truck, 1 umbrella, 38.6ms\nSpeed: 2.2ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.0ms\nSpeed: 2.2ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 37.3ms\nSpeed: 2.3ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.4ms\nSpeed: 2.0ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 1 umbrella, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 37.5ms\nSpeed: 2.2ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 38.4ms\nSpeed: 2.0ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 37.9ms\nSpeed: 2.3ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 38.2ms\nSpeed: 2.2ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 37.8ms\nSpeed: 2.1ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 37.6ms\nSpeed: 2.2ms preprocess, 37.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 38.1ms\nSpeed: 2.2ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 38.1ms\nSpeed: 2.3ms preprocess, 38.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 38.0ms\nSpeed: 2.1ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 37.9ms\nSpeed: 2.0ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 37.6ms\nSpeed: 2.4ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 38.1ms\nSpeed: 2.0ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 38.1ms\nSpeed: 2.0ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 37.6ms\nSpeed: 1.9ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.7ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.7ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 38.1ms\nSpeed: 2.6ms preprocess, 38.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.1ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.3ms\nSpeed: 1.9ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.8ms\nSpeed: 2.6ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 37.8ms\nSpeed: 2.2ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.8ms\nSpeed: 2.2ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 38.6ms\nSpeed: 2.2ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.8ms\nSpeed: 2.1ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.2ms\nSpeed: 2.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.0ms\nSpeed: 2.3ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.5ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.7ms\nSpeed: 2.2ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.7ms preprocess, 38.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.2ms\nSpeed: 2.5ms preprocess, 37.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.4ms\nSpeed: 2.3ms preprocess, 38.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.9ms\nSpeed: 2.1ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.8ms\nSpeed: 2.0ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.7ms\nSpeed: 2.0ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 37.9ms\nSpeed: 2.4ms preprocess, 37.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.2ms\nSpeed: 2.6ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.8ms preprocess, 38.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.1ms\nSpeed: 2.8ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.2ms\nSpeed: 2.1ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.9ms\nSpeed: 2.6ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 37.5ms\nSpeed: 2.6ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.4ms\nSpeed: 2.6ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.0ms\nSpeed: 2.7ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.6ms\nSpeed: 2.2ms preprocess, 38.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.5ms\nSpeed: 2.3ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.8ms\nSpeed: 2.3ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.6ms\nSpeed: 2.2ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.1ms\nSpeed: 2.1ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.7ms\nSpeed: 2.2ms preprocess, 38.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 37.7ms\nSpeed: 2.0ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.1ms\nSpeed: 2.2ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 38.5ms\nSpeed: 2.0ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.1ms\nSpeed: 2.6ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.7ms\nSpeed: 2.4ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 36.8ms\nSpeed: 2.2ms preprocess, 36.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 36.4ms\nSpeed: 2.6ms preprocess, 36.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 37.1ms\nSpeed: 2.3ms preprocess, 37.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.4ms\nSpeed: 2.2ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.7ms\nSpeed: 2.7ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.8ms\nSpeed: 2.2ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.7ms\nSpeed: 2.3ms preprocess, 38.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 40.1ms\nSpeed: 2.3ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.1ms\nSpeed: 2.6ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.2ms\nSpeed: 2.6ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.7ms\nSpeed: 2.6ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.7ms\nSpeed: 2.6ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 38.2ms\nSpeed: 2.1ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 38.3ms\nSpeed: 2.1ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.5ms\nSpeed: 2.2ms preprocess, 38.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.1ms\nSpeed: 2.0ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 38.8ms\nSpeed: 2.2ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.7ms\nSpeed: 2.2ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 37.9ms\nSpeed: 2.0ms preprocess, 37.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.5ms\nSpeed: 2.7ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.6ms\nSpeed: 2.3ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.2ms\nSpeed: 2.4ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.5ms\nSpeed: 2.1ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.1ms\nSpeed: 2.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.5ms\nSpeed: 2.0ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.2ms\nSpeed: 2.0ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.7ms\nSpeed: 2.0ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.2ms\nSpeed: 2.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.3ms\nSpeed: 2.6ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.8ms\nSpeed: 2.7ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.0ms\nSpeed: 2.1ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.0ms\nSpeed: 2.1ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 1 truck, 38.7ms\nSpeed: 3.0ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.0ms\nSpeed: 2.1ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.3ms\nSpeed: 2.6ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.2ms\nSpeed: 2.6ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.0ms\nSpeed: 2.3ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.9ms\nSpeed: 2.3ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 39.0ms\nSpeed: 2.1ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.1ms\nSpeed: 2.2ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.6ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 40.0ms\nSpeed: 2.2ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.1ms\nSpeed: 2.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 37.8ms\nSpeed: 2.1ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.8ms\nSpeed: 2.0ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.4ms\nSpeed: 2.4ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.4ms\nSpeed: 2.3ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 39.6ms\nSpeed: 2.2ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 38.7ms\nSpeed: 2.2ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.8ms\nSpeed: 2.3ms preprocess, 38.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.4ms\nSpeed: 2.3ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 40.1ms\nSpeed: 2.3ms preprocess, 40.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.4ms\nSpeed: 2.1ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.1ms\nSpeed: 2.6ms preprocess, 39.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.3ms\nSpeed: 2.0ms preprocess, 38.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.4ms\nSpeed: 2.3ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.7ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 37.9ms\nSpeed: 2.0ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 39.4ms\nSpeed: 2.3ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 truck, 39.5ms\nSpeed: 2.0ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 38.5ms\nSpeed: 2.3ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 4 cars, 2 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 38.6ms\nSpeed: 2.1ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 39.8ms\nSpeed: 2.1ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.5ms\nSpeed: 2.0ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.8ms\nSpeed: 2.3ms preprocess, 38.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 37.4ms\nSpeed: 2.5ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 1 truck, 38.3ms\nSpeed: 2.3ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 40.0ms\nSpeed: 2.3ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.1ms\nSpeed: 2.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.6ms\nSpeed: 2.2ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.0ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.7ms\nSpeed: 2.1ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.7ms\nSpeed: 2.1ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.7ms\nSpeed: 2.6ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.1ms\nSpeed: 2.6ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 38.8ms\nSpeed: 2.2ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.6ms\nSpeed: 2.3ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.4ms\nSpeed: 2.3ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 40.0ms\nSpeed: 2.2ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.0ms\nSpeed: 2.3ms preprocess, 39.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.8ms\nSpeed: 2.1ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.8ms\nSpeed: 2.4ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.6ms\nSpeed: 2.2ms preprocess, 38.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.2ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.9ms\nSpeed: 2.1ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 40.3ms\nSpeed: 2.1ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.6ms\nSpeed: 2.1ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.5ms\nSpeed: 2.0ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.9ms\nSpeed: 2.5ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.6ms\nSpeed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.4ms\nSpeed: 2.1ms preprocess, 39.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.3ms\nSpeed: 2.5ms preprocess, 39.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.8ms\nSpeed: 2.1ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.3ms\nSpeed: 2.2ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.7ms\nSpeed: 2.1ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.1ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.6ms\nSpeed: 2.3ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.5ms\nSpeed: 2.1ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 39.2ms\nSpeed: 2.1ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 truck, 39.8ms\nSpeed: 2.6ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.2ms\nSpeed: 2.1ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.7ms\nSpeed: 2.1ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.2ms\nSpeed: 2.1ms preprocess, 39.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.1ms\nSpeed: 2.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.7ms\nSpeed: 2.1ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.4ms\nSpeed: 2.6ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.9ms\nSpeed: 2.2ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.0ms\nSpeed: 2.1ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 truck, 39.2ms\nSpeed: 2.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.5ms\nSpeed: 2.1ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.7ms\nSpeed: 2.1ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.9ms\nSpeed: 2.1ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 40.0ms\nSpeed: 2.3ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 38.9ms\nSpeed: 2.4ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 40.3ms\nSpeed: 2.0ms preprocess, 40.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.8ms\nSpeed: 2.1ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.7ms\nSpeed: 2.7ms preprocess, 38.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 38.8ms\nSpeed: 2.0ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 40.4ms\nSpeed: 2.3ms preprocess, 40.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.6ms\nSpeed: 2.6ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.7ms\nSpeed: 2.0ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.1ms\nSpeed: 2.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.7ms\nSpeed: 2.0ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.8ms\nSpeed: 2.2ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 40.7ms\nSpeed: 2.0ms preprocess, 40.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.4ms\nSpeed: 2.6ms preprocess, 38.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 38.0ms\nSpeed: 2.8ms preprocess, 38.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.5ms\nSpeed: 2.0ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.6ms\nSpeed: 2.0ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 37.6ms\nSpeed: 2.3ms preprocess, 37.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.7ms\nSpeed: 2.1ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.4ms\nSpeed: 2.6ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 40.2ms\nSpeed: 2.0ms preprocess, 40.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.7ms\nSpeed: 2.0ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 40.2ms\nSpeed: 2.3ms preprocess, 40.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.0ms\nSpeed: 2.4ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.6ms\nSpeed: 2.3ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.5ms\nSpeed: 2.3ms preprocess, 38.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.3ms\nSpeed: 2.4ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.8ms\nSpeed: 2.3ms preprocess, 38.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.6ms\nSpeed: 2.3ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.5ms\nSpeed: 2.3ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.8ms\nSpeed: 2.3ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 37.6ms\nSpeed: 2.1ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 40.0ms\nSpeed: 2.2ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.0ms\nSpeed: 2.7ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.0ms\nSpeed: 2.3ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 38.3ms\nSpeed: 3.1ms preprocess, 38.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.0ms\nSpeed: 2.8ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 49.2ms\nSpeed: 2.4ms preprocess, 49.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.2ms\nSpeed: 2.4ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.0ms\nSpeed: 2.3ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.5ms\nSpeed: 2.1ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.5ms\nSpeed: 2.0ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.2ms\nSpeed: 2.6ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.6ms\nSpeed: 2.3ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 38.6ms\nSpeed: 2.6ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.7ms\nSpeed: 2.3ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.5ms\nSpeed: 2.1ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.5ms\nSpeed: 2.6ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 39.7ms\nSpeed: 2.0ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 40.1ms\nSpeed: 2.1ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 38.6ms\nSpeed: 2.7ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 truck, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 39.0ms\nSpeed: 2.7ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 39.4ms\nSpeed: 2.3ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 39.7ms\nSpeed: 2.0ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 39.8ms\nSpeed: 2.6ms preprocess, 39.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 38.4ms\nSpeed: 2.6ms preprocess, 38.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 37.5ms\nSpeed: 2.7ms preprocess, 37.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 38.9ms\nSpeed: 2.9ms preprocess, 38.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 38.7ms\nSpeed: 3.0ms preprocess, 38.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 38.0ms\nSpeed: 3.4ms preprocess, 38.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.9ms\nSpeed: 3.0ms preprocess, 38.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 41.9ms\nSpeed: 2.6ms preprocess, 41.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 39.1ms\nSpeed: 2.7ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 1 truck, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 38.8ms\nSpeed: 2.7ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 38.8ms\nSpeed: 2.6ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 39.1ms\nSpeed: 2.9ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 38.6ms\nSpeed: 2.6ms preprocess, 38.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 39.1ms\nSpeed: 2.5ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 38.8ms\nSpeed: 2.9ms preprocess, 38.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 truck, 39.5ms\nSpeed: 2.5ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.8ms\nSpeed: 2.6ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 4 trucks, 40.5ms\nSpeed: 2.2ms preprocess, 40.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 4 trucks, 37.9ms\nSpeed: 2.2ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 2 trucks, 39.8ms\nSpeed: 2.7ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 2 trucks, 40.4ms\nSpeed: 2.2ms preprocess, 40.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 truck, 38.1ms\nSpeed: 2.0ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 3 trucks, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.6ms\nSpeed: 2.0ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 1 truck, 38.2ms\nSpeed: 2.3ms preprocess, 38.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 38.7ms\nSpeed: 2.0ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 4 trucks, 38.9ms\nSpeed: 2.3ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 3 trucks, 38.6ms\nSpeed: 2.1ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 2 trucks, 38.6ms\nSpeed: 2.0ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 4 trucks, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 2 trucks, 39.6ms\nSpeed: 2.1ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 2 trucks, 38.7ms\nSpeed: 2.6ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 2 trucks, 39.7ms\nSpeed: 2.3ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 2 trucks, 39.4ms\nSpeed: 2.6ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 38.6ms\nSpeed: 2.6ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 3 trucks, 39.6ms\nSpeed: 2.6ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 39.0ms\nSpeed: 2.4ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 3 trucks, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 39.9ms\nSpeed: 2.1ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 3 trucks, 39.1ms\nSpeed: 2.7ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 5 cars, 1 bus, 3 trucks, 39.6ms\nSpeed: 2.6ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 6 cars, 1 bus, 2 trucks, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 39.1ms\nSpeed: 2.6ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 38.8ms\nSpeed: 2.1ms preprocess, 38.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.9ms\nSpeed: 2.0ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 39.6ms\nSpeed: 2.2ms preprocess, 39.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 truck, 39.6ms\nSpeed: 2.2ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.4ms\nSpeed: 2.6ms preprocess, 39.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 39.7ms\nSpeed: 2.1ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 1 truck, 39.6ms\nSpeed: 2.0ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 39.5ms\nSpeed: 2.1ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 2 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 1 truck, 40.0ms\nSpeed: 2.1ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.4ms\nSpeed: 2.1ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 2 trucks, 39.7ms\nSpeed: 2.1ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 5 trucks, 38.9ms\nSpeed: 2.1ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 3 trucks, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 truck, 39.2ms\nSpeed: 2.1ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.7ms\nSpeed: 2.8ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.6ms\nSpeed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 39.8ms\nSpeed: 2.1ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.2ms\nSpeed: 2.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 39.3ms\nSpeed: 2.6ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 3 trucks, 40.0ms\nSpeed: 2.4ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 1 truck, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 1 truck, 39.1ms\nSpeed: 2.6ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 39.9ms\nSpeed: 2.3ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 7 cars, 1 bus, 1 truck, 41.0ms\nSpeed: 2.2ms preprocess, 41.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.6ms\nSpeed: 2.2ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.3ms\nSpeed: 2.6ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 3 trucks, 40.0ms\nSpeed: 2.6ms preprocess, 40.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 39.1ms\nSpeed: 2.2ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 40.0ms\nSpeed: 2.2ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 38.5ms\nSpeed: 2.6ms preprocess, 38.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 1 truck, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 truck, 40.1ms\nSpeed: 2.7ms preprocess, 40.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 40.0ms\nSpeed: 2.3ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 39.3ms\nSpeed: 2.6ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 40.0ms\nSpeed: 2.1ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 39.3ms\nSpeed: 2.6ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 40.0ms\nSpeed: 2.3ms preprocess, 40.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 1 truck, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 40.0ms\nSpeed: 2.3ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 1 umbrella, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 trucks, 1 umbrella, 39.8ms\nSpeed: 2.6ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 trucks, 1 umbrella, 40.1ms\nSpeed: 2.1ms preprocess, 40.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 38.4ms\nSpeed: 2.6ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 39.8ms\nSpeed: 2.3ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 39.5ms\nSpeed: 2.1ms preprocess, 39.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 3 trucks, 1 umbrella, 39.9ms\nSpeed: 2.2ms preprocess, 39.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 40.0ms\nSpeed: 2.0ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 39.5ms\nSpeed: 2.6ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 38.9ms\nSpeed: 2.6ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 39.9ms\nSpeed: 2.6ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 36.6ms\nSpeed: 2.1ms preprocess, 36.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 40.3ms\nSpeed: 2.1ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 39.9ms\nSpeed: 2.6ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 2 trucks, 1 umbrella, 40.5ms\nSpeed: 2.2ms preprocess, 40.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 40.2ms\nSpeed: 2.2ms preprocess, 40.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 trucks, 1 umbrella, 40.0ms\nSpeed: 2.2ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 40.0ms\nSpeed: 2.2ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 40.3ms\nSpeed: 2.1ms preprocess, 40.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 39.4ms\nSpeed: 2.0ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 38.9ms\nSpeed: 2.0ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 39.7ms\nSpeed: 2.2ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 40.2ms\nSpeed: 2.2ms preprocess, 40.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.2ms\nSpeed: 2.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.3ms\nSpeed: 2.1ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 1 umbrella, 39.2ms\nSpeed: 2.6ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 4 trucks, 1 umbrella, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 39.0ms\nSpeed: 2.0ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 38.9ms\nSpeed: 2.3ms preprocess, 38.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 39.3ms\nSpeed: 2.3ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 1 umbrella, 39.8ms\nSpeed: 2.3ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 39.2ms\nSpeed: 2.2ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 40.3ms\nSpeed: 2.4ms preprocess, 40.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 39.7ms\nSpeed: 2.2ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 39.9ms\nSpeed: 2.0ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 3 trucks, 1 umbrella, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 3 trucks, 1 umbrella, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 39.2ms\nSpeed: 2.3ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 1 umbrella, 40.2ms\nSpeed: 2.6ms preprocess, 40.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 39.3ms\nSpeed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 1 umbrella, 40.3ms\nSpeed: 2.2ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 4 trucks, 1 umbrella, 39.0ms\nSpeed: 2.2ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 buss, 4 trucks, 1 umbrella, 39.5ms\nSpeed: 2.0ms preprocess, 39.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 2 buss, 2 trucks, 39.3ms\nSpeed: 2.2ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 2 trucks, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 3 trucks, 40.3ms\nSpeed: 2.6ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 39.6ms\nSpeed: 2.7ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 40.8ms\nSpeed: 2.2ms preprocess, 40.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 12 cars, 1 bus, 3 trucks, 38.9ms\nSpeed: 2.2ms preprocess, 38.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 4 trucks, 39.5ms\nSpeed: 2.1ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 40.4ms\nSpeed: 2.3ms preprocess, 40.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 1 bus, 3 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 5 trucks, 40.3ms\nSpeed: 2.2ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 42.1ms\nSpeed: 2.2ms preprocess, 42.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 37.5ms\nSpeed: 2.2ms preprocess, 37.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 40.9ms\nSpeed: 2.3ms preprocess, 40.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 39.4ms\nSpeed: 2.1ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 3 trucks, 40.5ms\nSpeed: 2.3ms preprocess, 40.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 38.5ms\nSpeed: 2.3ms preprocess, 38.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 40.1ms\nSpeed: 2.0ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 40.8ms\nSpeed: 2.0ms preprocess, 40.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 39.1ms\nSpeed: 2.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 38.7ms\nSpeed: 2.6ms preprocess, 38.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 40.6ms\nSpeed: 2.3ms preprocess, 40.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.7ms\nSpeed: 2.1ms preprocess, 39.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 6 trucks, 39.9ms\nSpeed: 2.1ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 5 trucks, 40.2ms\nSpeed: 2.2ms preprocess, 40.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 39.5ms\nSpeed: 2.2ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 11 cars, 4 trucks, 39.8ms\nSpeed: 2.0ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.7ms\nSpeed: 2.1ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 41.4ms\nSpeed: 2.0ms preprocess, 41.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.7ms\nSpeed: 2.2ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.5ms\nSpeed: 2.4ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 39.1ms\nSpeed: 2.1ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 40.3ms\nSpeed: 2.2ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 4 trucks, 40.3ms\nSpeed: 2.0ms preprocess, 40.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 5 trucks, 39.4ms\nSpeed: 2.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 39.6ms\nSpeed: 2.1ms preprocess, 39.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 39.1ms\nSpeed: 2.3ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 5 trucks, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.5ms\nSpeed: 2.3ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 6 trucks, 39.8ms\nSpeed: 2.6ms preprocess, 39.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 4 trucks, 39.8ms\nSpeed: 2.2ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 5 trucks, 39.7ms\nSpeed: 2.2ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 4 trucks, 39.4ms\nSpeed: 2.1ms preprocess, 39.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 4 trucks, 39.9ms\nSpeed: 2.3ms preprocess, 39.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 43.9ms\nSpeed: 2.2ms preprocess, 43.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 4 trucks, 43.8ms\nSpeed: 2.2ms preprocess, 43.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 3 trucks, 38.2ms\nSpeed: 2.6ms preprocess, 38.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 2 buss, 3 trucks, 39.7ms\nSpeed: 2.3ms preprocess, 39.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 5 trucks, 39.3ms\nSpeed: 2.6ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 5 trucks, 40.1ms\nSpeed: 2.2ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 8 cars, 1 bus, 5 trucks, 40.0ms\nSpeed: 2.0ms preprocess, 40.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 40.5ms\nSpeed: 2.0ms preprocess, 40.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 1 bus, 2 trucks, 39.0ms\nSpeed: 2.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 2 trucks, 40.6ms\nSpeed: 2.0ms preprocess, 40.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 39.8ms\nSpeed: 2.7ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 39.5ms\nSpeed: 2.6ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 9 cars, 2 buss, 3 trucks, 40.8ms\nSpeed: 2.6ms preprocess, 40.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 2 trucks, 39.5ms\nSpeed: 1.9ms preprocess, 39.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 2 buss, 2 trucks, 40.1ms\nSpeed: 2.0ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n\n0: 480x640 10 cars, 1 bus, 3 trucks, 41.0ms\nSpeed: 2.1ms preprocess, 41.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\nVideo processing complete: 900 frames processed.\nCalculating average speeds...\nMiddle frame saved as 'middle_frame.jpg'.\nPeak frame saved as 'peak_frame.jpg'.\n\n--- Context Generated ---\n\n- Video duration: 30.00 seconds\n- Unique vehicles: car: 51, truck: 9, bus: 4 (total: 64)\n- Average vehicles per frame: car: 6.29, truck: 0.82, bus: 0.08\n- Average speeds (pixels/s): car: 87.65, truck: 111.03, bus: 42.94\n- Peak traffic: 12 vehicles at 27.73 seconds\n- Middle frame (at 15.00 seconds): car: 4, truck: 1\n- Average congestion index: 1.44 (0=low, 1=moderate, >2=high)\n- Emergency alerts: None\n- Temporal trends: First 25%: 1715 vehicles, Middle 50%: 3007 vehicles, Last 25%: 1750 vehicles\nExporting data to CSV...\nTraffic data exported to 'traffic_data.csv'.\nGenerating traffic density heatmap...\nHeatmap saved as 'heatmap.png'.\nGenerating text report...\n\n--- Generated Report ---\n\nThe traffic flow analysis reveals a number of factors that influence traffic flow. A total of 64 unique vehicles were detected, with peak traffic occurring at 27.73 seconds. The average congestion index was 1.44 seconds. Recommendations should include: \"Traffic management strategies should include\" and \"Use common sense\" for traffic management. The report is based on a 30-second video of duration 30.00 seconds, with a total number of 64 vehicles detected. It is available for download at the following URL: http://www.researchmonkey.com/r/TrafficAnalysisReport.\nGenerating PDF report...\nPDF report generated as 'report.pdf'.\n\nVideo processing complete. Ask any question about the video (type 'exit' to quit).\nSuggested questions:\n1. How many vehicles were detected in total?\n2. What types of vehicles were detected?\n3. What was the peak traffic time?\n4. How many unique vehicles passed from 2 to 5 seconds?\n5. Describe the traffic in the video.\n6. What was happening in the middle of the video?\n7. How many cars were detected?\n8. How many trucks were detected?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  How many vehicles were detected in total?\n"},{"name":"stdout","text":"\nAnswer:\nTotal unique vehicles detected: 64.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  What types of vehicles were detected?\n"},{"name":"stdout","text":"\nAnswer:\nTypes of vehicles detected: car, truck, bus.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  What was the peak traffic time?\n"},{"name":"stdout","text":"\nAnswer:\nPeak traffic occurred at 27.73 seconds with 12 vehicles in frame.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  How many unique vehicles passed from 2 to 5 seconds?\n"},{"name":"stdout","text":"\nAnswer:\nFrom 2.0s to 5.0s, 16 unique vehicles passed: 15 car(s), 1 truck(s)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  Describe the traffic in the video.\n"},{"name":"stderr","text":"Your max_length is set to 58, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n","output_type":"stream"},{"name":"stdout","text":"\nAnswer:\nThe video shows 12 vehicles at 27.73 seconds.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  How many cars were detected?\n"},{"name":"stdout","text":"\nAnswer:\n51 car(s) detected in total. Average of 6.29 per frame. Average speed: 87.65 pixels/second.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nAsk a question (or type 'exit' to quit):  How many trucks were detected?\n"},{"name":"stdout","text":"\nAnswer:\n9 truck(s) detected in total. Average of 0.82 per frame. Average speed: 111.03 pixels/second.\n","output_type":"stream"}],"execution_count":null}]}