# -*- coding: utf-8 -*-
"""Grok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxfCwFzbPziI_HvfWhrb6vNGNnhtYgSz
"""

!huggingface-cli login

!pip install ultralytics

import cv2
import numpy as np
from ultralytics import YOLO
from transformers import pipeline
from PIL import Image
from google.colab.patches import cv2_imshow

# Load your trained YOLO model
model_path = r"/content/best.pt"
model = YOLO(model_path)

# Video path and capture
video_path = r"/content/Test_video_1.mp4"
cap = cv2.VideoCapture(video_path)

# Get total number of frames
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
middle_index = total_frames // 2 if total_frames > 0 else -1
selected_frame = None

# Dictionaries to store vehicle counts and track mappings
vehicle_counts = {}  # Counts unique vehicles per type
track_id_to_class = {}  # Maps track IDs to vehicle types

frame_index = 0

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        break

    # Optionally resize the frame
    frame = cv2.resize(frame, (700, 500))

    # Run YOLO tracking
    results = model.track(frame, persist=True)
    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes in [x1, y1, x2, y2]
    confidences = results[0].boxes.conf.cpu().numpy()
    classes = results[0].boxes.cls.cpu().numpy().astype(int)
    # Extract track IDs, handle case where no IDs are present
    track_ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else []

    # Count unique vehicles
    for conf, cls, track_id in zip(confidences, classes, track_ids):
        if conf < 0.5:
            continue  # Filter low-confidence detections
        label = results[0].names[cls]  # Get vehicle type (e.g., "car", "ambulance")
        if track_id not in track_id_to_class:
            # New vehicle detected
            track_id_to_class[track_id] = label
            vehicle_counts[label] = vehicle_counts.get(label, 0) + 1

    # Visualize predictions (optional, includes track IDs)
    annotated_frame = results[0].plot()

    # Save the middle frame for the report
    if frame_index == middle_index:
        selected_frame = frame.copy()

    frame_index += 1

cap.release()
cv2.destroyAllWindows()

# Convert selected frame to PIL image
if selected_frame is not None:
    selected_frame_rgb = cv2.cvtColor(selected_frame, cv2.COLOR_BGR2RGB)
    selected_frame_pil = Image.fromarray(selected_frame_rgb)
else:
    print("Warning: No frame was selected. Using counts only for report generation.")
    selected_frame_pil = None

# Set up the image-text-to-text pipeline with Llama-3.2-11B-Vision
pipe = pipeline("image-text-to-text", model="meta-llama/Llama-3.2-11B-Vision", use_auth_token=True)

# Build the report prompt with unique vehicle counts
report_prompt = (
    f"Based on this frame from a vehicle detection video and the following unique vehicle counts across all frames:\n"
)
for vehicle_type, count in vehicle_counts.items():
    report_prompt += f"{vehicle_type}: {count}\n"
report_prompt += "Generate a report summarizing the unique vehicle counts and describing the scene in the selected frame."

# Generate the initial report
if selected_frame_pil is not None:
    result = pipe(image=selected_frame_pil, text=report_prompt, max_new_tokens=300)
    report_text = result[0]['generated_text']
else:
    # Fallback prompt without image
    report_prompt_fallback = (
        f"Vehicle Detection Report:\n"
    )
    for vehicle_type, count in vehicle_counts.items():
        report_prompt_fallback += f"{vehicle_type}: {count}\n"
    report_prompt_fallback += "Based on these unique counts, provide a summary of the vehicle detections."
    result = pipe(image=None, text=report_prompt_fallback, max_new_tokens=300)
    report_text = result[0]['generated_text']

# Print the generated report
print("\n--- Generated Report ---\n")
print(report_text)

# Start chat interaction
print("\nVideo processing complete. You can now ask questions about the video (type 'exit' to quit).")

while True:
    user_query = input("Ask a question: ")
    if user_query.lower() == "exit":
        print("Goodbye.")
        break

    # Handle specific counting questions
    if "how many vehicles" in user_query.lower() or "total number of vehicles" in user_query.lower():
        total_vehicles = sum(vehicle_counts.values())
        response = f"There are {total_vehicles} vehicles in total in the video."
    elif "how many" in user_query.lower():
        words = user_query.lower().split()
        vehicle_type = None
        for word in words:
            if word in vehicle_counts:
                vehicle_type = word
                break
            elif word + "s" in vehicle_counts:  # Check plural by adding 's'
                vehicle_type = word + "s"
                break
            elif word.endswith("s") and word[:-1] in vehicle_counts:  # Check singular by removing 's'
                vehicle_type = word[:-1]
                break
        if vehicle_type and vehicle_type in vehicle_counts:
            count = vehicle_counts[vehicle_type]
            response = f"There are {count} {vehicle_type}(s) in the video."
        else:
            response = "I'm not sure about that vehicle type."
    else:
        # Build prompt for general questions
        prompt = f"Based on the following unique vehicle counts across all frames:\n"
        for vt, count in vehicle_counts.items():
            prompt += f"{vt}: {count}\n"
        prompt += f"And the provided frame, answer the following question: {user_query}"

        # Generate response using the model
        if selected_frame_pil is not None:
            result = pipe(image=selected_frame_pil, text=prompt, max_new_tokens=300)
            response = result[0]['generated_text']
        else:
            prompt_no_image = prompt.replace("and the provided frame", "")
            result = pipe(image=None, text=prompt_no_image, max_new_tokens=300)
            response = result[0]['generated_text']

    # Display the response
    print("\nAnswer:")
    print(response)
    print()